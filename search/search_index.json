{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u76ee\u7684 \u00b6 \u901a\u8fc7\u8ba4\u8bc6\u4e00\u4e2a\u6846\u67b6\uff0c\u5b66\u4e60\u6846\u67b6\u91cc\u9762\u7684\u601d\u60f3\uff0c\u56e0\u4e3a\u6846\u67b6\u90fd\u662f\u6709\u5171\u6027\u7684\uff0c\u7136\u540e\u5c31\u662f\u603b\u7ed3 \u524d\u4eba\u8d70\u8fc7\u7684\u8def\uff0c\u8df3\u8fc7\u4e00\u4e9b\u5751\uff0c\u4e5f\u662f\u4e3a\u4e86\u5728\u4ed6\u57fa\u7840\u4e0a\u53d1\u6325\u6211\u4eec\u7684\u4f5c\u7528\uff0c\u7ee7\u800c\u521b\u65b0\u3002 \u5173\u4e8e python \u00b6 python \u8fd9\u4e9b\u5e74\u5982\u96e8\u540e\u6625\u7b0b\u6210\u957f\u8d77\u6765\u4e86\uff0c\u66f4\u91cd\u8981\u7684\u662f\u4ed6\u7684\u751f\u6001\u8d77\u6765\u4e86\uff0c\u5f88\u591a\u4e1c\u897f\u5c31\u4e0d\u7528\u91cd\u590d\u9020\u8f6e\u5b50\u4e86\uff0c \u4ed6\u7684\u8bed\u6cd5\u76f8\u5bf9\u7b80\u5355\uff0c\u4e0a\u624b\u4e5f\u662f\u76f8\u5bf9\u5bb9\u6613\uff0c\u4f46\u662f\u5e76\u4e0d\u610f\u5473\u7740\u5c31\u597d\u5b66\uff0c\u5e9e\u5927\u7684\u5206\u652f\u7ed3\u6784\u7167\u6210\u4e86\u5982\u679c\u60f3\u5199\u51fa\u9ad8\u8d28\u91cf\u7684 \u4ee3\u7801\u4e5f\u662f\u975e\u5e38\u96be\u5f97\u3002 \u4fdd\u6301\u72ec\u7acb\u601d\u8003 \u00b6 \u4fdd\u6301\u72ec\u7acb\u601d\u8003\u5728\u8fd9\u4fe1\u606f\u7206\u70b8\u7684\u65f6\u4ee3\u662f\u975e\u5e38\u56f0\u96be\u7684\uff0c\u4e00\u4e2a\u662f\u4fe1\u606f\u7684\u83b7\u53d6\u975e\u5e38\u5bb9\u6613\uff0c\u4f46\u662f\u5bb9\u6613\u7684\u540c\u65f6\u5c31\u7167\u6210\u6211\u4eec\u672c\u80fd\u5c31\u4e0d\u60f3\u53bb \u60f3\u66f4\u591a\u7684\u4e1c\u897f\uff0c\u56e0\u4e3a\u601d\u8003\u672c\u8eab\u662f\u9700\u8981\u7cbe\u529b\u53bb\u505a\u7684\u3002\u5f53\u6211\u4eec\u83b7\u53d6\u4fe1\u606f\u5982\u4f55\u53bb\u6d88\u5316\u4ed6\u4e5f\u662f\u4e0d\u5bb9\u6613\u7684\uff0c\u6709\u7684\u4fe1\u606f\u5bc6\u5ea6\u5927\uff0c\u9700\u8981\u7ec6\u56bc\u6162\u54bd\uff0c \u5b66\u4e60\u4e00\u95e8\u9700\u8981 \u00b6 \u5b66\u4e60\u57fa\u672c\u8bed\u6cd5 \u5b66\u4e60\u57fa\u672c\u5e93 \u5b9e\u8df5\uff0c\u5b9e\u8df5\u662f\u4e3a\u4e86\u52a0\u5f3a\u7406\u89e3\u3002\u4e5f\u662f\u4e3a\u4e86\u6d88\u5316\u77e5\u8bc6 \u672c\u7535\u5b50\u4e66\u7684\u5236\u4f5c\u65b9\u5f0f \u00b6 \u5b89\u88c5\u4f9d\u8d56 1 2 3 4 5 6 7 8 pip install mkdocs # \u5236\u4f5c\u7535\u5b50\u4e66, http://markdown-docs-zh.readthedocs.io/zh_CN/latest/ # https://stackoverflow.com/questions/27882261/mkdocs-and-mathjax/31874157 pip install https : // github . com / mitya57 / python - markdown - math / archive / master . zip # \u5efa\u8bae\u76f4\u63a5\u5b89\u88c5 requirements.txt \u4e2d\u7684\u4f9d\u8d56\u3002\u5df2\u7ecf\u9501\u5b9a\u4e86\u7248\u672c\uff0c\u4e0d\u540c\u7684 mkdocs \u4e4b\u95f4\u6709\u4e00\u5b9a\u5dee\u5f02 pip install - r requirements . txt # \u5982\u679c\u4f60 fork \u4e86\u672c\u9879\u76ee\uff0c\u53ef\u4ee5\u5b9a\u671f\u62c9\u53d6\u4e3b\u4ed3\u5e93\u7684\u4ee3\u7801\u6765\u83b7\u53d6\u66f4\u65b0\uff0c\u76ee\u524d\u8fd8\u5728\u4e0d\u65ad\u66f4\u65b0\u76f8\u5173\u7ae0\u8282 1 2 3 4 5 \u4e5f\u53ef\u4ee5\u5728clone\u672c\u7535\u5b50\u4e66\u540e\u8fdb\u884c\u4e8c\u6b21\u521b\u4f5c mkdocs serve # \u4fee\u6539\u81ea\u52a8\u66f4\u65b0\uff0c\u6d4f\u89c8\u5668\u6253\u5f00 http://localhost:8000 \u8bbf\u95ee # \u6570\u5b66\u516c\u5f0f\u53c2\u8003 https://www.zybuluo.com/codeep/note/163962 mkdocs gh - deploy # \u90e8\u7f72\u5230\u81ea\u5df1\u7684 github pages # \u7ed9 mkdocs \u589e\u52a0 gitalk \u8bc4\u8bba\u7cfb\u7edf https://50u1w4y.github.io/site/misc/gitalkBuild/","title":"\u7b80\u4ecb"},{"location":"#_1","text":"\u901a\u8fc7\u8ba4\u8bc6\u4e00\u4e2a\u6846\u67b6\uff0c\u5b66\u4e60\u6846\u67b6\u91cc\u9762\u7684\u601d\u60f3\uff0c\u56e0\u4e3a\u6846\u67b6\u90fd\u662f\u6709\u5171\u6027\u7684\uff0c\u7136\u540e\u5c31\u662f\u603b\u7ed3 \u524d\u4eba\u8d70\u8fc7\u7684\u8def\uff0c\u8df3\u8fc7\u4e00\u4e9b\u5751\uff0c\u4e5f\u662f\u4e3a\u4e86\u5728\u4ed6\u57fa\u7840\u4e0a\u53d1\u6325\u6211\u4eec\u7684\u4f5c\u7528\uff0c\u7ee7\u800c\u521b\u65b0\u3002","title":"\u76ee\u7684"},{"location":"#python","text":"python \u8fd9\u4e9b\u5e74\u5982\u96e8\u540e\u6625\u7b0b\u6210\u957f\u8d77\u6765\u4e86\uff0c\u66f4\u91cd\u8981\u7684\u662f\u4ed6\u7684\u751f\u6001\u8d77\u6765\u4e86\uff0c\u5f88\u591a\u4e1c\u897f\u5c31\u4e0d\u7528\u91cd\u590d\u9020\u8f6e\u5b50\u4e86\uff0c \u4ed6\u7684\u8bed\u6cd5\u76f8\u5bf9\u7b80\u5355\uff0c\u4e0a\u624b\u4e5f\u662f\u76f8\u5bf9\u5bb9\u6613\uff0c\u4f46\u662f\u5e76\u4e0d\u610f\u5473\u7740\u5c31\u597d\u5b66\uff0c\u5e9e\u5927\u7684\u5206\u652f\u7ed3\u6784\u7167\u6210\u4e86\u5982\u679c\u60f3\u5199\u51fa\u9ad8\u8d28\u91cf\u7684 \u4ee3\u7801\u4e5f\u662f\u975e\u5e38\u96be\u5f97\u3002","title":"\u5173\u4e8epython"},{"location":"#_2","text":"\u4fdd\u6301\u72ec\u7acb\u601d\u8003\u5728\u8fd9\u4fe1\u606f\u7206\u70b8\u7684\u65f6\u4ee3\u662f\u975e\u5e38\u56f0\u96be\u7684\uff0c\u4e00\u4e2a\u662f\u4fe1\u606f\u7684\u83b7\u53d6\u975e\u5e38\u5bb9\u6613\uff0c\u4f46\u662f\u5bb9\u6613\u7684\u540c\u65f6\u5c31\u7167\u6210\u6211\u4eec\u672c\u80fd\u5c31\u4e0d\u60f3\u53bb \u60f3\u66f4\u591a\u7684\u4e1c\u897f\uff0c\u56e0\u4e3a\u601d\u8003\u672c\u8eab\u662f\u9700\u8981\u7cbe\u529b\u53bb\u505a\u7684\u3002\u5f53\u6211\u4eec\u83b7\u53d6\u4fe1\u606f\u5982\u4f55\u53bb\u6d88\u5316\u4ed6\u4e5f\u662f\u4e0d\u5bb9\u6613\u7684\uff0c\u6709\u7684\u4fe1\u606f\u5bc6\u5ea6\u5927\uff0c\u9700\u8981\u7ec6\u56bc\u6162\u54bd\uff0c","title":"\u4fdd\u6301\u72ec\u7acb\u601d\u8003"},{"location":"#_3","text":"\u5b66\u4e60\u57fa\u672c\u8bed\u6cd5 \u5b66\u4e60\u57fa\u672c\u5e93 \u5b9e\u8df5\uff0c\u5b9e\u8df5\u662f\u4e3a\u4e86\u52a0\u5f3a\u7406\u89e3\u3002\u4e5f\u662f\u4e3a\u4e86\u6d88\u5316\u77e5\u8bc6","title":"\u5b66\u4e60\u4e00\u95e8\u9700\u8981"},{"location":"#_4","text":"\u5b89\u88c5\u4f9d\u8d56 1 2 3 4 5 6 7 8 pip install mkdocs # \u5236\u4f5c\u7535\u5b50\u4e66, http://markdown-docs-zh.readthedocs.io/zh_CN/latest/ # https://stackoverflow.com/questions/27882261/mkdocs-and-mathjax/31874157 pip install https : // github . com / mitya57 / python - markdown - math / archive / master . zip # \u5efa\u8bae\u76f4\u63a5\u5b89\u88c5 requirements.txt \u4e2d\u7684\u4f9d\u8d56\u3002\u5df2\u7ecf\u9501\u5b9a\u4e86\u7248\u672c\uff0c\u4e0d\u540c\u7684 mkdocs \u4e4b\u95f4\u6709\u4e00\u5b9a\u5dee\u5f02 pip install - r requirements . txt # \u5982\u679c\u4f60 fork \u4e86\u672c\u9879\u76ee\uff0c\u53ef\u4ee5\u5b9a\u671f\u62c9\u53d6\u4e3b\u4ed3\u5e93\u7684\u4ee3\u7801\u6765\u83b7\u53d6\u66f4\u65b0\uff0c\u76ee\u524d\u8fd8\u5728\u4e0d\u65ad\u66f4\u65b0\u76f8\u5173\u7ae0\u8282 1 2 3 4 5 \u4e5f\u53ef\u4ee5\u5728clone\u672c\u7535\u5b50\u4e66\u540e\u8fdb\u884c\u4e8c\u6b21\u521b\u4f5c mkdocs serve # \u4fee\u6539\u81ea\u52a8\u66f4\u65b0\uff0c\u6d4f\u89c8\u5668\u6253\u5f00 http://localhost:8000 \u8bbf\u95ee # \u6570\u5b66\u516c\u5f0f\u53c2\u8003 https://www.zybuluo.com/codeep/note/163962 mkdocs gh - deploy # \u90e8\u7f72\u5230\u81ea\u5df1\u7684 github pages # \u7ed9 mkdocs \u589e\u52a0 gitalk \u8bc4\u8bba\u7cfb\u7edf https://50u1w4y.github.io/site/misc/gitalkBuild/","title":"\u672c\u7535\u5b50\u4e66\u7684\u5236\u4f5c\u65b9\u5f0f"},{"location":"basics/scrapy/00_scrapy_setting/00_scrapy_setting/","text":"Hello Go! \u00b6 \u98df\u6307 \\color{blue}{\u5f53\u8718\u86db\u7f51\u65e0\u60c5\u5730\u67e5\u5c01\u4e86\u6211\u7684\u7089\u53f0\uff0c \u5f53\u7070\u70ec\u7684\u4f59\u70df\u53f9\u606f\u7740\u8d2b\u56f0\u7684\u60b2\u54c0\uff0c \u6211\u4f9d\u7136\u56fa\u6267\u5730\u94fa\u5e73\u5931\u671b\u7684\u7070\u70ec} scrapy \u7684setting \u7684\u5e38\u89c1\u914d\u7f6e \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 # -*- coding: utf-8 -*- import os # \u9879\u76ee\u540d BOT_NAME = 'ArticleSpider' # Scrapy \u641c\u7d22spider\u7684\u6a21\u5757\u5217\u8868\uff0c\u9ed8\u8ba4\uff1a[xxx.spiders] SPIDER_MODULES = [ 'ArticleSpider.spiders' ] # \u4f7f\u7528genspider \u547d\u4ee4\u521b\u5efa\u65b0spider\u7684\u6a21\u5757\uff0c\u9ed8\u8ba4'xxx.spider' NEWSPIDER_MODULE = 'ArticleSpider.spiders' # \u722c\u53d6\u7684\u9ed8\u8ba4User-Agent,\u9664\u975e\u88ab\u8986\u76d6 USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\" # Scrapy\u662f\u5426\u9075\u5faarobots.txt\u7b56\u7565 ROBOTSTXT_OBEY = False # Scrapy downloader \u5e76\u53d1\u8bf7\u6c42(concurrent requests)\u7684\u6700\u5927\u503c,\u9ed8\u8ba4: 16 CONCURRENT_REQUESTS = 32 # \u9650\u901f DOWNLOAD_DELAY = 2 # \u4e0b\u8f7d\u5ef6\u8fdf\u8bbe\u7f6e\u53ea\u6709\u4e00\u4e2a\u6709\u6548 #CONCURRENT_REQUESTS_PER_DOMAIN = 16 # \u5bf9\u5355\u4e2a\u7f51\u7ad9\u8fdb\u884c\u6bd4\u90a3\u4e2a\u53d1\u8bf7\u6c42\u7684\u6700\u5927\u503c #CONCURRENT_REQUESTS_PER_IP = 16 # \u5bf9\u5355\u4e2aip\u8fdb\u884c\u5e76\u53d1\u8bf7\u6c42\u7684\u6700\u5927\u503c\uff0c\u5982\u679c\u975e0\uff0c\u5219\u5ffd\u7565 # \u662f\u5426\u8b66\u7528Cookie(\u9ed8\u8ba4\u542f\u7528) COOKIES_ENABLED = True COOKIES_DEBUG = True #\u7981\u7528Telnet\u63a7\u5236\u53f0\uff08\u9ed8\u8ba4\u542f\u7528\uff09 #TELNETCONSOLE_ENABLED = False # \u8986\u76d6\u9ed8\u8ba4\u8bf7\u6c42\u6807\u5934 #DEFAULT_REQUEST_HEADERS = { # 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', # 'Accept-Language': 'en', #} # \u542f\u7528\u6216\u7981\u6b62\u722c\u866b\u4e2d\u95f4\u4ef6 # SPIDER_MIDDLEWARES = { # 'ArticleSpider.middlewares.ArticlespiderSpiderMiddleware': 543, # } # \u542f\u7528\u6216\u7981\u6b62\u4e0b\u518d\u8d77\u4e2d\u95f4\u4ef6 DOWNLOADER_MIDDLEWARES = { # 'ArticleSpider.middlewares.JSPageMiddleware': 1, # 'ArticleSpider.middlewares.RandomUserAgentMiddlware': 543, # 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 2, 'ArticleSpider.middlewares.RandomProxyMiddleware' : 3 , } # \u542f\u7528\u6216\u7981\u6b62\u6216\u8005\u7a0b\u5e8f #EXTENSIONS = { # 'scrapy.extensions.telnet.TelnetConsole': None, #} # \u914d\u7f6e\u9879\u76ee\u7ba1\u9053 ITEM_PIPELINES = { # 'ArticleSpider.pipelines.DoubanspiderPipeline' : 300, # 'ArticleSpider.pipelines.ImagesPipeline': 1, # 'ArticleSpider.pipelines.DouyuPipeline':2, 'ArticleSpider.pipelines.JobblePipeline' : 300 , } # \u53ef\u914d\u7f6e RANDOM_UA_TYPE = \"random\" # \u8bbe\u7f6e\u6700\u5c0f\u957f\u5bbd\uff0c\u4e0d\u7136\u592a\u5c0f\u6ca1\u5565\u610f\u4e49 IMAGES_MIN_HEIGHT = 20 IMAGES_MIN_WIDTH = 20 # \u542f\u7528\u548c\u914d\u7f6eAuthThrottle\u6269\u5c55(\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u7981\u7528) AUTOTHROTTLE_ENABLED = True # \u521d\u59cb\u5316\u4e0b\u8f7d\u5ef6\u8fdf #AUTOTHROTTLE_START_DELAY = 5 # \u5728\u9ad8\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\u8bbe\u7f6e\u7684\u8db3\u5e95\u554a\u4e0b\u8f7d\u5ef6\u8fdf #AUTOTHROTTLE_MAX_DELAY = 60 #Scrapy\u8bf7\u6c42\u7684\u5e73\u5747\u6570\u91cf\u5e94\u8be5\u5e76\u884c\u53d1\u9001\u6bcf\u4e2a\u8fdc\u7a0b\u670d\u52a1\u5668 #AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0 # \u542f\u7528\u663e\u793a\u6240\u6536\u5230\u7684\u6bcf\u4e2a\u54cd\u5e94\u7684\u8c03\u8282\u7edf\u8ba1\u4fe1\u606f #AUTOTHROTTLE_DEBUG = False # \u542f\u7528\u548c\u914d\u7f6eHTTP\u7f13\u5b58\uff08\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u7981\u7528\uff09 #HTTPCACHE_ENABLED = True #HTTPCACHE_EXPIRATION_SECS = 0 #HTTPCACHE_DIR = 'httpcache' #HTTPCACHE_IGNORE_HTTP_CODES = [] #HTTPCACHE_STORAGE = \u5e38\u89c1\u95ee\u9898 \u00b6 \u4e24\u4e2a\u4e2d\u95f4\u4ef6 spider_middlewares \u548c downloader_middlewares \u7684\u533a\u522b\uff1f \u53c2\u8003\u94fe\u63a5 \u00b6 https://zhuanlan.zhihu.com/p/42498126","title":"Scrapy \u6e90\u7801 settings"},{"location":"basics/scrapy/00_scrapy_setting/00_scrapy_setting/#hello-go","text":"\u98df\u6307 \\color{blue}{\u5f53\u8718\u86db\u7f51\u65e0\u60c5\u5730\u67e5\u5c01\u4e86\u6211\u7684\u7089\u53f0\uff0c \u5f53\u7070\u70ec\u7684\u4f59\u70df\u53f9\u606f\u7740\u8d2b\u56f0\u7684\u60b2\u54c0\uff0c \u6211\u4f9d\u7136\u56fa\u6267\u5730\u94fa\u5e73\u5931\u671b\u7684\u7070\u70ec}","title":"Hello Go!"},{"location":"basics/scrapy/00_scrapy_setting/00_scrapy_setting/#scrapy-setting","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 # -*- coding: utf-8 -*- import os # \u9879\u76ee\u540d BOT_NAME = 'ArticleSpider' # Scrapy \u641c\u7d22spider\u7684\u6a21\u5757\u5217\u8868\uff0c\u9ed8\u8ba4\uff1a[xxx.spiders] SPIDER_MODULES = [ 'ArticleSpider.spiders' ] # \u4f7f\u7528genspider \u547d\u4ee4\u521b\u5efa\u65b0spider\u7684\u6a21\u5757\uff0c\u9ed8\u8ba4'xxx.spider' NEWSPIDER_MODULE = 'ArticleSpider.spiders' # \u722c\u53d6\u7684\u9ed8\u8ba4User-Agent,\u9664\u975e\u88ab\u8986\u76d6 USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\" # Scrapy\u662f\u5426\u9075\u5faarobots.txt\u7b56\u7565 ROBOTSTXT_OBEY = False # Scrapy downloader \u5e76\u53d1\u8bf7\u6c42(concurrent requests)\u7684\u6700\u5927\u503c,\u9ed8\u8ba4: 16 CONCURRENT_REQUESTS = 32 # \u9650\u901f DOWNLOAD_DELAY = 2 # \u4e0b\u8f7d\u5ef6\u8fdf\u8bbe\u7f6e\u53ea\u6709\u4e00\u4e2a\u6709\u6548 #CONCURRENT_REQUESTS_PER_DOMAIN = 16 # \u5bf9\u5355\u4e2a\u7f51\u7ad9\u8fdb\u884c\u6bd4\u90a3\u4e2a\u53d1\u8bf7\u6c42\u7684\u6700\u5927\u503c #CONCURRENT_REQUESTS_PER_IP = 16 # \u5bf9\u5355\u4e2aip\u8fdb\u884c\u5e76\u53d1\u8bf7\u6c42\u7684\u6700\u5927\u503c\uff0c\u5982\u679c\u975e0\uff0c\u5219\u5ffd\u7565 # \u662f\u5426\u8b66\u7528Cookie(\u9ed8\u8ba4\u542f\u7528) COOKIES_ENABLED = True COOKIES_DEBUG = True #\u7981\u7528Telnet\u63a7\u5236\u53f0\uff08\u9ed8\u8ba4\u542f\u7528\uff09 #TELNETCONSOLE_ENABLED = False # \u8986\u76d6\u9ed8\u8ba4\u8bf7\u6c42\u6807\u5934 #DEFAULT_REQUEST_HEADERS = { # 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', # 'Accept-Language': 'en', #} # \u542f\u7528\u6216\u7981\u6b62\u722c\u866b\u4e2d\u95f4\u4ef6 # SPIDER_MIDDLEWARES = { # 'ArticleSpider.middlewares.ArticlespiderSpiderMiddleware': 543, # } # \u542f\u7528\u6216\u7981\u6b62\u4e0b\u518d\u8d77\u4e2d\u95f4\u4ef6 DOWNLOADER_MIDDLEWARES = { # 'ArticleSpider.middlewares.JSPageMiddleware': 1, # 'ArticleSpider.middlewares.RandomUserAgentMiddlware': 543, # 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 2, 'ArticleSpider.middlewares.RandomProxyMiddleware' : 3 , } # \u542f\u7528\u6216\u7981\u6b62\u6216\u8005\u7a0b\u5e8f #EXTENSIONS = { # 'scrapy.extensions.telnet.TelnetConsole': None, #} # \u914d\u7f6e\u9879\u76ee\u7ba1\u9053 ITEM_PIPELINES = { # 'ArticleSpider.pipelines.DoubanspiderPipeline' : 300, # 'ArticleSpider.pipelines.ImagesPipeline': 1, # 'ArticleSpider.pipelines.DouyuPipeline':2, 'ArticleSpider.pipelines.JobblePipeline' : 300 , } # \u53ef\u914d\u7f6e RANDOM_UA_TYPE = \"random\" # \u8bbe\u7f6e\u6700\u5c0f\u957f\u5bbd\uff0c\u4e0d\u7136\u592a\u5c0f\u6ca1\u5565\u610f\u4e49 IMAGES_MIN_HEIGHT = 20 IMAGES_MIN_WIDTH = 20 # \u542f\u7528\u548c\u914d\u7f6eAuthThrottle\u6269\u5c55(\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u7981\u7528) AUTOTHROTTLE_ENABLED = True # \u521d\u59cb\u5316\u4e0b\u8f7d\u5ef6\u8fdf #AUTOTHROTTLE_START_DELAY = 5 # \u5728\u9ad8\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\u8bbe\u7f6e\u7684\u8db3\u5e95\u554a\u4e0b\u8f7d\u5ef6\u8fdf #AUTOTHROTTLE_MAX_DELAY = 60 #Scrapy\u8bf7\u6c42\u7684\u5e73\u5747\u6570\u91cf\u5e94\u8be5\u5e76\u884c\u53d1\u9001\u6bcf\u4e2a\u8fdc\u7a0b\u670d\u52a1\u5668 #AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0 # \u542f\u7528\u663e\u793a\u6240\u6536\u5230\u7684\u6bcf\u4e2a\u54cd\u5e94\u7684\u8c03\u8282\u7edf\u8ba1\u4fe1\u606f #AUTOTHROTTLE_DEBUG = False # \u542f\u7528\u548c\u914d\u7f6eHTTP\u7f13\u5b58\uff08\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u7981\u7528\uff09 #HTTPCACHE_ENABLED = True #HTTPCACHE_EXPIRATION_SECS = 0 #HTTPCACHE_DIR = 'httpcache' #HTTPCACHE_IGNORE_HTTP_CODES = [] #HTTPCACHE_STORAGE =","title":"scrapy \u7684setting \u7684\u5e38\u89c1\u914d\u7f6e"},{"location":"basics/scrapy/00_scrapy_setting/00_scrapy_setting/#_1","text":"\u4e24\u4e2a\u4e2d\u95f4\u4ef6 spider_middlewares \u548c downloader_middlewares \u7684\u533a\u522b\uff1f","title":"\u5e38\u89c1\u95ee\u9898"},{"location":"basics/scrapy/00_scrapy_setting/00_scrapy_setting/#_2","text":"https://zhuanlan.zhihu.com/p/42498126","title":"\u53c2\u8003\u94fe\u63a5"},{"location":"basics/scrapy/01_scrapy_request_response/01_scrapy_request_response/","text":"setting \u6e90\u7801 \u00b6 \u98df\u6307 \\color{blue}{\u7528\u7f8e\u4e3d\u7684\u96ea\u82b1\u5199\u4e0b\uff1a\u76f8\u4fe1\u672a\u6765\u3002} Request \u90e8\u5206\u6e90\u7801 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class Request ( object_ref ): def __init__ ( self , url , callback = None , method = 'GET' , headers = None , body = None , cookies = None , meta = None , encoding = 'utf-8' , priority = 0 , dont_filter = False , errback = None , flags = None , cb_kwargs = None ): self . _encoding = encoding # this one has to be set first self . method = str ( method ) . upper () self . _set_url ( url ) self . _set_body ( body ) assert isinstance ( priority , int ), \"Request priority not an integer: %r \" % priority self . priority = priority if callback is not None and not callable ( callback ): raise TypeError ( 'callback must be a callable, got %s ' % type ( callback ) . __name__ ) if errback is not None and not callable ( errback ): raise TypeError ( 'errback must be a callable, got %s ' % type ( errback ) . __name__ ) assert callback or not errback , \"Cannot use errback without a callback\" self . callback = callback self . errback = errback self . cookies = cookies or {} self . headers = Headers ( headers or {}, encoding = encoding ) self . dont_filter = dont_filter self . _meta = dict ( meta ) if meta else None self . _cb_kwargs = dict ( cb_kwargs ) if cb_kwargs else None self . flags = [] if flags is None else list ( flags ) @property def cb_kwargs ( self ): if self . _cb_kwargs is None : self . _cb_kwargs = {} return self . _cb_kwargs @property def meta ( self ): if self . _meta is None : self . _meta = {} return self . _meta def _get_url ( self ): return self . _url def _set_url ( self , url ): if not isinstance ( url , six . string_types ): raise TypeError ( 'Request url must be str or unicode, got %s :' % type ( url ) . __name__ ) s = safe_url_string ( url , self . encoding ) self . _url = escape_ajax ( s ) if ':' not in self . _url : raise ValueError ( 'Missing scheme in request url: %s ' % self . _url ) url = property ( _get_url , obsolete_setter ( _set_url , 'url' )) def _get_body ( self ): return self . _body def _set_body ( self , body ): if body is None : self . _body = b '' else : self . _body = to_bytes ( body , self . encoding ) body = property ( _get_body , obsolete_setter ( _set_body , 'body' )) @property def encoding ( self ): return self . _encoding def __str__ ( self ): return \"< %s %s >\" % ( self . method , self . url ) __repr__ = __str__ def copy ( self ): \"\"\"Return a copy of this Request\"\"\" return self . replace () def replace ( self , * args , ** kwargs ): for x in [ 'url' , 'method' , 'headers' , 'body' , 'cookies' , 'meta' , 'flags' , 'encoding' , 'priority' , 'dont_filter' , 'callback' , 'errback' , 'cb_kwargs' ]: kwargs . setdefault ( x , getattr ( self , x )) cls = kwargs . pop ( 'cls' , self . __class__ ) return cls ( * args , ** kwargs ) @classmethod def from_curl ( cls , curl_command , ignore_unknown_options = True , ** kwargs ): request_kwargs = curl_to_request_kwargs ( curl_command , ignore_unknown_options ) request_kwargs . update ( kwargs ) return cls ( ** request_kwargs ) \u53c2\u6570\u8bf4\u660e 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 url : \u5c31\u662f\u9700\u8981\u8bf7\u6c42\uff0c\u5e76\u8fdb\u884c\u4e0b\u4e00\u6b65\u5904\u7406\u7684 url callback : \u6307\u5b9a\u8be5\u8bf7\u6c42\u8fd4\u56de\u7684 Response \uff0c\u7531\u90a3\u4e2a\u51fd\u6570\u6765\u5904\u7406\u3002 method : \u8bf7\u6c42\u4e00\u822c\u4e0d\u9700\u8981\u6307\u5b9a\uff0c\u9ed8\u8ba4 GET\u65b9\u6cd5 \uff0c\u53ef\u8bbe\u7f6e\u4e3a \"GET\" , \"POST\" , \"PUT\" \u7b49\uff0c\u4e14\u4fdd\u8bc1\u5b57\u7b26\u4e32\u5927\u5199 headers : \u8bf7\u6c42\u65f6\uff0c\u5305\u542b\u7684\u5934\u6587\u4ef6\u3002\u4e00\u822c\u4e0d\u9700\u8981\u3002\u5185\u5bb9\u4e00\u822c\u5982\u4e0b\uff1a # \u81ea\u5df1\u5199\u8fc7\u722c\u866b\u7684\u80af\u5b9a\u77e5\u9053 Host : media . readthedocs . org User - Agent : Mozilla /5.0 (Windows NT 6.2; WOW64; rv:33.0) Gecko/20100101 Firefox/ 33.0 Accept : text /css,*/ *; q = 0.1 Accept - Language : zh - cn , zh ; q = 0.8 , en - us ; q = 0.5 , en ; q = 0.3 Accept - Encoding : gzip , deflate Referer : http :// scrapy - chs . readthedocs . org /zh_CN/0.24/ Cookie : _ga = GA1 . 2.1612165614 . 1415584110 ; Connection : keep - alive If - Modified - Since : Mon , 25 Aug 2014 21 : 59 : 35 GMT Cache - Control : max - age = 0 meta : \u6bd4\u8f83\u5e38\u7528\uff0c\u5728\u4e0d\u540c\u7684\u8bf7\u6c42\u4e4b\u95f4\u4f20\u9012\u6570\u636e\u4f7f\u7528\u7684\u3002\u5b57\u5178 dict\u578b request_with_cookies = Request ( url = \"http://www.example.com\" , cookies ={ 'currency' : 'USD' , 'country' : 'UY' }, meta ={ 'dont_merge_cookies' : True } ) encoding : \u4f7f\u7528\u9ed8\u8ba4\u7684 'utf-8' \u5c31\u884c\u3002 dont_filter : \u8868\u660e\u8be5\u8bf7\u6c42\u4e0d\u7531\u8c03\u5ea6\u5668\u8fc7\u6ee4\u3002\u8fd9\u662f\u5f53\u4f60\u60f3\u4f7f\u7528\u591a\u6b21\u6267\u884c\u76f8\u540c\u7684\u8bf7\u6c42 , \u5ffd\u7565\u91cd\u590d\u7684\u8fc7\u6ee4\u5668\u3002\u9ed8\u8ba4\u4e3a False \u3002 errback : \u6307\u5b9a\u9519\u8bef\u5904\u7406\u51fd\u6570 Response \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # \u90e8\u5206\u4ee3\u7801 class Response ( object_ref ): def __init__ ( self , url , status = 200 , headers = None , body = '' , flags = None , request = None ): self . headers = Headers ( headers or {}) self . status = int ( status ) self . _set_body ( body ) self . _set_url ( url ) self . request = request self . flags = [] if flags is None else list ( flags ) @property def meta ( self ): try : return self . request . meta except AttributeError : raise AttributeError ( \"Response.meta not available, this response \" \\ \"is not tied to any request\" ) \u5927\u90e8\u5206\u53c2\u6570\u548c\u4e0a\u9762\u7684\u5dee\u4e0d\u591a\uff1a 1 2 3 4 status : \u54cd\u5e94\u7801 _set_body ( body ) \uff1a \u54cd\u5e94\u4f53 _set_url ( url ) \uff1a\u54cd\u5e94 url self . request = request \u6211\u4eec\u7528\u8c46\u74e3250\u7684\u4f8b\u5b50\u4e3e\u4e2a\u4f8b\u5b50\u770b\u4e0bresponse\u5230\u5e95\u8fd4\u56de\u4e86\u4ec0\u4e48 \u53d1\u9001POST \u00b6 \u53ef\u4ee5\u4f7f\u7528 yield scrapy.FormRequest(url, formdata, callback)\u65b9\u6cd5\u53d1\u9001POST\u8bf7\u6c42\u3002 \u5982\u679c\u5e0c\u671b\u7a0b\u5e8f\u6267\u884c\u4e00\u5f00\u59cb\u5c31\u53d1\u9001POST\u8bf7\u6c42\uff0c\u53ef\u4ee5\u91cd\u5199Spider\u7c7b\u7684start_requests(self) \u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0d\u518d\u8c03\u7528start_urls\u91cc\u7684url\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # \u90e8\u5206\u6e90\u7801 class FormRequest ( Request ): valid_form_methods = [ 'GET' , 'POST' ] def __init__ ( self , * args , ** kwargs ): formdata = kwargs . pop ( 'formdata' , None ) if formdata and kwargs . get ( 'method' ) is None: kwargs [ 'method' ] = 'POST' super ( FormRequest , self ). __init__ (* args , ** kwargs ) if formdata: items = formdata . items () if isinstance ( formdata , dict ) else formdata querystr = _urlencode ( items , self . encoding ) if self . method == 'POST' : self . headers . setdefault ( b'Content-Type' , b'application /x- www-form-urlencoded' ) self . _set_body ( querystr ) else: self . _set_url ( self . url + ( '&' if '?' in self . url else '?' ) + querystr ) \u53c2\u6570\u8bf4\u660e\uff1a 1 formdata : \u5b57\u5178\uff0c\u8bf7\u6c42\u53c2\u6570 \u4e3e\u4f8b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class mySpider ( scrapy . Spider ) : # start_urls = [\"http: //www.example.com/\"] def start_requests ( self ) : url = ' http : //www.renren.com/PLogin.do' # FormRequest \u662fScrapy\u53d1\u9001POST\u8bf7\u6c42\u7684\u65b9\u6cd5 yield scrapy . FormRequest ( url = url , formdata = { \"email\" : \"mr_mao_hacker@163.com\" , \"password\" : \"axxxxxxxe\" }, callback = self . parse_page ) def parse_page ( self , response ) : # do something \u7b80\u5355\u6a21\u62df\u767b\u9646 \u00b6 \u4f7f\u7528FormRequest.from_response()\u65b9\u6cd5\u6a21\u62df\u7528\u6237\u767b\u5f55 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # \u90e8\u5206\u4ee3\u7801 import scrapy class LoginSpider ( scrapy . Spider ): name = 'example.com' start_urls = [ 'http://www.example.com/users/login.php' ] def parse ( self , response ): return scrapy . FormRequest . from_response ( response , formdata = { 'username' : 'john' , 'password' : 'secret' }, callback = self . after_login ) def after_login ( self , response ): # check login succeed before going on if \"authentication failed\" in response . body : self . log ( \"Login failed\" , level = log . ERROR ) return # continue scraping with authenticated session...","title":"Scrapy \u6e90\u7801 request\u3001response"},{"location":"basics/scrapy/01_scrapy_request_response/01_scrapy_request_response/#setting","text":"\u98df\u6307 \\color{blue}{\u7528\u7f8e\u4e3d\u7684\u96ea\u82b1\u5199\u4e0b\uff1a\u76f8\u4fe1\u672a\u6765\u3002}","title":"setting \u6e90\u7801"},{"location":"basics/scrapy/01_scrapy_request_response/01_scrapy_request_response/#request","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class Request ( object_ref ): def __init__ ( self , url , callback = None , method = 'GET' , headers = None , body = None , cookies = None , meta = None , encoding = 'utf-8' , priority = 0 , dont_filter = False , errback = None , flags = None , cb_kwargs = None ): self . _encoding = encoding # this one has to be set first self . method = str ( method ) . upper () self . _set_url ( url ) self . _set_body ( body ) assert isinstance ( priority , int ), \"Request priority not an integer: %r \" % priority self . priority = priority if callback is not None and not callable ( callback ): raise TypeError ( 'callback must be a callable, got %s ' % type ( callback ) . __name__ ) if errback is not None and not callable ( errback ): raise TypeError ( 'errback must be a callable, got %s ' % type ( errback ) . __name__ ) assert callback or not errback , \"Cannot use errback without a callback\" self . callback = callback self . errback = errback self . cookies = cookies or {} self . headers = Headers ( headers or {}, encoding = encoding ) self . dont_filter = dont_filter self . _meta = dict ( meta ) if meta else None self . _cb_kwargs = dict ( cb_kwargs ) if cb_kwargs else None self . flags = [] if flags is None else list ( flags ) @property def cb_kwargs ( self ): if self . _cb_kwargs is None : self . _cb_kwargs = {} return self . _cb_kwargs @property def meta ( self ): if self . _meta is None : self . _meta = {} return self . _meta def _get_url ( self ): return self . _url def _set_url ( self , url ): if not isinstance ( url , six . string_types ): raise TypeError ( 'Request url must be str or unicode, got %s :' % type ( url ) . __name__ ) s = safe_url_string ( url , self . encoding ) self . _url = escape_ajax ( s ) if ':' not in self . _url : raise ValueError ( 'Missing scheme in request url: %s ' % self . _url ) url = property ( _get_url , obsolete_setter ( _set_url , 'url' )) def _get_body ( self ): return self . _body def _set_body ( self , body ): if body is None : self . _body = b '' else : self . _body = to_bytes ( body , self . encoding ) body = property ( _get_body , obsolete_setter ( _set_body , 'body' )) @property def encoding ( self ): return self . _encoding def __str__ ( self ): return \"< %s %s >\" % ( self . method , self . url ) __repr__ = __str__ def copy ( self ): \"\"\"Return a copy of this Request\"\"\" return self . replace () def replace ( self , * args , ** kwargs ): for x in [ 'url' , 'method' , 'headers' , 'body' , 'cookies' , 'meta' , 'flags' , 'encoding' , 'priority' , 'dont_filter' , 'callback' , 'errback' , 'cb_kwargs' ]: kwargs . setdefault ( x , getattr ( self , x )) cls = kwargs . pop ( 'cls' , self . __class__ ) return cls ( * args , ** kwargs ) @classmethod def from_curl ( cls , curl_command , ignore_unknown_options = True , ** kwargs ): request_kwargs = curl_to_request_kwargs ( curl_command , ignore_unknown_options ) request_kwargs . update ( kwargs ) return cls ( ** request_kwargs ) \u53c2\u6570\u8bf4\u660e 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 url : \u5c31\u662f\u9700\u8981\u8bf7\u6c42\uff0c\u5e76\u8fdb\u884c\u4e0b\u4e00\u6b65\u5904\u7406\u7684 url callback : \u6307\u5b9a\u8be5\u8bf7\u6c42\u8fd4\u56de\u7684 Response \uff0c\u7531\u90a3\u4e2a\u51fd\u6570\u6765\u5904\u7406\u3002 method : \u8bf7\u6c42\u4e00\u822c\u4e0d\u9700\u8981\u6307\u5b9a\uff0c\u9ed8\u8ba4 GET\u65b9\u6cd5 \uff0c\u53ef\u8bbe\u7f6e\u4e3a \"GET\" , \"POST\" , \"PUT\" \u7b49\uff0c\u4e14\u4fdd\u8bc1\u5b57\u7b26\u4e32\u5927\u5199 headers : \u8bf7\u6c42\u65f6\uff0c\u5305\u542b\u7684\u5934\u6587\u4ef6\u3002\u4e00\u822c\u4e0d\u9700\u8981\u3002\u5185\u5bb9\u4e00\u822c\u5982\u4e0b\uff1a # \u81ea\u5df1\u5199\u8fc7\u722c\u866b\u7684\u80af\u5b9a\u77e5\u9053 Host : media . readthedocs . org User - Agent : Mozilla /5.0 (Windows NT 6.2; WOW64; rv:33.0) Gecko/20100101 Firefox/ 33.0 Accept : text /css,*/ *; q = 0.1 Accept - Language : zh - cn , zh ; q = 0.8 , en - us ; q = 0.5 , en ; q = 0.3 Accept - Encoding : gzip , deflate Referer : http :// scrapy - chs . readthedocs . org /zh_CN/0.24/ Cookie : _ga = GA1 . 2.1612165614 . 1415584110 ; Connection : keep - alive If - Modified - Since : Mon , 25 Aug 2014 21 : 59 : 35 GMT Cache - Control : max - age = 0 meta : \u6bd4\u8f83\u5e38\u7528\uff0c\u5728\u4e0d\u540c\u7684\u8bf7\u6c42\u4e4b\u95f4\u4f20\u9012\u6570\u636e\u4f7f\u7528\u7684\u3002\u5b57\u5178 dict\u578b request_with_cookies = Request ( url = \"http://www.example.com\" , cookies ={ 'currency' : 'USD' , 'country' : 'UY' }, meta ={ 'dont_merge_cookies' : True } ) encoding : \u4f7f\u7528\u9ed8\u8ba4\u7684 'utf-8' \u5c31\u884c\u3002 dont_filter : \u8868\u660e\u8be5\u8bf7\u6c42\u4e0d\u7531\u8c03\u5ea6\u5668\u8fc7\u6ee4\u3002\u8fd9\u662f\u5f53\u4f60\u60f3\u4f7f\u7528\u591a\u6b21\u6267\u884c\u76f8\u540c\u7684\u8bf7\u6c42 , \u5ffd\u7565\u91cd\u590d\u7684\u8fc7\u6ee4\u5668\u3002\u9ed8\u8ba4\u4e3a False \u3002 errback : \u6307\u5b9a\u9519\u8bef\u5904\u7406\u51fd\u6570","title":"Request \u90e8\u5206\u6e90\u7801"},{"location":"basics/scrapy/01_scrapy_request_response/01_scrapy_request_response/#response","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # \u90e8\u5206\u4ee3\u7801 class Response ( object_ref ): def __init__ ( self , url , status = 200 , headers = None , body = '' , flags = None , request = None ): self . headers = Headers ( headers or {}) self . status = int ( status ) self . _set_body ( body ) self . _set_url ( url ) self . request = request self . flags = [] if flags is None else list ( flags ) @property def meta ( self ): try : return self . request . meta except AttributeError : raise AttributeError ( \"Response.meta not available, this response \" \\ \"is not tied to any request\" ) \u5927\u90e8\u5206\u53c2\u6570\u548c\u4e0a\u9762\u7684\u5dee\u4e0d\u591a\uff1a 1 2 3 4 status : \u54cd\u5e94\u7801 _set_body ( body ) \uff1a \u54cd\u5e94\u4f53 _set_url ( url ) \uff1a\u54cd\u5e94 url self . request = request \u6211\u4eec\u7528\u8c46\u74e3250\u7684\u4f8b\u5b50\u4e3e\u4e2a\u4f8b\u5b50\u770b\u4e0bresponse\u5230\u5e95\u8fd4\u56de\u4e86\u4ec0\u4e48","title":"Response"},{"location":"basics/scrapy/01_scrapy_request_response/01_scrapy_request_response/#post","text":"\u53ef\u4ee5\u4f7f\u7528 yield scrapy.FormRequest(url, formdata, callback)\u65b9\u6cd5\u53d1\u9001POST\u8bf7\u6c42\u3002 \u5982\u679c\u5e0c\u671b\u7a0b\u5e8f\u6267\u884c\u4e00\u5f00\u59cb\u5c31\u53d1\u9001POST\u8bf7\u6c42\uff0c\u53ef\u4ee5\u91cd\u5199Spider\u7c7b\u7684start_requests(self) \u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0d\u518d\u8c03\u7528start_urls\u91cc\u7684url\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # \u90e8\u5206\u6e90\u7801 class FormRequest ( Request ): valid_form_methods = [ 'GET' , 'POST' ] def __init__ ( self , * args , ** kwargs ): formdata = kwargs . pop ( 'formdata' , None ) if formdata and kwargs . get ( 'method' ) is None: kwargs [ 'method' ] = 'POST' super ( FormRequest , self ). __init__ (* args , ** kwargs ) if formdata: items = formdata . items () if isinstance ( formdata , dict ) else formdata querystr = _urlencode ( items , self . encoding ) if self . method == 'POST' : self . headers . setdefault ( b'Content-Type' , b'application /x- www-form-urlencoded' ) self . _set_body ( querystr ) else: self . _set_url ( self . url + ( '&' if '?' in self . url else '?' ) + querystr ) \u53c2\u6570\u8bf4\u660e\uff1a 1 formdata : \u5b57\u5178\uff0c\u8bf7\u6c42\u53c2\u6570 \u4e3e\u4f8b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class mySpider ( scrapy . Spider ) : # start_urls = [\"http: //www.example.com/\"] def start_requests ( self ) : url = ' http : //www.renren.com/PLogin.do' # FormRequest \u662fScrapy\u53d1\u9001POST\u8bf7\u6c42\u7684\u65b9\u6cd5 yield scrapy . FormRequest ( url = url , formdata = { \"email\" : \"mr_mao_hacker@163.com\" , \"password\" : \"axxxxxxxe\" }, callback = self . parse_page ) def parse_page ( self , response ) : # do something","title":"\u53d1\u9001POST"},{"location":"basics/scrapy/01_scrapy_request_response/01_scrapy_request_response/#_1","text":"\u4f7f\u7528FormRequest.from_response()\u65b9\u6cd5\u6a21\u62df\u7528\u6237\u767b\u5f55 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # \u90e8\u5206\u4ee3\u7801 import scrapy class LoginSpider ( scrapy . Spider ): name = 'example.com' start_urls = [ 'http://www.example.com/users/login.php' ] def parse ( self , response ): return scrapy . FormRequest . from_response ( response , formdata = { 'username' : 'john' , 'password' : 'secret' }, callback = self . after_login ) def after_login ( self , response ): # check login succeed before going on if \"authentication failed\" in response . body : self . log ( \"Login failed\" , level = log . ERROR ) return # continue scraping with authenticated session...","title":"\u7b80\u5355\u6a21\u62df\u767b\u9646"},{"location":"basics/scrapy/02_scrapy_spider/02_scrapy_spider/","text":"\u524d\u8a00 \u00b6 Spider\u7c7b\u5b9a\u4e49\u4e86\u5982\u4f55\u722c\u53d6\u67d0\u4e2a(\u6216\u67d0\u4e9b)\u7f51\u7ad9\u3002\u5305\u62ec\u4e86\u722c\u53d6\u7684\u52a8\u4f5c(\u4f8b\u5982:\u662f\u5426\u8ddf\u8fdb\u94fe\u63a5)\u4ee5\u53ca\u5982\u4f55\u4ece\u7f51\u9875\u7684\u5185\u5bb9\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e(\u722c\u53d6item)\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0cSpider\u5c31\u662f\u60a8\u5b9a\u4e49\u722c\u53d6\u7684\u52a8\u4f5c\u53ca\u5206\u6790\u67d0\u4e2a\u7f51\u9875(\u6216\u8005\u662f\u6709\u4e9b\u7f51\u9875)\u7684\u5730\u65b9\u3002 class scrapy.Spider\u662f\u6700\u57fa\u672c\u7684\u7c7b\uff0c\u6240\u6709\u7f16\u5199\u7684\u722c\u866b\u5fc5\u987b\u7ee7\u627f\u8fd9\u4e2a\u7c7b\u3002 \u6e90\u7801\u53c2\u8003 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # \u6240\u6709\u722c\u866b\u7684\u57fa\u7c7b\uff0c\u7528\u6237\u5b9a\u4e49\u7684\u722c\u866b\u5fc5\u987b\u4ece\u8fd9\u4e2a\u7c7b\u7ee7\u627f class Spider ( object_ref ): \"\"\"Base class for scrapy spiders. All spiders must inherit from this class. \"\"\" # \u5b9a\u4e49spider\u540d\u5b57\u7684\u5b57\u7b26\u4e32(string) ,spider\u7684\u540d\u5b57\u5b9a\u4e49\u4e86Scrapy\u5982\u4f55\u5b9a\u4f4d(\u5e76\u521d\u59cb\u5316)spider,\u6240\u4ee5\u5176\u5fc5\u987b\u662f\u552f\u4e00\u7684 # name\u662fspider\u6700\u91cd\u8981\u7684\u5c5e\u6027\uff0c\u800c\u4e14\u662f\u5fc5\u987b\u7684 # \u4e00\u822c\u505a\u6cd5\u662f\u4ee5\u8be5\u7f51\u7ad9(domain)(\u52a0\u6216\u4e0d\u52a0 \u540e\u7f00 )\u6765\u547d\u540dspider\u3002 \u4f8b\u5982\uff0c\u5982\u679cspider\u722c\u53d6 mywebsite.com \uff0c\u8be5spider\u901a\u5e38\u4f1a\u88ab\u547d\u540d\u4e3a mywebsite name = None custom_settings = None #\u521d\u59cb\u5316\uff0c\u63d0\u53d6\u722c\u866b\u540d\u5b57\uff0cstart_ruls def __init__ ( self , name = None , ** kwargs ): if name is not None: self . name = name # \u5982\u679c\u722c\u866b\u6ca1\u6709\u540d\u5b57\uff0c\u4e2d\u65ad\u540e\u7eed\u64cd\u4f5c\u5219\u62a5\u9519 elif not getattr ( self , 'name' , None ): raise ValueError ( \"%s must have a name\" % type ( self ). __name__ ) # python \u5bf9\u8c61\u6216\u7c7b\u578b\u901a\u8fc7\u5185\u7f6e\u6210\u5458__dict__\u6765\u5b58\u50a8\u6210\u5458\u4fe1\u606f self . __dict__ . update ( kwargs ) #URL\u5217\u8868\u3002\u5f53\u6ca1\u6709\u6307\u5b9a\u7684URL\u65f6\uff0cspider\u5c06\u4ece\u8be5\u5217\u8868\u4e2d\u5f00\u59cb\u8fdb\u884c\u722c\u53d6\u3002 \u56e0\u6b64\uff0c\u7b2c\u4e00\u4e2a\u88ab\u83b7\u53d6\u5230\u7684\u9875\u9762\u7684URL\u5c06\u662f\u8be5\u5217\u8868\u4e4b\u4e00\u3002 \u540e\u7eed\u7684URL\u5c06\u4f1a\u4ece\u83b7\u53d6\u5230\u7684\u6570\u636e\u4e2d\u63d0\u53d6\u3002 if not hasattr ( self , 'start_urls' ): self . start_urls = [] # \u6253\u5370Scrapy\u6267\u884c\u540e\u7684log\u4fe1\u606f,\u662f\u975e\u5e38\u91cd\u8981\u7684 def log ( self , message , level = logging . DEBUG , ** kw ): pass #\u8be5\u65b9\u6cd5\u5c06\u8bfb\u53d6start_urls\u5185\u7684\u5730\u5740\uff0c\u5e76\u4e3a\u6bcf\u4e00\u4e2a\u5730\u5740\u751f\u6210\u4e00\u4e2aRequest\u5bf9\u8c61\uff0c\u4ea4\u7ed9Scrapy\u4e0b\u8f7d\u5e76\u8fd4\u56deResponse #\u8be5\u65b9\u6cd5\u4ec5\u8c03\u7528\u4e00\u6b21 def start_requests ( self ): for url in self . start_urls: yield Request ( url , dont_filter = True ) #start_requests()\u4e2d\u8c03\u7528\uff0c\u5b9e\u9645\u751f\u6210Request\u7684\u51fd\u6570\u3002 #Request\u5bf9\u8c61\u9ed8\u8ba4\u7684\u56de\u8c03\u51fd\u6570\u4e3aparse()\uff0c\u63d0\u4ea4\u7684\u65b9\u5f0f\u4e3aget def make_requests_from_url ( self , url ): return Request ( url , dont_filter = True ) #\u9ed8\u8ba4\u7684Request\u5bf9\u8c61\u56de\u8c03\u51fd\u6570\uff0c\u5904\u7406\u8fd4\u56de\u7684response\u3002 #\u751f\u6210Item\u6216\u8005Request\u5bf9\u8c61\u3002\u7528\u6237\u5fc5\u987b\u5b9e\u73b0\u8fd9\u4e2a\u7c7b def parse ( self , response ): raise NotImplementedError ( '{}.parse callback is not defined' . format ( self . __class__ . __name__ )) \u4e3b\u8981\u903b\u8f91 1 2 3 4 5 __init__ () : \u521d\u59cb\u5316\u722c\u866b\u540d\u5b57\u548c start_urls\u5217\u8868 start_requests () \u8c03\u7528 make_requests_from url (): \u751f\u6210 Requests\u5bf9\u8c61\u4ea4\u7ed9Scrapy\u4e0b\u8f7d\u5e76\u8fd4\u56deresponse parse () : \u89e3\u6790 response \uff0c\u5e76\u8fd4\u56de Item\u6216Requests \uff08\u9700\u6307\u5b9a\u56de\u8c03\u51fd\u6570\uff09\u3002 Item\u4f20\u7ed9Item pipline\u6301\u4e45\u5316 \uff0c \u800c Requests\u4ea4\u7531Scrapy\u4e0b\u8f7d \uff0c\u5e76\u7531\u6307\u5b9a\u7684\u56de\u8c03\u51fd\u6570\u5904\u7406\uff08\u9ed8\u8ba4 parse ()) \uff0c\u4e00\u76f4\u8fdb\u884c\u5faa\u73af\uff0c\u76f4\u5230\u5904\u7406\u5b8c\u6240\u6709\u7684\u6570\u636e\u4e3a\u6b62\u3002 \u53c2\u6570\u89e3\u91ca \u00b6 name 1 \u5fc5\u987b\u5b9a\u4e49\uff0c\u722c\u866b\u7684\u540d\u79f0\uff0c\u5b57\u7b26\u4e32\u7c7b\u578b\uff0c\u8fd9\u4e2a\u540d\u79f0\u6807\u8bc6\u8fd9\u4e2a\u722c\u866b\uff0c\u6240\u4ee5\u4e0d\u80fd\u91cd\u590d start_urls 1 list\u7c7b\u578b\uff0c\u5f00\u59cb\u722c\u866b\u7684URL\u5217\u8868(\u4e00\u4e2a\u6216\u8005\u591a\u4e2a)\uff0c\u53ef\u4ee5\u4e0d\u7528\u5b9a\u4e49\uff0c\u7136\u540e\u7528start_request()\u51fd\u6570\u4ee3\u66ff\uff0c\u53bb\u5faa\u73af\u5217\u8868\u4e2d\u7684url\u53bbrequest custom_settings 1 \u5b57\u5178\u7c7b\u578b\uff0c\u53ef\u4ee5\u8bbe\u7f6esetting\u4e2d\u7684\u503c\u3002\u9488\u5bf9\u4e0d\u540c\u7684\u722c\u866b\u8bbe\u7f6e\u4e0d\u540c\u7684\u503c\uff0c\u56e0\u4e3asettings\u6587\u4ef6\u53ea\u6709\u4e00\u4e2a 1 2 3 4 custom_settings = { 'ROBOTSTXT_OBEY' : False, 'DOWNLOAD_DELAY' : 5 } \u5c0f\u6280\u5de7\uff1a\u8bbe\u7f6esetting\u503c\u7684\u4f18\u5148\u7ea7 1. \u547d\u4ee4\u884c\u9009\u9879 2. custom_settings 3. setting.py\u6587\u4ef6 4. \u5168\u5c40\u9ed8\u8ba4 \u601d\u8003\u70b9 1. \u4e3a\u4ec0\u4e48\u6211\u4eec\u7ee7\u627f\u9700\u8981\u5b9e\u73b0parse\u65b9\u6cd5\uff0c\u6709\u4e0d\u5b9e\u73b0\u7684\u65b9\u6cd5\u5417\uff1f 2. \u5f88\u591a\u65f6\u5019\u9700\u8981\u767b\u5f55\u600e\u4e48\u7ee7\u627f\u91cd\u5199\u65b9\u6cd5 3. \u5982\u679c\u6211\u4eec\u6ce8\u91ca\u4e86setting\u4e2d\u7684\u67d0\u4e9b\u53c2\u6570\uff0c\u4f46\u662f\u5728custom_setting\u4e2d\u5374\u8bbe\u7f6e\u4e86\u6709\u7528\u5417\uff0c\u6bd4\u5982\u4e2d\u95f4\u4ef6 4. \u56de\u7b54 \u8fd9\u91cc\u6211\u4eec\u53ef\u4ee5\u91cd\u5199start_requests()\u65b9\u6cd5 \u8fd9\u91cc\u6211\u4eec\u4e3e\u4e2a\u767b\u5f55github\u7684\u4f8b\u5b50 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class GithubSpider ( Spider ): name = \"github_login\" allowed_domains = [ \"github.com\" ] url = 'https://github.com/login' start_urls = [ url ] post_headers = { \"Accept\" : \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\" , \"Accept-Encoding\" : \"gzip, deflate\" , \"Accept-Language\" : \"zh-CN,zh;q=0.8,en;q=0.6\" , \"Cache-Control\" : \"no-cache\" , \"Connection\" : \"keep-alive\" , \"Content-Type\" : \"application/x-www-form-urlencoded\" , \"User-Agent\" : \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.75 Safari/537.36\" , \"Referer\" : \"https://github.com/\" , } def start_requests ( self ): return [ Request ( \"https://github.com/login\" , meta = { 'cookiejar' : 1 }, callback = self . post_login )] # FormRequeset def post_login ( self , response ): # \u5148\u53bb\u62ff\u9690\u85cf\u7684\u8868\u5355\u53c2\u6570authenticity_token authenticity_token = response . xpath ( '//input[@name=\"authenticity_token\"]/@value' ) . extract_first () logging . info ( 'authenticity_token=' + authenticity_token ) # FormRequeset.from_response\u662fScrapy\u63d0\u4f9b\u7684\u4e00\u4e2a\u51fd\u6570, \u7528\u4e8epost\u8868\u5355 # \u767b\u9646\u6210\u529f\u540e, \u4f1a\u8c03\u7528after_login\u56de\u8c03\u51fd\u6570\uff0c\u5982\u679curl\u8ddfRequest\u9875\u9762\u7684\u4e00\u6837\u5c31\u7701\u7565\u6389 return [ FormRequest . from_response ( response , url = 'https://github.com/session' , meta = { 'cookiejar' : response . meta [ 'cookiejar' ]}, headers = self . post_headers , # \u6ce8\u610f\u6b64\u5904\u7684headers formdata = { 'utf8' : '\u2713' , 'login' : '' , 'password' : '' , 'authenticity_token' : authenticity_token }, callback = self . after_login , dont_filter = True )] def after_login ( self , response ): # \u767b\u5f55\u4e4b\u540e\uff0c\u5f00\u59cb\u8fdb\u5165\u6211\u8981\u722c\u53d6\u7684\u79c1\u4fe1\u9875\u9762 for url in self . start_urls : # \u56e0\u4e3a\u6211\u4eec\u4e0a\u9762\u5b9a\u4e49\u4e86Rule\uff0c\u6240\u4ee5\u53ea\u9700\u8981\u7b80\u5355\u7684\u751f\u6210\u521d\u59cb\u722c\u53d6Request\u5373\u53ef yield Request ( url , meta = { 'cookiejar' : response . meta [ 'cookiejar' ]}) \u8fd9\u662f\u4e00\u4e2a\u6807\u51c6\u7684\u767b\u5f55\u6a21\u677f\u5177\u4f53\u6b65\u9aa4\u5c31\u662f - \u5148\u8bbf\u95ee\u767b\u5f55\u9875\u9762 - \u7136\u540e\u8868\u5355\u63d0\u4ea4 - \u6b63\u5e38\u9875\u9762\u4e1a\u52a1\u903b\u8f91\u4e66\u5199 3. 1 2 3 4 5 6 7 8 9 10 11 \u4e00\u822c\u5728\u4e00\u4e2a\u9879\u76ee\u4e2d\u6709\u591a\u4e2a\u722c\u866b\u7684\u65f6\u5019\u6211\u4eec\u4e60\u60ef\u628a\u516c\u5171\u7684\u653e\u5728 setting\u4e2d \uff0c\u6bcf\u4e2a\u722c\u866b\u79c1\u6709\u7684\u5c31\u653e\u5728 custom_settings\u4e2d \u6bd4\u5982\u4e2d\u95f4\u4ef6\u7684\u95ee\u9898\uff0c\u6709\u7684\u9700\u8981\u4ee3\u7406\uff0c\u800c\u6709\u7684\u4e0d\u9700\u8981\uff0c\u90a3\u600e\u4e48\u529e\u5462 custom_settings = { \"DOWNLOADER_MIDDLEWARES\" : { 'a.middlewares.ProxyMiddleware' : 543 , } }","title":"Scrapy \u6e90\u7801 Spider"},{"location":"basics/scrapy/02_scrapy_spider/02_scrapy_spider/#_1","text":"Spider\u7c7b\u5b9a\u4e49\u4e86\u5982\u4f55\u722c\u53d6\u67d0\u4e2a(\u6216\u67d0\u4e9b)\u7f51\u7ad9\u3002\u5305\u62ec\u4e86\u722c\u53d6\u7684\u52a8\u4f5c(\u4f8b\u5982:\u662f\u5426\u8ddf\u8fdb\u94fe\u63a5)\u4ee5\u53ca\u5982\u4f55\u4ece\u7f51\u9875\u7684\u5185\u5bb9\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e(\u722c\u53d6item)\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0cSpider\u5c31\u662f\u60a8\u5b9a\u4e49\u722c\u53d6\u7684\u52a8\u4f5c\u53ca\u5206\u6790\u67d0\u4e2a\u7f51\u9875(\u6216\u8005\u662f\u6709\u4e9b\u7f51\u9875)\u7684\u5730\u65b9\u3002 class scrapy.Spider\u662f\u6700\u57fa\u672c\u7684\u7c7b\uff0c\u6240\u6709\u7f16\u5199\u7684\u722c\u866b\u5fc5\u987b\u7ee7\u627f\u8fd9\u4e2a\u7c7b\u3002","title":"\u524d\u8a00"},{"location":"basics/scrapy/02_scrapy_spider/02_scrapy_spider/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # \u6240\u6709\u722c\u866b\u7684\u57fa\u7c7b\uff0c\u7528\u6237\u5b9a\u4e49\u7684\u722c\u866b\u5fc5\u987b\u4ece\u8fd9\u4e2a\u7c7b\u7ee7\u627f class Spider ( object_ref ): \"\"\"Base class for scrapy spiders. All spiders must inherit from this class. \"\"\" # \u5b9a\u4e49spider\u540d\u5b57\u7684\u5b57\u7b26\u4e32(string) ,spider\u7684\u540d\u5b57\u5b9a\u4e49\u4e86Scrapy\u5982\u4f55\u5b9a\u4f4d(\u5e76\u521d\u59cb\u5316)spider,\u6240\u4ee5\u5176\u5fc5\u987b\u662f\u552f\u4e00\u7684 # name\u662fspider\u6700\u91cd\u8981\u7684\u5c5e\u6027\uff0c\u800c\u4e14\u662f\u5fc5\u987b\u7684 # \u4e00\u822c\u505a\u6cd5\u662f\u4ee5\u8be5\u7f51\u7ad9(domain)(\u52a0\u6216\u4e0d\u52a0 \u540e\u7f00 )\u6765\u547d\u540dspider\u3002 \u4f8b\u5982\uff0c\u5982\u679cspider\u722c\u53d6 mywebsite.com \uff0c\u8be5spider\u901a\u5e38\u4f1a\u88ab\u547d\u540d\u4e3a mywebsite name = None custom_settings = None #\u521d\u59cb\u5316\uff0c\u63d0\u53d6\u722c\u866b\u540d\u5b57\uff0cstart_ruls def __init__ ( self , name = None , ** kwargs ): if name is not None: self . name = name # \u5982\u679c\u722c\u866b\u6ca1\u6709\u540d\u5b57\uff0c\u4e2d\u65ad\u540e\u7eed\u64cd\u4f5c\u5219\u62a5\u9519 elif not getattr ( self , 'name' , None ): raise ValueError ( \"%s must have a name\" % type ( self ). __name__ ) # python \u5bf9\u8c61\u6216\u7c7b\u578b\u901a\u8fc7\u5185\u7f6e\u6210\u5458__dict__\u6765\u5b58\u50a8\u6210\u5458\u4fe1\u606f self . __dict__ . update ( kwargs ) #URL\u5217\u8868\u3002\u5f53\u6ca1\u6709\u6307\u5b9a\u7684URL\u65f6\uff0cspider\u5c06\u4ece\u8be5\u5217\u8868\u4e2d\u5f00\u59cb\u8fdb\u884c\u722c\u53d6\u3002 \u56e0\u6b64\uff0c\u7b2c\u4e00\u4e2a\u88ab\u83b7\u53d6\u5230\u7684\u9875\u9762\u7684URL\u5c06\u662f\u8be5\u5217\u8868\u4e4b\u4e00\u3002 \u540e\u7eed\u7684URL\u5c06\u4f1a\u4ece\u83b7\u53d6\u5230\u7684\u6570\u636e\u4e2d\u63d0\u53d6\u3002 if not hasattr ( self , 'start_urls' ): self . start_urls = [] # \u6253\u5370Scrapy\u6267\u884c\u540e\u7684log\u4fe1\u606f,\u662f\u975e\u5e38\u91cd\u8981\u7684 def log ( self , message , level = logging . DEBUG , ** kw ): pass #\u8be5\u65b9\u6cd5\u5c06\u8bfb\u53d6start_urls\u5185\u7684\u5730\u5740\uff0c\u5e76\u4e3a\u6bcf\u4e00\u4e2a\u5730\u5740\u751f\u6210\u4e00\u4e2aRequest\u5bf9\u8c61\uff0c\u4ea4\u7ed9Scrapy\u4e0b\u8f7d\u5e76\u8fd4\u56deResponse #\u8be5\u65b9\u6cd5\u4ec5\u8c03\u7528\u4e00\u6b21 def start_requests ( self ): for url in self . start_urls: yield Request ( url , dont_filter = True ) #start_requests()\u4e2d\u8c03\u7528\uff0c\u5b9e\u9645\u751f\u6210Request\u7684\u51fd\u6570\u3002 #Request\u5bf9\u8c61\u9ed8\u8ba4\u7684\u56de\u8c03\u51fd\u6570\u4e3aparse()\uff0c\u63d0\u4ea4\u7684\u65b9\u5f0f\u4e3aget def make_requests_from_url ( self , url ): return Request ( url , dont_filter = True ) #\u9ed8\u8ba4\u7684Request\u5bf9\u8c61\u56de\u8c03\u51fd\u6570\uff0c\u5904\u7406\u8fd4\u56de\u7684response\u3002 #\u751f\u6210Item\u6216\u8005Request\u5bf9\u8c61\u3002\u7528\u6237\u5fc5\u987b\u5b9e\u73b0\u8fd9\u4e2a\u7c7b def parse ( self , response ): raise NotImplementedError ( '{}.parse callback is not defined' . format ( self . __class__ . __name__ )) \u4e3b\u8981\u903b\u8f91 1 2 3 4 5 __init__ () : \u521d\u59cb\u5316\u722c\u866b\u540d\u5b57\u548c start_urls\u5217\u8868 start_requests () \u8c03\u7528 make_requests_from url (): \u751f\u6210 Requests\u5bf9\u8c61\u4ea4\u7ed9Scrapy\u4e0b\u8f7d\u5e76\u8fd4\u56deresponse parse () : \u89e3\u6790 response \uff0c\u5e76\u8fd4\u56de Item\u6216Requests \uff08\u9700\u6307\u5b9a\u56de\u8c03\u51fd\u6570\uff09\u3002 Item\u4f20\u7ed9Item pipline\u6301\u4e45\u5316 \uff0c \u800c Requests\u4ea4\u7531Scrapy\u4e0b\u8f7d \uff0c\u5e76\u7531\u6307\u5b9a\u7684\u56de\u8c03\u51fd\u6570\u5904\u7406\uff08\u9ed8\u8ba4 parse ()) \uff0c\u4e00\u76f4\u8fdb\u884c\u5faa\u73af\uff0c\u76f4\u5230\u5904\u7406\u5b8c\u6240\u6709\u7684\u6570\u636e\u4e3a\u6b62\u3002","title":"\u6e90\u7801\u53c2\u8003"},{"location":"basics/scrapy/02_scrapy_spider/02_scrapy_spider/#_3","text":"name 1 \u5fc5\u987b\u5b9a\u4e49\uff0c\u722c\u866b\u7684\u540d\u79f0\uff0c\u5b57\u7b26\u4e32\u7c7b\u578b\uff0c\u8fd9\u4e2a\u540d\u79f0\u6807\u8bc6\u8fd9\u4e2a\u722c\u866b\uff0c\u6240\u4ee5\u4e0d\u80fd\u91cd\u590d start_urls 1 list\u7c7b\u578b\uff0c\u5f00\u59cb\u722c\u866b\u7684URL\u5217\u8868(\u4e00\u4e2a\u6216\u8005\u591a\u4e2a)\uff0c\u53ef\u4ee5\u4e0d\u7528\u5b9a\u4e49\uff0c\u7136\u540e\u7528start_request()\u51fd\u6570\u4ee3\u66ff\uff0c\u53bb\u5faa\u73af\u5217\u8868\u4e2d\u7684url\u53bbrequest custom_settings 1 \u5b57\u5178\u7c7b\u578b\uff0c\u53ef\u4ee5\u8bbe\u7f6esetting\u4e2d\u7684\u503c\u3002\u9488\u5bf9\u4e0d\u540c\u7684\u722c\u866b\u8bbe\u7f6e\u4e0d\u540c\u7684\u503c\uff0c\u56e0\u4e3asettings\u6587\u4ef6\u53ea\u6709\u4e00\u4e2a 1 2 3 4 custom_settings = { 'ROBOTSTXT_OBEY' : False, 'DOWNLOAD_DELAY' : 5 } \u5c0f\u6280\u5de7\uff1a\u8bbe\u7f6esetting\u503c\u7684\u4f18\u5148\u7ea7 1. \u547d\u4ee4\u884c\u9009\u9879 2. custom_settings 3. setting.py\u6587\u4ef6 4. \u5168\u5c40\u9ed8\u8ba4 \u601d\u8003\u70b9 1. \u4e3a\u4ec0\u4e48\u6211\u4eec\u7ee7\u627f\u9700\u8981\u5b9e\u73b0parse\u65b9\u6cd5\uff0c\u6709\u4e0d\u5b9e\u73b0\u7684\u65b9\u6cd5\u5417\uff1f 2. \u5f88\u591a\u65f6\u5019\u9700\u8981\u767b\u5f55\u600e\u4e48\u7ee7\u627f\u91cd\u5199\u65b9\u6cd5 3. \u5982\u679c\u6211\u4eec\u6ce8\u91ca\u4e86setting\u4e2d\u7684\u67d0\u4e9b\u53c2\u6570\uff0c\u4f46\u662f\u5728custom_setting\u4e2d\u5374\u8bbe\u7f6e\u4e86\u6709\u7528\u5417\uff0c\u6bd4\u5982\u4e2d\u95f4\u4ef6 4. \u56de\u7b54 \u8fd9\u91cc\u6211\u4eec\u53ef\u4ee5\u91cd\u5199start_requests()\u65b9\u6cd5 \u8fd9\u91cc\u6211\u4eec\u4e3e\u4e2a\u767b\u5f55github\u7684\u4f8b\u5b50 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class GithubSpider ( Spider ): name = \"github_login\" allowed_domains = [ \"github.com\" ] url = 'https://github.com/login' start_urls = [ url ] post_headers = { \"Accept\" : \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\" , \"Accept-Encoding\" : \"gzip, deflate\" , \"Accept-Language\" : \"zh-CN,zh;q=0.8,en;q=0.6\" , \"Cache-Control\" : \"no-cache\" , \"Connection\" : \"keep-alive\" , \"Content-Type\" : \"application/x-www-form-urlencoded\" , \"User-Agent\" : \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.75 Safari/537.36\" , \"Referer\" : \"https://github.com/\" , } def start_requests ( self ): return [ Request ( \"https://github.com/login\" , meta = { 'cookiejar' : 1 }, callback = self . post_login )] # FormRequeset def post_login ( self , response ): # \u5148\u53bb\u62ff\u9690\u85cf\u7684\u8868\u5355\u53c2\u6570authenticity_token authenticity_token = response . xpath ( '//input[@name=\"authenticity_token\"]/@value' ) . extract_first () logging . info ( 'authenticity_token=' + authenticity_token ) # FormRequeset.from_response\u662fScrapy\u63d0\u4f9b\u7684\u4e00\u4e2a\u51fd\u6570, \u7528\u4e8epost\u8868\u5355 # \u767b\u9646\u6210\u529f\u540e, \u4f1a\u8c03\u7528after_login\u56de\u8c03\u51fd\u6570\uff0c\u5982\u679curl\u8ddfRequest\u9875\u9762\u7684\u4e00\u6837\u5c31\u7701\u7565\u6389 return [ FormRequest . from_response ( response , url = 'https://github.com/session' , meta = { 'cookiejar' : response . meta [ 'cookiejar' ]}, headers = self . post_headers , # \u6ce8\u610f\u6b64\u5904\u7684headers formdata = { 'utf8' : '\u2713' , 'login' : '' , 'password' : '' , 'authenticity_token' : authenticity_token }, callback = self . after_login , dont_filter = True )] def after_login ( self , response ): # \u767b\u5f55\u4e4b\u540e\uff0c\u5f00\u59cb\u8fdb\u5165\u6211\u8981\u722c\u53d6\u7684\u79c1\u4fe1\u9875\u9762 for url in self . start_urls : # \u56e0\u4e3a\u6211\u4eec\u4e0a\u9762\u5b9a\u4e49\u4e86Rule\uff0c\u6240\u4ee5\u53ea\u9700\u8981\u7b80\u5355\u7684\u751f\u6210\u521d\u59cb\u722c\u53d6Request\u5373\u53ef yield Request ( url , meta = { 'cookiejar' : response . meta [ 'cookiejar' ]}) \u8fd9\u662f\u4e00\u4e2a\u6807\u51c6\u7684\u767b\u5f55\u6a21\u677f\u5177\u4f53\u6b65\u9aa4\u5c31\u662f - \u5148\u8bbf\u95ee\u767b\u5f55\u9875\u9762 - \u7136\u540e\u8868\u5355\u63d0\u4ea4 - \u6b63\u5e38\u9875\u9762\u4e1a\u52a1\u903b\u8f91\u4e66\u5199 3. 1 2 3 4 5 6 7 8 9 10 11 \u4e00\u822c\u5728\u4e00\u4e2a\u9879\u76ee\u4e2d\u6709\u591a\u4e2a\u722c\u866b\u7684\u65f6\u5019\u6211\u4eec\u4e60\u60ef\u628a\u516c\u5171\u7684\u653e\u5728 setting\u4e2d \uff0c\u6bcf\u4e2a\u722c\u866b\u79c1\u6709\u7684\u5c31\u653e\u5728 custom_settings\u4e2d \u6bd4\u5982\u4e2d\u95f4\u4ef6\u7684\u95ee\u9898\uff0c\u6709\u7684\u9700\u8981\u4ee3\u7406\uff0c\u800c\u6709\u7684\u4e0d\u9700\u8981\uff0c\u90a3\u600e\u4e48\u529e\u5462 custom_settings = { \"DOWNLOADER_MIDDLEWARES\" : { 'a.middlewares.ProxyMiddleware' : 543 , } }","title":"\u53c2\u6570\u89e3\u91ca"},{"location":"basics/scrapy/03_scrapy_crawlspiders/03_scrapy_crawlspiders/","text":"\u524d\u8a00 \u00b6 CrawlSpiders \u8fd9\u4e2a\u722c\u866b\u5f88\u5f3a\u5927\uff0c\u4ed6\u662f\u4e00\u822c\u7528\u6765\u722c\u53d6\u6574\u7ad9\u7684\u8d44\u6e90 \u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\u53ef\u4ee5\u5feb\u901f\u521b\u5efa CrawlSpider\u6a21\u677f \u7684\u4ee3\u7801\uff1ascrapy genspider -t crawl xxx xxx.com class scrapy.spiders.CrawlSpider \u662fSpider\u7684\u6d3e\u751f\u7c7b CrawlSpider\u7c7b\u5b9a\u4e49\u4e86\u4e00\u4e9b\u89c4\u5219(rule)\u6765\u63d0\u4f9b\u8ddf\u8fdblink\u7684\u65b9\u4fbf\u7684\u673a\u5236\uff0c\u4ece\u722c\u53d6\u7684\u7f51\u9875\u4e2d\u83b7\u53d6link\u5e76\u7ee7\u7eed\u722c\u53d6\u7684\u5de5\u4f5c\u66f4\u9002\u5408\u3002 CrawlSpiders\u6e90\u7801\u53c2\u8003 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class CrawlSpider ( Spider ): rules = () def __init__ ( self , * a , ** kw ): super ( CrawlSpider , self ) . __init__ ( * a , ** kw ) self . _compile_rules () # \u9996\u5148\u8c03\u7528parse()\u6765\u5904\u7406start_urll # parse() \u5219\u5c06\u8fd9\u4e9brespinse\u5bf9\u8c61\u8239\u4f53\u7ed9_parse_response()\u51fd\u6570\u5904\u7406,\u5e76\u8bbe\u7f6e\u56de\u8c03\u51fd\u6570\u4e3aparse_start_url() # \u8bbe\u7f6e\u4e86\u8ddf\u8fdb\u6807\u5fd7\u4f4dTrue # parse\u5c06\u8fd4\u56deitem\u548c\u8ddf\u8fdb\u4e86\u7684Request\u5bf9\u8c61 def parse ( self , response ): return self . _parse_response ( response , self . parse_start_url , cb_kwargs = {}, follow = True ) # \u5904\u7406start_url\u4e2d\u8fd4\u56de\u7684response\uff0c\u9700\u8981\u91cd\u5199 def parse_start_url ( self , response ): return [] def process_results ( self , response , results ): return results def _build_request ( self , rule , link ): r = Request ( url = link . url , callback = self . _response_downloaded ) r . meta . update ( rule = rule , link_text = link . text ) return r # \u4eceresponse\u4e2d\u62bd\u53d6\u7b26\u5408\u4efb\u610f\u7528\u6237\u5b9a\u4e49\u7684\u89c4\u5219\u7684\u94fe\u63a5\uff0c\u5e76\u6784\u9020\u6210Resquest\u5bf9\u8c61\u8fd4\u56de def _requests_to_follow ( self , response ): if not isinstance ( response , HtmlResponse ): return seen = set () #\u62bd\u53d6\u4e4b\u5185\u7684\u6240\u6709\u94fe\u63a5\uff0c\u53ea\u8981\u901a\u8fc7\u4efb\u610f\u4e00\u4e2a'\u89c4\u5219'\uff0c\u5373\u8868\u793a\u5408\u6cd5 for n , rule in enumerate ( self . _rules ): links = [ lnk for lnk in rule . link_extractor . extract_links ( response ) if lnk not in seen ] #\u4f7f\u7528\u7528\u6237\u6307\u5b9a\u7684process_links\u5904\u7406\u6bcf\u4e2a\u8fde\u63a5 if links and rule . process_links : links = rule . process_links ( links ) #\u5c06\u94fe\u63a5\u52a0\u5165seen\u96c6\u5408\uff0c\u4e3a\u6bcf\u4e2a\u94fe\u63a5\u751f\u6210Request\u5bf9\u8c61\uff0c\u5e76\u8bbe\u7f6e\u56de\u8c03\u51fd\u6570\u4e3a_repsonse_downloaded() for link in links : seen . add ( link ) #\u6784\u9020Request\u5bf9\u8c61\uff0c\u5e76\u5c06Rule\u89c4\u5219\u4e2d\u5b9a\u4e49\u7684\u56de\u8c03\u51fd\u6570\u4f5c\u4e3a\u8fd9\u4e2aRequest\u5bf9\u8c61\u7684\u56de\u8c03\u51fd\u6570 request = self . _build_request ( n , link ) #\u5bf9\u6bcf\u4e2aRequest\u8c03\u7528process_request()\u51fd\u6570\u3002\u8be5\u51fd\u6570\u9ed8\u8ba4\u4e3aindentify\uff0c\u5373\u4e0d\u505a\u4efb\u4f55\u5904\u7406\uff0c\u76f4\u63a5\u8fd4\u56de\u8be5Request. yield rule . _process_request ( request , response ) #\u5904\u7406\u901a\u8fc7rule\u63d0\u53d6\u51fa\u7684\u8fde\u63a5\uff0c\u5e76\u8fd4\u56deitem\u4ee5\u53carequest def _response_downloaded ( self , response ): rule = self . _rules [ response . meta [ 'rule' ]] return self . _parse_response ( response , rule . callback , rule . cb_kwargs , rule . follow ) #\u89e3\u6790response\u5bf9\u8c61\uff0c\u4f1a\u7528callback\u89e3\u6790\u5904\u7406\u4ed6\uff0c\u5e76\u8fd4\u56derequest\u6216Item\u5bf9\u8c61 def _parse_response ( self , response , callback , cb_kwargs , follow = True ): #\u9996\u5148\u5224\u65ad\u662f\u5426\u8bbe\u7f6e\u4e86\u56de\u8c03\u51fd\u6570\u3002\uff08\u8be5\u56de\u8c03\u51fd\u6570\u53ef\u80fd\u662frule\u4e2d\u7684\u89e3\u6790\u51fd\u6570\uff0c\u4e5f\u53ef\u80fd\u662f parse_start_url\u51fd\u6570\uff09 #\u5982\u679c\u8bbe\u7f6e\u4e86\u56de\u8c03\u51fd\u6570\uff08parse_start_url()\uff09\uff0c\u90a3\u4e48\u9996\u5148\u7528parse_start_url()\u5904\u7406response\u5bf9\u8c61\uff0c #\u7136\u540e\u518d\u4ea4\u7ed9process_results\u5904\u7406\u3002\u8fd4\u56decb_res\u7684\u4e00\u4e2a\u5217\u8868 if callback : # \u5982\u679c\u662fparse\u8c03\u7528\u7684\uff0c\u5219\u4f1a\u89e3\u6790\u6210Request\u5bf9\u8c61 # \u5982\u679c\u662frule callback\uff0c\u5219\u4f1a\u89e3\u6790\u6210Item cb_res = callback ( response , ** cb_kwargs ) or () cb_res = self . process_results ( response , cb_res ) for requests_or_item in iterate_spider_output ( cb_res ): yield requests_or_item #\u5982\u679c\u9700\u8981\u8ddf\u8fdb\uff0c\u90a3\u4e48\u4f7f\u7528\u5b9a\u4e49\u7684Rule\u89c4\u5219\u63d0\u53d6\u5e76\u8fd4\u56de\u8fd9\u4e9bRequest\u5bf9\u8c61 if follow and self . _follow_links : #\u8fd4\u56de\u6bcf\u4e2aRequest\u5bf9\u8c61 for request_or_item in self . _requests_to_follow ( response ): yield request_or_item def _compile_rules ( self ): self . _rules = [ copy . copy ( r ) for r in self . rules ] for rule in self . _rules : rule . _compile ( self ) @classmethod def from_crawler ( cls , crawler , * args , ** kwargs ): spider = super ( CrawlSpider , cls ) . from_crawler ( crawler , * args , ** kwargs ) spider . _follow_links = crawler . settings . getbool ( 'CRAWLSPIDER_FOLLOW_LINKS' , True ) return spider LinkExtractors\u6e90\u7801\u53c2\u8003 \u00b6 class scrapy.linkextractors.LinkExtractor Link Extractors \u7684\u76ee\u7684\u5f88\u7b80\u5355: \u63d0\u53d6\u94fe\u63a5\uff61 \u6bcf\u4e2aLinkExtractor\u6709\u552f\u4e00\u7684\u516c\u5171\u65b9\u6cd5\u662f extract_links()\uff0c\u5b83\u63a5\u6536\u4e00\u4e2a Response \u5bf9\u8c61\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a scrapy.link.Link \u5bf9\u8c61\u3002 Link Extractors\u8981\u5b9e\u4f8b\u5316\u4e00\u6b21\uff0c\u5e76\u4e14 extract_links \u65b9\u6cd5\u4f1a\u6839\u636e\u4e0d\u540c\u7684 response \u8c03\u7528\u591a\u6b21\u63d0\u53d6\u94fe\u63a5\uff61 1 2 3 4 5 6 7 8 9 10 11 12 13 class scrapy.linkextractors.LinkExtractor( allow = (), deny = (), allow_domains = (), deny_domains = (), deny_extensions = None, restrict_xpaths = (), tags = ('a','area'), attrs = ('href'), canonicalize = True, unique = True, process_value = None ) \u4e3b\u8981\u53c2\u6570 1 2 3 4 5 6 7 8 9 allow \uff1a\u6ee1\u8db3\u62ec\u53f7\u4e2d\u201c\u6b63\u5219\u8868\u8fbe\u5f0f\u201d\u7684\u503c\u4f1a\u88ab\u63d0\u53d6\uff0c\u5982\u679c\u4e3a\u7a7a\uff0c\u5219\u5168\u90e8\u5339\u914d\u3002 deny \uff1a\u4e0e\u8fd9\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f ( \u6216\u6b63\u5219\u8868\u8fbe\u5f0f\u5217\u8868 ) \u4e0d\u5339\u914d\u7684 URL\u4e00\u5b9a\u4e0d\u63d0\u53d6 \u3002 allow_domains \uff1a\u4f1a\u88ab\u63d0\u53d6\u7684\u94fe\u63a5\u7684 domains \u3002 deny_domains \uff1a\u4e00\u5b9a\u4e0d\u4f1a\u88ab\u63d0\u53d6\u94fe\u63a5\u7684 domains \u3002 restrict_xpaths \uff1a\u4f7f\u7528 xpath\u8868\u8fbe\u5f0f \uff0c\u548c allow\u5171\u540c\u4f5c\u7528\u8fc7\u6ee4\u94fe\u63a5 \u3002","title":"Scrapy \u6e90\u7801 CrawlSpider"},{"location":"basics/scrapy/03_scrapy_crawlspiders/03_scrapy_crawlspiders/#_1","text":"CrawlSpiders \u8fd9\u4e2a\u722c\u866b\u5f88\u5f3a\u5927\uff0c\u4ed6\u662f\u4e00\u822c\u7528\u6765\u722c\u53d6\u6574\u7ad9\u7684\u8d44\u6e90 \u901a\u8fc7\u4e0b\u9762\u7684\u547d\u4ee4\u53ef\u4ee5\u5feb\u901f\u521b\u5efa CrawlSpider\u6a21\u677f \u7684\u4ee3\u7801\uff1ascrapy genspider -t crawl xxx xxx.com class scrapy.spiders.CrawlSpider \u662fSpider\u7684\u6d3e\u751f\u7c7b CrawlSpider\u7c7b\u5b9a\u4e49\u4e86\u4e00\u4e9b\u89c4\u5219(rule)\u6765\u63d0\u4f9b\u8ddf\u8fdblink\u7684\u65b9\u4fbf\u7684\u673a\u5236\uff0c\u4ece\u722c\u53d6\u7684\u7f51\u9875\u4e2d\u83b7\u53d6link\u5e76\u7ee7\u7eed\u722c\u53d6\u7684\u5de5\u4f5c\u66f4\u9002\u5408\u3002","title":"\u524d\u8a00"},{"location":"basics/scrapy/03_scrapy_crawlspiders/03_scrapy_crawlspiders/#crawlspiders","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class CrawlSpider ( Spider ): rules = () def __init__ ( self , * a , ** kw ): super ( CrawlSpider , self ) . __init__ ( * a , ** kw ) self . _compile_rules () # \u9996\u5148\u8c03\u7528parse()\u6765\u5904\u7406start_urll # parse() \u5219\u5c06\u8fd9\u4e9brespinse\u5bf9\u8c61\u8239\u4f53\u7ed9_parse_response()\u51fd\u6570\u5904\u7406,\u5e76\u8bbe\u7f6e\u56de\u8c03\u51fd\u6570\u4e3aparse_start_url() # \u8bbe\u7f6e\u4e86\u8ddf\u8fdb\u6807\u5fd7\u4f4dTrue # parse\u5c06\u8fd4\u56deitem\u548c\u8ddf\u8fdb\u4e86\u7684Request\u5bf9\u8c61 def parse ( self , response ): return self . _parse_response ( response , self . parse_start_url , cb_kwargs = {}, follow = True ) # \u5904\u7406start_url\u4e2d\u8fd4\u56de\u7684response\uff0c\u9700\u8981\u91cd\u5199 def parse_start_url ( self , response ): return [] def process_results ( self , response , results ): return results def _build_request ( self , rule , link ): r = Request ( url = link . url , callback = self . _response_downloaded ) r . meta . update ( rule = rule , link_text = link . text ) return r # \u4eceresponse\u4e2d\u62bd\u53d6\u7b26\u5408\u4efb\u610f\u7528\u6237\u5b9a\u4e49\u7684\u89c4\u5219\u7684\u94fe\u63a5\uff0c\u5e76\u6784\u9020\u6210Resquest\u5bf9\u8c61\u8fd4\u56de def _requests_to_follow ( self , response ): if not isinstance ( response , HtmlResponse ): return seen = set () #\u62bd\u53d6\u4e4b\u5185\u7684\u6240\u6709\u94fe\u63a5\uff0c\u53ea\u8981\u901a\u8fc7\u4efb\u610f\u4e00\u4e2a'\u89c4\u5219'\uff0c\u5373\u8868\u793a\u5408\u6cd5 for n , rule in enumerate ( self . _rules ): links = [ lnk for lnk in rule . link_extractor . extract_links ( response ) if lnk not in seen ] #\u4f7f\u7528\u7528\u6237\u6307\u5b9a\u7684process_links\u5904\u7406\u6bcf\u4e2a\u8fde\u63a5 if links and rule . process_links : links = rule . process_links ( links ) #\u5c06\u94fe\u63a5\u52a0\u5165seen\u96c6\u5408\uff0c\u4e3a\u6bcf\u4e2a\u94fe\u63a5\u751f\u6210Request\u5bf9\u8c61\uff0c\u5e76\u8bbe\u7f6e\u56de\u8c03\u51fd\u6570\u4e3a_repsonse_downloaded() for link in links : seen . add ( link ) #\u6784\u9020Request\u5bf9\u8c61\uff0c\u5e76\u5c06Rule\u89c4\u5219\u4e2d\u5b9a\u4e49\u7684\u56de\u8c03\u51fd\u6570\u4f5c\u4e3a\u8fd9\u4e2aRequest\u5bf9\u8c61\u7684\u56de\u8c03\u51fd\u6570 request = self . _build_request ( n , link ) #\u5bf9\u6bcf\u4e2aRequest\u8c03\u7528process_request()\u51fd\u6570\u3002\u8be5\u51fd\u6570\u9ed8\u8ba4\u4e3aindentify\uff0c\u5373\u4e0d\u505a\u4efb\u4f55\u5904\u7406\uff0c\u76f4\u63a5\u8fd4\u56de\u8be5Request. yield rule . _process_request ( request , response ) #\u5904\u7406\u901a\u8fc7rule\u63d0\u53d6\u51fa\u7684\u8fde\u63a5\uff0c\u5e76\u8fd4\u56deitem\u4ee5\u53carequest def _response_downloaded ( self , response ): rule = self . _rules [ response . meta [ 'rule' ]] return self . _parse_response ( response , rule . callback , rule . cb_kwargs , rule . follow ) #\u89e3\u6790response\u5bf9\u8c61\uff0c\u4f1a\u7528callback\u89e3\u6790\u5904\u7406\u4ed6\uff0c\u5e76\u8fd4\u56derequest\u6216Item\u5bf9\u8c61 def _parse_response ( self , response , callback , cb_kwargs , follow = True ): #\u9996\u5148\u5224\u65ad\u662f\u5426\u8bbe\u7f6e\u4e86\u56de\u8c03\u51fd\u6570\u3002\uff08\u8be5\u56de\u8c03\u51fd\u6570\u53ef\u80fd\u662frule\u4e2d\u7684\u89e3\u6790\u51fd\u6570\uff0c\u4e5f\u53ef\u80fd\u662f parse_start_url\u51fd\u6570\uff09 #\u5982\u679c\u8bbe\u7f6e\u4e86\u56de\u8c03\u51fd\u6570\uff08parse_start_url()\uff09\uff0c\u90a3\u4e48\u9996\u5148\u7528parse_start_url()\u5904\u7406response\u5bf9\u8c61\uff0c #\u7136\u540e\u518d\u4ea4\u7ed9process_results\u5904\u7406\u3002\u8fd4\u56decb_res\u7684\u4e00\u4e2a\u5217\u8868 if callback : # \u5982\u679c\u662fparse\u8c03\u7528\u7684\uff0c\u5219\u4f1a\u89e3\u6790\u6210Request\u5bf9\u8c61 # \u5982\u679c\u662frule callback\uff0c\u5219\u4f1a\u89e3\u6790\u6210Item cb_res = callback ( response , ** cb_kwargs ) or () cb_res = self . process_results ( response , cb_res ) for requests_or_item in iterate_spider_output ( cb_res ): yield requests_or_item #\u5982\u679c\u9700\u8981\u8ddf\u8fdb\uff0c\u90a3\u4e48\u4f7f\u7528\u5b9a\u4e49\u7684Rule\u89c4\u5219\u63d0\u53d6\u5e76\u8fd4\u56de\u8fd9\u4e9bRequest\u5bf9\u8c61 if follow and self . _follow_links : #\u8fd4\u56de\u6bcf\u4e2aRequest\u5bf9\u8c61 for request_or_item in self . _requests_to_follow ( response ): yield request_or_item def _compile_rules ( self ): self . _rules = [ copy . copy ( r ) for r in self . rules ] for rule in self . _rules : rule . _compile ( self ) @classmethod def from_crawler ( cls , crawler , * args , ** kwargs ): spider = super ( CrawlSpider , cls ) . from_crawler ( crawler , * args , ** kwargs ) spider . _follow_links = crawler . settings . getbool ( 'CRAWLSPIDER_FOLLOW_LINKS' , True ) return spider","title":"CrawlSpiders\u6e90\u7801\u53c2\u8003"},{"location":"basics/scrapy/03_scrapy_crawlspiders/03_scrapy_crawlspiders/#linkextractors","text":"class scrapy.linkextractors.LinkExtractor Link Extractors \u7684\u76ee\u7684\u5f88\u7b80\u5355: \u63d0\u53d6\u94fe\u63a5\uff61 \u6bcf\u4e2aLinkExtractor\u6709\u552f\u4e00\u7684\u516c\u5171\u65b9\u6cd5\u662f extract_links()\uff0c\u5b83\u63a5\u6536\u4e00\u4e2a Response \u5bf9\u8c61\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a scrapy.link.Link \u5bf9\u8c61\u3002 Link Extractors\u8981\u5b9e\u4f8b\u5316\u4e00\u6b21\uff0c\u5e76\u4e14 extract_links \u65b9\u6cd5\u4f1a\u6839\u636e\u4e0d\u540c\u7684 response \u8c03\u7528\u591a\u6b21\u63d0\u53d6\u94fe\u63a5\uff61 1 2 3 4 5 6 7 8 9 10 11 12 13 class scrapy.linkextractors.LinkExtractor( allow = (), deny = (), allow_domains = (), deny_domains = (), deny_extensions = None, restrict_xpaths = (), tags = ('a','area'), attrs = ('href'), canonicalize = True, unique = True, process_value = None ) \u4e3b\u8981\u53c2\u6570 1 2 3 4 5 6 7 8 9 allow \uff1a\u6ee1\u8db3\u62ec\u53f7\u4e2d\u201c\u6b63\u5219\u8868\u8fbe\u5f0f\u201d\u7684\u503c\u4f1a\u88ab\u63d0\u53d6\uff0c\u5982\u679c\u4e3a\u7a7a\uff0c\u5219\u5168\u90e8\u5339\u914d\u3002 deny \uff1a\u4e0e\u8fd9\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f ( \u6216\u6b63\u5219\u8868\u8fbe\u5f0f\u5217\u8868 ) \u4e0d\u5339\u914d\u7684 URL\u4e00\u5b9a\u4e0d\u63d0\u53d6 \u3002 allow_domains \uff1a\u4f1a\u88ab\u63d0\u53d6\u7684\u94fe\u63a5\u7684 domains \u3002 deny_domains \uff1a\u4e00\u5b9a\u4e0d\u4f1a\u88ab\u63d0\u53d6\u94fe\u63a5\u7684 domains \u3002 restrict_xpaths \uff1a\u4f7f\u7528 xpath\u8868\u8fbe\u5f0f \uff0c\u548c allow\u5171\u540c\u4f5c\u7528\u8fc7\u6ee4\u94fe\u63a5 \u3002","title":"LinkExtractors\u6e90\u7801\u53c2\u8003"},{"location":"basics/scrapy/04_scrapy_middlewares/04_scrapy_middlewares/","text":"\u4e2d\u95f4\u4ef6\u53ef\u7b97\u662f\u6211\u4eec\u722c\u866b\u7684\u662f\u5426\u7a33\u5b9a\u7684\u597d\u628a\u624b \u00b6 \u6211\u4eec\u5148\u770b\u4e0b\u4e0b\u8f7d\u4e2d\u95f4\u4ef6 Download Middlewares \u6fc0\u6d3b\u5b9e\u5728settinsg\u4e2d\u7684\u6216\u8005\u5728\u722c\u866b\u5185\u8bbe\u7f6ecustom_settings\u4e2d settings 1 2 3 4 DOWNLOADER_MIDDLEWARES = { 'myproject.middlewares.CustomDownloaderMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, } custom_settings 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class MeiziSpider ( scrapy . Spider ): name = 'meizi' allowed_domains = [ 'xxx' ] start_urls = [ 'xxx' ,] custom_settings = { \"ITEM_PIPELINES\" : { 'myproject.middlewares.CustomDownloaderMiddleware' : 300 , }, \"DOWNLOAD_DELAY\" : 0.1 , } \u4ece\u56fe\u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u5f97\u5230\uff0c\u4ed6\u662f\u8c03\u5ea6\u5668\u8bbf\u95eeInternet\u7684\u5f00\u59cb\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u5728\u4e2d\u95f4\u4ef6\u4e2d\u5b9a\u4e49\u6211\u4eec\u81ea\u5df1\u7684\u4e00\u4e9b\u4e1c\u897f \u6211\u4eec\u5728\u81ea\u5b9a\u4e49\u4e2d\u95f4\u4ef6\u7684\u65f6\u5019\u5c31\u975e\u5e38\u65b9\u4fbf process_request\u65b9\u6cd5 \u00b6 1 2 def process_request(self, request, spider): pass \u8bf7\u6c42\u53c2\u6570 \u00b6 request(Request \u5bf9\u8c61)\u2013\u5904\u7406\u7684request spider(Spider \u5bf9\u8c61)\u2013\u8be5request\u5bf9\u5e94\u7684spider \u89e3\u91ca \u00b6 \u5728\u8fd9\u4e2a\u4e2d\u95f4\u4ef6\u6211\u4eec\u53ef\u4ee5\u968f\u673a\u66f4\u6362\u6216\u6dfb\u52a0\u6211\u4eec\u7684\u9700\u6c42 \u5f53\u6bcf\u4e2arequest\u901a\u8fc7\u4e0b\u8f7d\u4e2d\u95f4\u4ef6\u65f6\uff0c\u6539\u65b9\u6cd5\u88ab\u8c03\u7528\u3002process_request()\u5fc5\u987b\u8fd4\u56de\u5176\u4e2d\u4e4b\u4e00\uff1a\u8fd4\u56deNone\u3001\u8fd4\u56de\u4e2aResponse\u5bf9\u8c61\uff0c\u8fd4\u56de\u4e00\u4e2aRequest\u5bf9\u8c61\u6216raise lgnoreRequest\u3002\u6700\u5e38\u4f7f\u7528\u7684\u662fNone 1. \u5982\u679c\u8fd4\u56deNone\u3002\u4f1a\u5c06\u5904\u7406\u8fc7\u540e\u7684reques \u4e22\u7ed9\u4e2d\u95f4\u4ef6\u94fe\u4e2d\u7684\u4e0b\u4e00\u4e2a\u4e2d\u95f4\u4ef6\u7231\u4f60\u7684process_request()\u65b9\u6cd5\u5904\u7406\uff0c\u77e5\u9053\u4e22\u5230\u4e0b\u8f7d\u5668\uff0c\u7531\u4e0b\u8f7d\u5668\u4e0b\u8f7d. ==\u5c31\u5982\u4e0a\u90a3\u4e2a\u66f4\u6362ip\u7684\u5c31\u662f\u8fd4\u56deNone== 2. \u5982\u679c\u8fd4\u56deResponse\u5bf9\u8c61\uff0c==Scrapy\u5c06\u4e0d\u4f1a\u8c03\u7528\u4efb\u4f55\u5176\u4ed6\u7684process_request() \u6216process_exception()\u65b9\u6cd5==\uff0c\u5c31\u4e0d\u4f1a\u4e22\u5230\u4e0b\u8f7d\u5668\u4e0b\u8f7d\uff0c\u76f4\u63a5\u5c06\u5176\u8fd4\u56de\u7684resionse\u4e22\u5230\u4e2d\u95f4\u4ef6\u94fe\u7684process_response()\u5904\u7406\u3002\u53ef\u4ee5\u901a\u8fc7scrapy.http.Response\u6784\u5efaResponse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from selenium import webdriver from scrapy.http import HtmlResponse import time class SeleniumMiddleware ( object ): def __init__ ( self ): self . driver = webdriver . Chrome () def process_request ( self , request , spider ): self . driver . get ( request . url ) time . sleep ( 2 ) body = self . driver . page_source return HtmlResponse ( self . driver . current_url , body = body , encoding = 'utf-8' , request = request ) \u8fd9\u91cc\u6211\u4eec\u5c06\u8bf7\u6c42\u6362\u6210\u4f7f\u7528 seleium\u53bb\u722c\u53d6 \uff0c\u4e0d\u4f1a\u8c03\u7528\u5176\u4ed6\u7684 process_request () \u548c process_exception () \u65b9\u6cd5 \u5982\u679c\u5176\u8fd4\u56deRequest \u5bf9\u8c61\uff0cScrapy\u5219\u505c\u6b62\u8c03\u7528process_request\u65b9\u6cd5\u5e76\u91cd\u65b0\u8c03\u5ea6\u8fd4\u56de\u7684reque,\u5f53\u5fc3\u8fd4\u56de\u7684request\u88ab\u6267\u884c\u540e\uff0c\u76f8\u5e94\u5730\u4e2d\u95f4\u4ef6\u94fe\u5c06\u4f1a\u6839\u636e\u4e0b\u8f7d\u7684response\u88ab\u8c03\u7528\uff0c\u76f8\u5e94\u5730\u4e2d\u95f4\u94fe\u5c06\u4f1a\u6839\u636e\u4e0b\u8f7d\u7684response\u88ab\u8c03\u7528 \u5982\u679c\u5176raise\u4e00\u4e2algnoreRequest\u5f02\u5e38\uff0c\u5219\u5b89\u88c5\u7684\u4e0b\u8f7d\u4e2d\u95f4\u4ef6\u7684process_exception()\u65b9\u6cd5\u4f1a\u88ab\u8c03\u7528\uff0c\u5982\u679c\u6ca1\u6709\u4efb\u4f55\u4e00\u4e2a\u65b9\u6cd5\u5904\u7406\u8be5\u5f02\u5e38\uff0c\u5219request\u7684errback(Request.errback)\u65b9\u6cd5\u4f1a\u88ab\u8c03\u7528\u3002\u5982\u679c\u6ca1\u6709\u4ee3\u7801\u5904\u7406\u629b\u51fa\u7684\u5f02\u5e38\uff0c\u5219\u8be5\u5f02\u5e38\u88ab\u5ffd\u7565\u4e14\u4e0d\u8bb0\u5f55\uff08\u4e0d\u540c\u4e8e\u5176\u4ed6\u5f02\u5e38\u90a3\u6837\uff09 \u4e3e\u4f8b \u6bd4\u5982\u6211\u4eec\u5199\u4e00\u4e2a\u8bf7\u6c42\u7684\u65f6\u5019\u968f\u673a\u66f4\u6362\u8bf7\u6c42\u5934 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # \u8fd9\u4e2a\u7528fake\u968f\u673a\u66f4\u6362\u8bbf\u95ee\u5934\uff0c\u4e00\u822c\u722c\u866b\u90fd\u4f1a\u52a0\u7684\uff0c\u65b9\u4fbf\u53cd\u6252 from fake_useragent import UserAgent class RandomUserAgentMiddlware ( object ): #\u968f\u673a\u66f4\u6362user-agent def __init__ ( self , crawler ): super ( RandomUserAgentMiddlware , self ) . __init__ () self . ua = UserAgent () self . ua_type = crawler . settings . get ( \"RANDOM_UA_TYPE\" , \"random\" ) @classmethod def from_crawler ( cls , crawler ): return cls ( crawler ) def process_request ( self , request , spider ): def get_ua (): return getattr ( self . ua , self . ua_type ) request . headers . setdefault ( 'User-Agent' , get_ua ()) process_response\u65b9\u6cd5 \u00b6 1 2 def process_response(self, rsponse, spider)\u65b9\u6cd5 pass \u89e3\u91ca \u5f53\u4e0b\u8f7d\u7684response\u8fd4\u56de\u65f6\uff0cprocess_response()\u88ab\u8c03\u7528\uff0c\u4e14 \u5fc5\u987b\u8fd4\u56de\u4e00\u4e0b\u4e4b\u4e00\uff0c\u8fd4\u56de\u4e00\u4e2aResponse\u5bf9\u8c61\u3001\u8fd4\u56de\u4e00\u4e2arequest\u5bf9\u8c61\u6216raise\u4e00\u4e2algnoreRequest \u5f02\u5e38 \u8bf7\u6c42\u53c2\u6570 1.\u5982\u679c\u5c06\u5176\u8fd4\u56de\u4e00\u4e2aresponse\u5bf9\u8c61(\u53ef\u4ee5\u4e0e\u4f20\u5165\u7684response \u76f8\u540c\uff0c\u4e5f\u53ef\u4ee5\u662f\u5168\u65b0\u7684\u5bf9\u8c61)\uff0c\u8be5response\u4f1a\u88ab\u5728\u94fe\u4e2d\u7684\u5176\u4ed6\u4e2d\u95f4\u4ef6process_response()\u65b9\u6cd5\u5904\u7406 2.\u5982\u679c\u8fd4\u56de\u4e00\u4e2arequest\u5bf9\u8c61\uff0c\u5219\u4e2d\u95f4\u4ef6\u94fe\u505c\u6b62\uff0c\u8fd4\u56de\u7684request\u4f1a\u88ab\u91cd\u5199\u8c03\u5ea6\u4e0b\u8f7d\uff0c\u5904\u7406\u96f7\u8bd7\u96e8process_request() \u8fd4\u56derequest\u6240\u505a\u7684\u90a3\u6837 3.\u5982\u679c\u5176\u629b\u51fa\u4e00\u4e2algnorRequest \u5f02\u5e38\uff0c\u5219\u8c03\u7528request\u7684errback(Request.errback)\u3002\u5982\u679c\u6ca1\u6709\u4ee3\u7801\u5904\u7406\u629b\u51fa\u7684\u5f02\u5e38\uff0c\u5219\u8be5\u5f02\u5e38\u88ab\u5ffd\u7565\u4e0d\u8bb0\u5f55 \u53c2\u6570 request (Request \u5bf9\u8c61) \u2013 response\u6240\u5bf9\u5e94\u7684request response (Response \u5bf9\u8c61) \u2013 \u88ab\u5904\u7406\u7684response spider (Spider \u5bf9\u8c61) \u2013 response\u6240\u5bf9\u5e94\u7684spider \u4e3e\u4f8b \u6bd4\u5982\u6211\u4eec\u5199\u4e00\u4e2a\u5982\u679c\u9047\u5230\u72b6\u6001\u7801\u95ee\u9898\uff0c\u7136\u540e\u66f4\u6362\u4ee3\u7406 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from scrapy.downloadermiddlewares.retry import RetryMiddleware from scrapy.utils.response import response_status_message class CustomRetryMiddleware ( RetryMiddleware ): def process_response ( self , request , response , spider ): if request . meta . get ( 'dont_retry' , False ): return response if response . status in self . retry_http_codes : reason = response_status_message ( response . status ) #\u5982\u679c\u8fd4\u56de\u4e86[500, 502, 503, 504, 522, 524, 408]\u8fd9\u4e9bcode\uff0c\u6362\u4e2aproxy\u8bd5\u8bd5 proxy = random . choice ( proxy_list ) request . meta [ 'proxy' ] = proxy return self . _retry ( request , reason , spider ) or response return response \u5728\u8fd9\u4e2a\u4e2d\u95f4\u4ef6\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6211\u4eec\u7684\u8fd4\u56deresponse\u4e4b\u7c7b\u7684 request \u7684\u8bf7\u6c42\u4f53\uff0c\u548cspider \u4f53\u4e4b\u7c7b\u7684 process_exception \u65b9\u6cd5 \u00b6 1 2 def process_exception(self, request, exception, spider): pass \u5f53\u4e0b\u8f7d\u5904\u7406\u5668(download handler)\u6216 process_request() (\u4e0b\u8f7d\u4e2d\u95f4\u4ef6)\u629b\u51fa\u5f02\u5e38(\u5305\u62ecIgnoreRequest\u5f02\u5e38)\u65f6\uff0cScrapy\u8c03\u7528 process_exception() \u3002process_exception() \u5e94\u8be5\u8fd4\u56de\u4ee5\u4e0b\u4e4b\u4e00: \u8fd4\u56de None \u3001 \u4e00\u4e2a Response \u5bf9\u8c61\u3001\u6216\u8005\u4e00\u4e2a Request \u5bf9\u8c61\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from scrapy.downloadermiddlewares.retry import RetryMiddleware from scrapy.utils.response import response_status_message class CustomRetryMiddleware ( RetryMiddleware ): #RetryMiddleware\u7c7b\u91cc\u6709\u4e2a\u5e38\u91cf\uff0c\u8bb0\u5f55\u4e86\u8fde\u63a5\u8d85\u65f6\u90a3\u4e9b\u5f02\u5e38 #EXCEPTIONS_TO_RETRY = (defer.TimeoutError, TimeoutError, DNSLookupError, # ConnectionRefusedError, ConnectionDone, ConnectError, # ConnectionLost, TCPTimedOutError, ResponseFailed, # IOError, TunnelError) def process_exception ( self , request , exception , spider ): if isinstance ( exception , self . EXCEPTIONS_TO_RETRY ) and not request . meta . get ( 'dont_retry' , False ): #\u8fd9\u91cc\u53ef\u4ee5\u5199\u51fa\u73b0\u5f02\u5e38\u90a3\u4e9b\u4f60\u7684\u5904\u7406 proxy = random . choice ( proxy_list ) request . meta [ 'proxy' ] = proxy time . sleep ( random . randint ( 3 , 5 )) return self . _retry ( request , exception , spider ) #_retry\u662fRetryMiddleware\u4e2d\u7684\u4e00\u4e2a\u79c1\u6709\u65b9\u6cd5\uff0c\u4e3b\u8981\u4f5c\u7528\u662f #1.\u5bf9request.meta\u4e2d\u7684retry_time\u8fdb\u884c+1 #2.\u5c06retry_times\u548cmax_retry_time\u8fdb\u884c\u6bd4\u8f83\uff0c\u5982\u679c\u524d\u8005\u5c0f\u4e8e\u7b49\u4e8e\u540e\u8005\uff0c\u5229\u7528copy\u65b9\u6cd5\u5728\u539f\u6765\u7684request\u4e0a\u590d\u5236\u4e00\u4e2a\u65b0request\uff0c\u5e76\u66f4\u65b0\u5176retry_times\uff0c\u5e76\u5c06dont_filter\u8bbe\u4e3aTrue\u6765\u9632\u6b62\u56e0url\u91cd\u590d\u800c\u88ab\u8fc7\u6ee4\u3002 #3.\u8bb0\u5f55\u91cd\u8bd5reason \u5982\u679c\u5176\u8fd4\u56de None \uff0cScrapy\u5c06\u4f1a\u7ee7\u7eed\u5904\u7406\u8be5\u5f02\u5e38\uff0c\u63a5\u7740\u8c03\u7528\u5df2\u5b89\u88c5\u7684\u5176\u4ed6\u4e2d\u95f4\u4ef6\u7684 process_exception() \u65b9\u6cd5\uff0c\u76f4\u5230\u6240\u6709\u4e2d\u95f4\u4ef6\u90fd\u88ab\u8c03\u7528\u5b8c\u6bd5\uff0c\u5219\u8c03\u7528\u9ed8\u8ba4\u7684\u5f02\u5e38\u5904\u7406\u3002 \u5982\u679c\u5176\u8fd4\u56de\u4e00\u4e2a Response \u5bf9\u8c61\uff0c\u5219\u5df2\u5b89\u88c5\u7684\u4e2d\u95f4\u4ef6\u94fe\u7684 process_response() \u65b9\u6cd5\u88ab\u8c03\u7528\u3002Scrapy\u5c06\u4e0d\u4f1a\u8c03\u7528\u4efb\u4f55\u5176\u4ed6\u4e2d\u95f4\u4ef6\u7684 process_exception() \u65b9\u6cd5\u3002 \u5982\u679c\u5176\u8fd4\u56de\u4e00\u4e2a Request \u5bf9\u8c61\uff0c \u5219\u8fd4\u56de\u7684request\u5c06\u4f1a\u88ab\u91cd\u65b0\u8c03\u7528\u4e0b\u8f7d\u3002\u8fd9\u5c06\u505c\u6b62\u4e2d\u95f4\u4ef6\u7684 process_exception() \u65b9\u6cd5\u6267\u884c\uff0c\u5c31\u5982\u8fd4\u56de\u4e00\u4e2aresponse\u7684\u90a3\u6837\u3002 \u53c2\u6570 1. request (\u662f Request \u5bf9\u8c61) \u2013 \u4ea7\u751f\u5f02\u5e38\u7684request 2. exception (Exception \u5bf9\u8c61) \u2013 \u629b\u51fa\u7684\u5f02\u5e38 3. spider (Spider \u5bf9\u8c61) \u2013 request\u5bf9\u5e94\u7684spider spider\u4e2d\u95f4\u4ef6 \u00b6 \u4ece\u67b6\u6784\u56fe\u4e2d\u6211\u4eec\u53ef\u4ee5\u77e5\u9053spider\u4e2d\u95f4\u4ef6\u662f\u7528\u4e8e\u5904\u7406response \u4ee5\u53caspider \u751f\u6210\u7684item\u548cRequest \u542f\u7528spider\u4e2d\u95f4\u4ef6\u6211\u4eec\u53ef\u4ee5\u5728setting\u4e2d\u8bbe\u7f6e settings 1 2 3 4 SPIDER_MIDDLEWARES = { 'myproject.middlewares.CustomSpiderMiddleware': 543, 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': None, } \u6216\u8005\u662f\u5728spider\u4e2d\u8bbe\u7f6e custom_settings 1 2 3 4 5 6 7 8 9 10 11 12 class Spider ( scrapy . Spider ): allowed_domains = [ 'xxx' ] start_urls = [ 'xxx' ,] custom_settings = { \"SPIDER_MIDDLEWARES\" : { 'myproject.middlewares.CustomSpiderMiddleware' : 543 , }, } \u7f16\u5199\u81ea\u5b9a\u4e49spider\u4e2d\u95f4\u4ef6 \u00b6 process_spider_input(response, spider) \u8fd9\u4e2a\u65b9\u6cd5\u65f6\u5728\u4e0b\u8f7d\u5668\u4e2d\u95f4\u4ef6\u5904\u7406\u5b8c\u6210\u540e\uff0c\u9a6c\u4e0a\u8981\u8fdb\u5165\u67d0\u4e2a\u56de\u8c03\u51fd\u6570parse_xxx()\u524d\u8c03\u7528\u3002process_spider_output(response, result, output)\u5728\u8fd9\u4e2a\u65b9\u6cd5\u5904\u7406\u5b8c\u6210\u4ee5\u540e\uff0c\u6570\u636e\u5982\u679c\u662fitem\uff0c\u5c31\u4f1a\u88ab\u4ea4\u7ed9pipeline\uff1b\u5982\u679c\u662f\u8bf7\u6c42\uff0c\u5c31\u4f1a\u88ab\u4ea4\u7ed9\u8c03\u5ea6\u5668\uff0c\u7136\u540e\u4e0b\u8f7d\u5668\u4e2d\u95f4\u4ef6\u624d\u4f1a\u5f00\u59cb\u8fd0\u884c\u3002\u8fd9\u4e2a\u65b9\u6cd5\u7684\u53c2\u6570result\u5c31\u662f\u722c\u866b\u722c\u51fa\u6765\u7684item\u6216\u8005scrapy.Request()\u3002\u7531\u4e8eyield\u5f97\u5230\u7684\u662f\u4e00\u4e2a\u751f\u6210\u5668\uff0c\u751f\u6210\u5668\u662f\u53ef\u4ee5\u8fed\u4ee3\u7684\uff0c\u6240\u4ee5result\u4e5f\u662f\u53ef\u4ee5\u8fed\u4ee3\u7684\uff0c\u53ef\u4ee5\u4f7f\u7528for\u5faa\u73af\u6765\u628a\u5b83\u5c55\u5f00\u3002 process_spider_output(response, result, spider) \u662f\u5728\u722c\u866b\u8fd0\u884cyield item \u6216\u662fyield scrapy.Request() \u7684\u65f6\u5019\u88ab\u8c03\u7528\u3002\u4e00\u822c\u8fd4\u56deresult process_spider_exception(response, exception, spider) \u5f53spider\u4e2d\u95f4\u4ef6\u629b\u51fa\u5f02\u5e38\u65f6\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u88ab\u8c03\u7528\uff0c\u8fd4\u56deNone\u6216\u53ef\u8fed\u4ee3\u5bf9\u8c61\u7684Request,dict,item \u603b\u7ed3 \u00b6 \u4e2d\u95f4\u4ef6\u7684\u6574\u4f53\u6d41\u7a0b\u5982\u4e0b \u4e2d\u95f4\u4ef6\u5927\u5927\u4e30\u5bcc\u4e86\u6211\u4eec\u7684\u9700\u8981\u7684\u64cd\u4f5c\uff01\u76f8\u5f53\u4e8e\u94a9\u5b50\u628a\uff01\u628a\u4e00\u4e9b\u5728\u4e1a\u52a1\u5904\u7406\u4e0a\u96be\u4ee5\u89e3\u51b3\u7684\u95ee\u9898\u7528\u4e2d\u95f4\u4ef6\u5904\u7406\u662f\u975e\u5e38\u7b80\u5355\u5730\u3002 \u53c2\u8003\u94fe\u63a5 \u00b6 https://zhuanlan.zhihu.com/p/42498126 https://www.cnblogs.com/fengf233/p/11453375.html https://www.kingname.info/2018/11/20/know-middleware-of-scrapy-3/","title":"Scrapy \u6e90\u7801 Middewares"},{"location":"basics/scrapy/04_scrapy_middlewares/04_scrapy_middlewares/#_1","text":"\u6211\u4eec\u5148\u770b\u4e0b\u4e0b\u8f7d\u4e2d\u95f4\u4ef6 Download Middlewares \u6fc0\u6d3b\u5b9e\u5728settinsg\u4e2d\u7684\u6216\u8005\u5728\u722c\u866b\u5185\u8bbe\u7f6ecustom_settings\u4e2d settings 1 2 3 4 DOWNLOADER_MIDDLEWARES = { 'myproject.middlewares.CustomDownloaderMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, } custom_settings 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class MeiziSpider ( scrapy . Spider ): name = 'meizi' allowed_domains = [ 'xxx' ] start_urls = [ 'xxx' ,] custom_settings = { \"ITEM_PIPELINES\" : { 'myproject.middlewares.CustomDownloaderMiddleware' : 300 , }, \"DOWNLOAD_DELAY\" : 0.1 , } \u4ece\u56fe\u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u5f97\u5230\uff0c\u4ed6\u662f\u8c03\u5ea6\u5668\u8bbf\u95eeInternet\u7684\u5f00\u59cb\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u5728\u4e2d\u95f4\u4ef6\u4e2d\u5b9a\u4e49\u6211\u4eec\u81ea\u5df1\u7684\u4e00\u4e9b\u4e1c\u897f \u6211\u4eec\u5728\u81ea\u5b9a\u4e49\u4e2d\u95f4\u4ef6\u7684\u65f6\u5019\u5c31\u975e\u5e38\u65b9\u4fbf","title":"\u4e2d\u95f4\u4ef6\u53ef\u7b97\u662f\u6211\u4eec\u722c\u866b\u7684\u662f\u5426\u7a33\u5b9a\u7684\u597d\u628a\u624b"},{"location":"basics/scrapy/04_scrapy_middlewares/04_scrapy_middlewares/#process_request","text":"1 2 def process_request(self, request, spider): pass","title":"process_request\u65b9\u6cd5"},{"location":"basics/scrapy/04_scrapy_middlewares/04_scrapy_middlewares/#_2","text":"request(Request \u5bf9\u8c61)\u2013\u5904\u7406\u7684request spider(Spider \u5bf9\u8c61)\u2013\u8be5request\u5bf9\u5e94\u7684spider","title":"\u8bf7\u6c42\u53c2\u6570"},{"location":"basics/scrapy/04_scrapy_middlewares/04_scrapy_middlewares/#_3","text":"\u5728\u8fd9\u4e2a\u4e2d\u95f4\u4ef6\u6211\u4eec\u53ef\u4ee5\u968f\u673a\u66f4\u6362\u6216\u6dfb\u52a0\u6211\u4eec\u7684\u9700\u6c42 \u5f53\u6bcf\u4e2arequest\u901a\u8fc7\u4e0b\u8f7d\u4e2d\u95f4\u4ef6\u65f6\uff0c\u6539\u65b9\u6cd5\u88ab\u8c03\u7528\u3002process_request()\u5fc5\u987b\u8fd4\u56de\u5176\u4e2d\u4e4b\u4e00\uff1a\u8fd4\u56deNone\u3001\u8fd4\u56de\u4e2aResponse\u5bf9\u8c61\uff0c\u8fd4\u56de\u4e00\u4e2aRequest\u5bf9\u8c61\u6216raise lgnoreRequest\u3002\u6700\u5e38\u4f7f\u7528\u7684\u662fNone 1. \u5982\u679c\u8fd4\u56deNone\u3002\u4f1a\u5c06\u5904\u7406\u8fc7\u540e\u7684reques \u4e22\u7ed9\u4e2d\u95f4\u4ef6\u94fe\u4e2d\u7684\u4e0b\u4e00\u4e2a\u4e2d\u95f4\u4ef6\u7231\u4f60\u7684process_request()\u65b9\u6cd5\u5904\u7406\uff0c\u77e5\u9053\u4e22\u5230\u4e0b\u8f7d\u5668\uff0c\u7531\u4e0b\u8f7d\u5668\u4e0b\u8f7d. ==\u5c31\u5982\u4e0a\u90a3\u4e2a\u66f4\u6362ip\u7684\u5c31\u662f\u8fd4\u56deNone== 2. \u5982\u679c\u8fd4\u56deResponse\u5bf9\u8c61\uff0c==Scrapy\u5c06\u4e0d\u4f1a\u8c03\u7528\u4efb\u4f55\u5176\u4ed6\u7684process_request() \u6216process_exception()\u65b9\u6cd5==\uff0c\u5c31\u4e0d\u4f1a\u4e22\u5230\u4e0b\u8f7d\u5668\u4e0b\u8f7d\uff0c\u76f4\u63a5\u5c06\u5176\u8fd4\u56de\u7684resionse\u4e22\u5230\u4e2d\u95f4\u4ef6\u94fe\u7684process_response()\u5904\u7406\u3002\u53ef\u4ee5\u901a\u8fc7scrapy.http.Response\u6784\u5efaResponse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from selenium import webdriver from scrapy.http import HtmlResponse import time class SeleniumMiddleware ( object ): def __init__ ( self ): self . driver = webdriver . Chrome () def process_request ( self , request , spider ): self . driver . get ( request . url ) time . sleep ( 2 ) body = self . driver . page_source return HtmlResponse ( self . driver . current_url , body = body , encoding = 'utf-8' , request = request ) \u8fd9\u91cc\u6211\u4eec\u5c06\u8bf7\u6c42\u6362\u6210\u4f7f\u7528 seleium\u53bb\u722c\u53d6 \uff0c\u4e0d\u4f1a\u8c03\u7528\u5176\u4ed6\u7684 process_request () \u548c process_exception () \u65b9\u6cd5 \u5982\u679c\u5176\u8fd4\u56deRequest \u5bf9\u8c61\uff0cScrapy\u5219\u505c\u6b62\u8c03\u7528process_request\u65b9\u6cd5\u5e76\u91cd\u65b0\u8c03\u5ea6\u8fd4\u56de\u7684reque,\u5f53\u5fc3\u8fd4\u56de\u7684request\u88ab\u6267\u884c\u540e\uff0c\u76f8\u5e94\u5730\u4e2d\u95f4\u4ef6\u94fe\u5c06\u4f1a\u6839\u636e\u4e0b\u8f7d\u7684response\u88ab\u8c03\u7528\uff0c\u76f8\u5e94\u5730\u4e2d\u95f4\u94fe\u5c06\u4f1a\u6839\u636e\u4e0b\u8f7d\u7684response\u88ab\u8c03\u7528 \u5982\u679c\u5176raise\u4e00\u4e2algnoreRequest\u5f02\u5e38\uff0c\u5219\u5b89\u88c5\u7684\u4e0b\u8f7d\u4e2d\u95f4\u4ef6\u7684process_exception()\u65b9\u6cd5\u4f1a\u88ab\u8c03\u7528\uff0c\u5982\u679c\u6ca1\u6709\u4efb\u4f55\u4e00\u4e2a\u65b9\u6cd5\u5904\u7406\u8be5\u5f02\u5e38\uff0c\u5219request\u7684errback(Request.errback)\u65b9\u6cd5\u4f1a\u88ab\u8c03\u7528\u3002\u5982\u679c\u6ca1\u6709\u4ee3\u7801\u5904\u7406\u629b\u51fa\u7684\u5f02\u5e38\uff0c\u5219\u8be5\u5f02\u5e38\u88ab\u5ffd\u7565\u4e14\u4e0d\u8bb0\u5f55\uff08\u4e0d\u540c\u4e8e\u5176\u4ed6\u5f02\u5e38\u90a3\u6837\uff09 \u4e3e\u4f8b \u6bd4\u5982\u6211\u4eec\u5199\u4e00\u4e2a\u8bf7\u6c42\u7684\u65f6\u5019\u968f\u673a\u66f4\u6362\u8bf7\u6c42\u5934 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # \u8fd9\u4e2a\u7528fake\u968f\u673a\u66f4\u6362\u8bbf\u95ee\u5934\uff0c\u4e00\u822c\u722c\u866b\u90fd\u4f1a\u52a0\u7684\uff0c\u65b9\u4fbf\u53cd\u6252 from fake_useragent import UserAgent class RandomUserAgentMiddlware ( object ): #\u968f\u673a\u66f4\u6362user-agent def __init__ ( self , crawler ): super ( RandomUserAgentMiddlware , self ) . __init__ () self . ua = UserAgent () self . ua_type = crawler . settings . get ( \"RANDOM_UA_TYPE\" , \"random\" ) @classmethod def from_crawler ( cls , crawler ): return cls ( crawler ) def process_request ( self , request , spider ): def get_ua (): return getattr ( self . ua , self . ua_type ) request . headers . setdefault ( 'User-Agent' , get_ua ())","title":"\u89e3\u91ca"},{"location":"basics/scrapy/04_scrapy_middlewares/04_scrapy_middlewares/#process_response","text":"1 2 def process_response(self, rsponse, spider)\u65b9\u6cd5 pass \u89e3\u91ca \u5f53\u4e0b\u8f7d\u7684response\u8fd4\u56de\u65f6\uff0cprocess_response()\u88ab\u8c03\u7528\uff0c\u4e14 \u5fc5\u987b\u8fd4\u56de\u4e00\u4e0b\u4e4b\u4e00\uff0c\u8fd4\u56de\u4e00\u4e2aResponse\u5bf9\u8c61\u3001\u8fd4\u56de\u4e00\u4e2arequest\u5bf9\u8c61\u6216raise\u4e00\u4e2algnoreRequest \u5f02\u5e38 \u8bf7\u6c42\u53c2\u6570 1.\u5982\u679c\u5c06\u5176\u8fd4\u56de\u4e00\u4e2aresponse\u5bf9\u8c61(\u53ef\u4ee5\u4e0e\u4f20\u5165\u7684response \u76f8\u540c\uff0c\u4e5f\u53ef\u4ee5\u662f\u5168\u65b0\u7684\u5bf9\u8c61)\uff0c\u8be5response\u4f1a\u88ab\u5728\u94fe\u4e2d\u7684\u5176\u4ed6\u4e2d\u95f4\u4ef6process_response()\u65b9\u6cd5\u5904\u7406 2.\u5982\u679c\u8fd4\u56de\u4e00\u4e2arequest\u5bf9\u8c61\uff0c\u5219\u4e2d\u95f4\u4ef6\u94fe\u505c\u6b62\uff0c\u8fd4\u56de\u7684request\u4f1a\u88ab\u91cd\u5199\u8c03\u5ea6\u4e0b\u8f7d\uff0c\u5904\u7406\u96f7\u8bd7\u96e8process_request() \u8fd4\u56derequest\u6240\u505a\u7684\u90a3\u6837 3.\u5982\u679c\u5176\u629b\u51fa\u4e00\u4e2algnorRequest \u5f02\u5e38\uff0c\u5219\u8c03\u7528request\u7684errback(Request.errback)\u3002\u5982\u679c\u6ca1\u6709\u4ee3\u7801\u5904\u7406\u629b\u51fa\u7684\u5f02\u5e38\uff0c\u5219\u8be5\u5f02\u5e38\u88ab\u5ffd\u7565\u4e0d\u8bb0\u5f55 \u53c2\u6570 request (Request \u5bf9\u8c61) \u2013 response\u6240\u5bf9\u5e94\u7684request response (Response \u5bf9\u8c61) \u2013 \u88ab\u5904\u7406\u7684response spider (Spider \u5bf9\u8c61) \u2013 response\u6240\u5bf9\u5e94\u7684spider \u4e3e\u4f8b \u6bd4\u5982\u6211\u4eec\u5199\u4e00\u4e2a\u5982\u679c\u9047\u5230\u72b6\u6001\u7801\u95ee\u9898\uff0c\u7136\u540e\u66f4\u6362\u4ee3\u7406 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from scrapy.downloadermiddlewares.retry import RetryMiddleware from scrapy.utils.response import response_status_message class CustomRetryMiddleware ( RetryMiddleware ): def process_response ( self , request , response , spider ): if request . meta . get ( 'dont_retry' , False ): return response if response . status in self . retry_http_codes : reason = response_status_message ( response . status ) #\u5982\u679c\u8fd4\u56de\u4e86[500, 502, 503, 504, 522, 524, 408]\u8fd9\u4e9bcode\uff0c\u6362\u4e2aproxy\u8bd5\u8bd5 proxy = random . choice ( proxy_list ) request . meta [ 'proxy' ] = proxy return self . _retry ( request , reason , spider ) or response return response \u5728\u8fd9\u4e2a\u4e2d\u95f4\u4ef6\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6211\u4eec\u7684\u8fd4\u56deresponse\u4e4b\u7c7b\u7684 request \u7684\u8bf7\u6c42\u4f53\uff0c\u548cspider \u4f53\u4e4b\u7c7b\u7684","title":"process_response\u65b9\u6cd5"},{"location":"basics/scrapy/04_scrapy_middlewares/04_scrapy_middlewares/#process_exception","text":"1 2 def process_exception(self, request, exception, spider): pass \u5f53\u4e0b\u8f7d\u5904\u7406\u5668(download handler)\u6216 process_request() (\u4e0b\u8f7d\u4e2d\u95f4\u4ef6)\u629b\u51fa\u5f02\u5e38(\u5305\u62ecIgnoreRequest\u5f02\u5e38)\u65f6\uff0cScrapy\u8c03\u7528 process_exception() \u3002process_exception() \u5e94\u8be5\u8fd4\u56de\u4ee5\u4e0b\u4e4b\u4e00: \u8fd4\u56de None \u3001 \u4e00\u4e2a Response \u5bf9\u8c61\u3001\u6216\u8005\u4e00\u4e2a Request \u5bf9\u8c61\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from scrapy.downloadermiddlewares.retry import RetryMiddleware from scrapy.utils.response import response_status_message class CustomRetryMiddleware ( RetryMiddleware ): #RetryMiddleware\u7c7b\u91cc\u6709\u4e2a\u5e38\u91cf\uff0c\u8bb0\u5f55\u4e86\u8fde\u63a5\u8d85\u65f6\u90a3\u4e9b\u5f02\u5e38 #EXCEPTIONS_TO_RETRY = (defer.TimeoutError, TimeoutError, DNSLookupError, # ConnectionRefusedError, ConnectionDone, ConnectError, # ConnectionLost, TCPTimedOutError, ResponseFailed, # IOError, TunnelError) def process_exception ( self , request , exception , spider ): if isinstance ( exception , self . EXCEPTIONS_TO_RETRY ) and not request . meta . get ( 'dont_retry' , False ): #\u8fd9\u91cc\u53ef\u4ee5\u5199\u51fa\u73b0\u5f02\u5e38\u90a3\u4e9b\u4f60\u7684\u5904\u7406 proxy = random . choice ( proxy_list ) request . meta [ 'proxy' ] = proxy time . sleep ( random . randint ( 3 , 5 )) return self . _retry ( request , exception , spider ) #_retry\u662fRetryMiddleware\u4e2d\u7684\u4e00\u4e2a\u79c1\u6709\u65b9\u6cd5\uff0c\u4e3b\u8981\u4f5c\u7528\u662f #1.\u5bf9request.meta\u4e2d\u7684retry_time\u8fdb\u884c+1 #2.\u5c06retry_times\u548cmax_retry_time\u8fdb\u884c\u6bd4\u8f83\uff0c\u5982\u679c\u524d\u8005\u5c0f\u4e8e\u7b49\u4e8e\u540e\u8005\uff0c\u5229\u7528copy\u65b9\u6cd5\u5728\u539f\u6765\u7684request\u4e0a\u590d\u5236\u4e00\u4e2a\u65b0request\uff0c\u5e76\u66f4\u65b0\u5176retry_times\uff0c\u5e76\u5c06dont_filter\u8bbe\u4e3aTrue\u6765\u9632\u6b62\u56e0url\u91cd\u590d\u800c\u88ab\u8fc7\u6ee4\u3002 #3.\u8bb0\u5f55\u91cd\u8bd5reason \u5982\u679c\u5176\u8fd4\u56de None \uff0cScrapy\u5c06\u4f1a\u7ee7\u7eed\u5904\u7406\u8be5\u5f02\u5e38\uff0c\u63a5\u7740\u8c03\u7528\u5df2\u5b89\u88c5\u7684\u5176\u4ed6\u4e2d\u95f4\u4ef6\u7684 process_exception() \u65b9\u6cd5\uff0c\u76f4\u5230\u6240\u6709\u4e2d\u95f4\u4ef6\u90fd\u88ab\u8c03\u7528\u5b8c\u6bd5\uff0c\u5219\u8c03\u7528\u9ed8\u8ba4\u7684\u5f02\u5e38\u5904\u7406\u3002 \u5982\u679c\u5176\u8fd4\u56de\u4e00\u4e2a Response \u5bf9\u8c61\uff0c\u5219\u5df2\u5b89\u88c5\u7684\u4e2d\u95f4\u4ef6\u94fe\u7684 process_response() \u65b9\u6cd5\u88ab\u8c03\u7528\u3002Scrapy\u5c06\u4e0d\u4f1a\u8c03\u7528\u4efb\u4f55\u5176\u4ed6\u4e2d\u95f4\u4ef6\u7684 process_exception() \u65b9\u6cd5\u3002 \u5982\u679c\u5176\u8fd4\u56de\u4e00\u4e2a Request \u5bf9\u8c61\uff0c \u5219\u8fd4\u56de\u7684request\u5c06\u4f1a\u88ab\u91cd\u65b0\u8c03\u7528\u4e0b\u8f7d\u3002\u8fd9\u5c06\u505c\u6b62\u4e2d\u95f4\u4ef6\u7684 process_exception() \u65b9\u6cd5\u6267\u884c\uff0c\u5c31\u5982\u8fd4\u56de\u4e00\u4e2aresponse\u7684\u90a3\u6837\u3002 \u53c2\u6570 1. request (\u662f Request \u5bf9\u8c61) \u2013 \u4ea7\u751f\u5f02\u5e38\u7684request 2. exception (Exception \u5bf9\u8c61) \u2013 \u629b\u51fa\u7684\u5f02\u5e38 3. spider (Spider \u5bf9\u8c61) \u2013 request\u5bf9\u5e94\u7684spider","title":"process_exception \u65b9\u6cd5"},{"location":"basics/scrapy/04_scrapy_middlewares/04_scrapy_middlewares/#spider","text":"\u4ece\u67b6\u6784\u56fe\u4e2d\u6211\u4eec\u53ef\u4ee5\u77e5\u9053spider\u4e2d\u95f4\u4ef6\u662f\u7528\u4e8e\u5904\u7406response \u4ee5\u53caspider \u751f\u6210\u7684item\u548cRequest \u542f\u7528spider\u4e2d\u95f4\u4ef6\u6211\u4eec\u53ef\u4ee5\u5728setting\u4e2d\u8bbe\u7f6e settings 1 2 3 4 SPIDER_MIDDLEWARES = { 'myproject.middlewares.CustomSpiderMiddleware': 543, 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': None, } \u6216\u8005\u662f\u5728spider\u4e2d\u8bbe\u7f6e custom_settings 1 2 3 4 5 6 7 8 9 10 11 12 class Spider ( scrapy . Spider ): allowed_domains = [ 'xxx' ] start_urls = [ 'xxx' ,] custom_settings = { \"SPIDER_MIDDLEWARES\" : { 'myproject.middlewares.CustomSpiderMiddleware' : 543 , }, }","title":"spider\u4e2d\u95f4\u4ef6"},{"location":"basics/scrapy/04_scrapy_middlewares/04_scrapy_middlewares/#spider_1","text":"process_spider_input(response, spider) \u8fd9\u4e2a\u65b9\u6cd5\u65f6\u5728\u4e0b\u8f7d\u5668\u4e2d\u95f4\u4ef6\u5904\u7406\u5b8c\u6210\u540e\uff0c\u9a6c\u4e0a\u8981\u8fdb\u5165\u67d0\u4e2a\u56de\u8c03\u51fd\u6570parse_xxx()\u524d\u8c03\u7528\u3002process_spider_output(response, result, output)\u5728\u8fd9\u4e2a\u65b9\u6cd5\u5904\u7406\u5b8c\u6210\u4ee5\u540e\uff0c\u6570\u636e\u5982\u679c\u662fitem\uff0c\u5c31\u4f1a\u88ab\u4ea4\u7ed9pipeline\uff1b\u5982\u679c\u662f\u8bf7\u6c42\uff0c\u5c31\u4f1a\u88ab\u4ea4\u7ed9\u8c03\u5ea6\u5668\uff0c\u7136\u540e\u4e0b\u8f7d\u5668\u4e2d\u95f4\u4ef6\u624d\u4f1a\u5f00\u59cb\u8fd0\u884c\u3002\u8fd9\u4e2a\u65b9\u6cd5\u7684\u53c2\u6570result\u5c31\u662f\u722c\u866b\u722c\u51fa\u6765\u7684item\u6216\u8005scrapy.Request()\u3002\u7531\u4e8eyield\u5f97\u5230\u7684\u662f\u4e00\u4e2a\u751f\u6210\u5668\uff0c\u751f\u6210\u5668\u662f\u53ef\u4ee5\u8fed\u4ee3\u7684\uff0c\u6240\u4ee5result\u4e5f\u662f\u53ef\u4ee5\u8fed\u4ee3\u7684\uff0c\u53ef\u4ee5\u4f7f\u7528for\u5faa\u73af\u6765\u628a\u5b83\u5c55\u5f00\u3002 process_spider_output(response, result, spider) \u662f\u5728\u722c\u866b\u8fd0\u884cyield item \u6216\u662fyield scrapy.Request() \u7684\u65f6\u5019\u88ab\u8c03\u7528\u3002\u4e00\u822c\u8fd4\u56deresult process_spider_exception(response, exception, spider) \u5f53spider\u4e2d\u95f4\u4ef6\u629b\u51fa\u5f02\u5e38\u65f6\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u88ab\u8c03\u7528\uff0c\u8fd4\u56deNone\u6216\u53ef\u8fed\u4ee3\u5bf9\u8c61\u7684Request,dict,item","title":"\u7f16\u5199\u81ea\u5b9a\u4e49spider\u4e2d\u95f4\u4ef6"},{"location":"basics/scrapy/04_scrapy_middlewares/04_scrapy_middlewares/#_4","text":"\u4e2d\u95f4\u4ef6\u7684\u6574\u4f53\u6d41\u7a0b\u5982\u4e0b \u4e2d\u95f4\u4ef6\u5927\u5927\u4e30\u5bcc\u4e86\u6211\u4eec\u7684\u9700\u8981\u7684\u64cd\u4f5c\uff01\u76f8\u5f53\u4e8e\u94a9\u5b50\u628a\uff01\u628a\u4e00\u4e9b\u5728\u4e1a\u52a1\u5904\u7406\u4e0a\u96be\u4ee5\u89e3\u51b3\u7684\u95ee\u9898\u7528\u4e2d\u95f4\u4ef6\u5904\u7406\u662f\u975e\u5e38\u7b80\u5355\u5730\u3002","title":"\u603b\u7ed3"},{"location":"basics/scrapy/04_scrapy_middlewares/04_scrapy_middlewares/#_5","text":"https://zhuanlan.zhihu.com/p/42498126 https://www.cnblogs.com/fengf233/p/11453375.html https://www.kingname.info/2018/11/20/know-middleware-of-scrapy-3/","title":"\u53c2\u8003\u94fe\u63a5"},{"location":"basics/scrapy/05_scrapy_piplines/05_scrapy_piplines/","text":"","title":"Scrapy \u6e90\u7801 pipelines"},{"location":"basics/scrapy/06_scrapy_log/06_scrapy_log/","text":"\u65e5\u5fd7 \u00b6 \u6ca1\u6709\u4e86\u65e5\u5fd7\u5c31\u597d\u50cf\u76f2\u4eba\u8d70\u8def\u4e00\u6837\uff0c\u6392\u67e5\u95ee\u9898\u4e5f\u662f\u6781\u5176\u4e0d\u65b9\u4fbf\uff0c\u4e0b\u9762\u6211\u4eec\u6765\u4ecb\u7ecd\u4e0bscrapy\u65e5\u5fd7\u8be5\u5982\u4f55\u914d\u7f6e\u5427\uff01 \u89e3\u91ca \u00b6 \u65e5\u5fd7\u67095\u79cd\uff0c\u6309\u7167\u4e25\u91cd\u7a0b\u5ea6\u6392\u5e8f\uff1a 1 DEBUG < INFO < WARNING < ERROR < CRITICAL \u4f7f\u7528\u65b9\u6cd5 setting \u4e2d\u8bbe\u7f6e 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # \u662f\u5426\u542f\u7528\u65e5\u5fd7 LOG_ENABLED = True # \u65e5\u5fd7\u4f7f\u7528\u7684\u7f16\u7801 LOG_ENCODING = 'utf-8' # \u65e5\u5fd7\u6587\u4ef6(\u6587\u4ef6\u540d) LOG_FILE = None # \u65e5\u5fd7\u683c\u5f0f LOG_FORMAT = ' %(asctime)s [ %(name)s ] %(levelname)s : %(message)s ' # \u65e5\u5fd7\u65f6\u95f4\u683c\u5f0f LOG_DATEFORMAT = '%Y-%m- %d %H:%M:%S' # \u65e5\u5fd7\u7ea7\u522b CRITICAL, ERROR, WARNING, INFO, DEBUG\uff0c\u5982\u679c\u8bbe\u7f6e\u4e3aDEBUG\u7684\u8bdd\u5176\u4ed6\u4e5f\u662fBUG\u51fa\u73b0\u90fd\u662f\u4f1a\u5b58\u653e\u548c\u6253\u5370\u51fa\u6765\u7684\u3002\u4f46\u662f\u5982\u679c\u8bbe\u7f6e\u4e86INFO\uff0cDEUG\u7684\u65e5\u5fd7\u5c31\u4f1a\u5ffd\u89c6 LOG_LEVEL = 'DEBUG' # \u5982\u679c\u7b49\u4e8eTrue\uff0c\u6240\u6709\u7684\u6807\u51c6\u8f93\u51fa\uff08\u5305\u62ec\u9519\u8bef\uff09\u90fd\u4f1a\u91cd\u5b9a\u5411\u5230\u65e5\u5fd7\uff0c\u4f8b\u5982\uff1aprint('hello') LOG_STDOUT = False # \u5982\u679c\u7b49\u4e8eTrue\uff0c\u65e5\u5fd7\u4ec5\u4ec5\u5305\u542b\u6839\u8def\u5f84\uff0cFalse\u663e\u793a\u65e5\u5fd7\u8f93\u51fa\u7ec4\u4ef6 LOG_SHORT_NAMES = False \u914d\u7f6e\u793a\u4f8b \u53ef\u4ee5\u5728setting.py\u4e2d\u6216\u8005\u8bf4\u5728custom_setting\u4e2d\u8bbe\u7f6e 1 2 3 4 5 6 7 8 9 10 11 # setting.py from datetime import datetime # \u6587\u4ef6\u53ca\u8def\u5f84\uff0clog\u76ee\u5f55\u9700\u8981\u5148\u5efa\u597d today = datetime . now () log_file_path = \"log/scrapy_{}_{}_{}.log\" . format ( today . year , today . month , today . day ) # \u65e5\u5fd7\u8f93\u51fa LOG_LEVEL = 'DEBUG' LOG_FILE = log_file_path \u4f7f\u7528\u6559\u7a0b 1 2 3 import logging logger = logging . getLogger ( __name__ ) logger . warning ( \"This is a warning\" ) 1 2 3 4 5 6 7 8 9 import scrapy class MySpider ( scrapy . Spider ): name = 'myspider' start_urls = [ 'https://scrapinghub.com' ] def parse ( self , response ): self . logger . info ( 'Parse function called on %s ' , response . url ) \u7136\u540e \u53c2\u8003\u94fe\u63a5 \u00b6 https://docs.scrapy.org/en/latest/topics/logging.html https://juejin.im/post/5aee70105188256712786b7f","title":"Scrapy \u6e90\u7801 log"},{"location":"basics/scrapy/06_scrapy_log/06_scrapy_log/#_1","text":"\u6ca1\u6709\u4e86\u65e5\u5fd7\u5c31\u597d\u50cf\u76f2\u4eba\u8d70\u8def\u4e00\u6837\uff0c\u6392\u67e5\u95ee\u9898\u4e5f\u662f\u6781\u5176\u4e0d\u65b9\u4fbf\uff0c\u4e0b\u9762\u6211\u4eec\u6765\u4ecb\u7ecd\u4e0bscrapy\u65e5\u5fd7\u8be5\u5982\u4f55\u914d\u7f6e\u5427\uff01","title":"\u65e5\u5fd7"},{"location":"basics/scrapy/06_scrapy_log/06_scrapy_log/#_2","text":"\u65e5\u5fd7\u67095\u79cd\uff0c\u6309\u7167\u4e25\u91cd\u7a0b\u5ea6\u6392\u5e8f\uff1a 1 DEBUG < INFO < WARNING < ERROR < CRITICAL \u4f7f\u7528\u65b9\u6cd5 setting \u4e2d\u8bbe\u7f6e 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # \u662f\u5426\u542f\u7528\u65e5\u5fd7 LOG_ENABLED = True # \u65e5\u5fd7\u4f7f\u7528\u7684\u7f16\u7801 LOG_ENCODING = 'utf-8' # \u65e5\u5fd7\u6587\u4ef6(\u6587\u4ef6\u540d) LOG_FILE = None # \u65e5\u5fd7\u683c\u5f0f LOG_FORMAT = ' %(asctime)s [ %(name)s ] %(levelname)s : %(message)s ' # \u65e5\u5fd7\u65f6\u95f4\u683c\u5f0f LOG_DATEFORMAT = '%Y-%m- %d %H:%M:%S' # \u65e5\u5fd7\u7ea7\u522b CRITICAL, ERROR, WARNING, INFO, DEBUG\uff0c\u5982\u679c\u8bbe\u7f6e\u4e3aDEBUG\u7684\u8bdd\u5176\u4ed6\u4e5f\u662fBUG\u51fa\u73b0\u90fd\u662f\u4f1a\u5b58\u653e\u548c\u6253\u5370\u51fa\u6765\u7684\u3002\u4f46\u662f\u5982\u679c\u8bbe\u7f6e\u4e86INFO\uff0cDEUG\u7684\u65e5\u5fd7\u5c31\u4f1a\u5ffd\u89c6 LOG_LEVEL = 'DEBUG' # \u5982\u679c\u7b49\u4e8eTrue\uff0c\u6240\u6709\u7684\u6807\u51c6\u8f93\u51fa\uff08\u5305\u62ec\u9519\u8bef\uff09\u90fd\u4f1a\u91cd\u5b9a\u5411\u5230\u65e5\u5fd7\uff0c\u4f8b\u5982\uff1aprint('hello') LOG_STDOUT = False # \u5982\u679c\u7b49\u4e8eTrue\uff0c\u65e5\u5fd7\u4ec5\u4ec5\u5305\u542b\u6839\u8def\u5f84\uff0cFalse\u663e\u793a\u65e5\u5fd7\u8f93\u51fa\u7ec4\u4ef6 LOG_SHORT_NAMES = False \u914d\u7f6e\u793a\u4f8b \u53ef\u4ee5\u5728setting.py\u4e2d\u6216\u8005\u8bf4\u5728custom_setting\u4e2d\u8bbe\u7f6e 1 2 3 4 5 6 7 8 9 10 11 # setting.py from datetime import datetime # \u6587\u4ef6\u53ca\u8def\u5f84\uff0clog\u76ee\u5f55\u9700\u8981\u5148\u5efa\u597d today = datetime . now () log_file_path = \"log/scrapy_{}_{}_{}.log\" . format ( today . year , today . month , today . day ) # \u65e5\u5fd7\u8f93\u51fa LOG_LEVEL = 'DEBUG' LOG_FILE = log_file_path \u4f7f\u7528\u6559\u7a0b 1 2 3 import logging logger = logging . getLogger ( __name__ ) logger . warning ( \"This is a warning\" ) 1 2 3 4 5 6 7 8 9 import scrapy class MySpider ( scrapy . Spider ): name = 'myspider' start_urls = [ 'https://scrapinghub.com' ] def parse ( self , response ): self . logger . info ( 'Parse function called on %s ' , response . url ) \u7136\u540e","title":"\u89e3\u91ca"},{"location":"basics/scrapy/06_scrapy_log/06_scrapy_log/#_3","text":"https://docs.scrapy.org/en/latest/topics/logging.html https://juejin.im/post/5aee70105188256712786b7f","title":"\u53c2\u8003\u94fe\u63a5"},{"location":"basics/scrapy/07_scrapy_redis/07_scrapy_redis/","text":"Scrapy-redis \u00b6 \u4f5c\u4e3a\u5bf9scrapy \u6846\u67b6\u7684\u8865\u5145\u3002\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u57fa\u7840\uff0c\u6211\u4eec\u5b9e\u9645\u6765\u770b\u4e00\u770b\u4ed6\u5230\u5e95\u5bf9scrapy\u505a\u4e86\u54ea\u4e9b\u8865\u5145\uff0c\u4e3a\u4ec0\u4e48\u5982\u6b64\u53d7\u6b22\u8fce \u4ece\u67b6\u6784\u56fe\u4e2d\u6211\u4eec\u53ef\u4ee5\u77e5\u9053\u4ed6\u662f\u5bf9scrapy\u672c\u8eab\u7684scheduler\u8fdb\u884c\u4e86\u6539\u9020\u3002\u90a3\u5230\u5e95\u4ec0\u4e48\u662fscheduleer(\u8c03\u5ea6\u5668)\u5462\uff1f\u5148\u67e5\u4e0b\u4ed6\u7684\u6e90\u7801 scrapy.core.scheduler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 # scrapy.core.scheduler.py: class Scheduler ( object ): \"\"\" Scrapy Scheduler. It allows to enqueue requests and then get a next request to download. Scheduler is also handling duplication filtering, via dupefilter. Prioritization and queueing is not performed by the Scheduler. User sets ``priority`` field for each Request, and a PriorityQueue (defined by :setting:`SCHEDULER_PRIORITY_QUEUE`) uses these priorities to dequeue requests in a desired order. Scheduler uses two PriorityQueue instances, configured to work in-memory and on-disk (optional). When on-disk queue is present, it is used by default, and an in-memory queue is used as a fallback for cases where a disk queue can't handle a request (can't serialize it). :setting:`SCHEDULER_MEMORY_QUEUE` and :setting:`SCHEDULER_DISK_QUEUE` allow to specify lower-level queue classes which PriorityQueue instances would be instantiated with, to keep requests on disk and in memory respectively. Overall, Scheduler is an object which holds several PriorityQueue instances (in-memory and on-disk) and implements fallback logic for them. Also, it handles dupefilters. \"\"\" def __init__ ( self , dupefilter , jobdir = None , dqclass = None , mqclass = None , logunser = False , stats = None , pqclass = None , crawler = None ): self . df = dupefilter self . dqdir = self . _dqdir ( jobdir ) self . pqclass = pqclass self . dqclass = dqclass self . mqclass = mqclass self . logunser = logunser self . stats = stats self . crawler = crawler # \u53ef\u4ee5\u770b\u5230\u8fd9\u662f\u4e00\u4e2a\u94a9\u5b50\uff0c\u4ed6\u80fd\u8bbf\u95ee\u5f53\u524d\u722c\u866b\u7684\u914d\u7f6e\uff0c\u7136\u540e\u8f7d\u5165 @ classmethod def from_crawler ( cls , crawler ): settings = crawler . settings # \u83b7\u53d6\u53bb\u91cd\u7528\u7684\u7c7b\uff0c\u9ed8\u8ba4\u662fscrapy.dupefilters.RFPDupeFilter dupefilter_cls = load_object ( settings [ 'DUPEFILTER_CLASS' ]) # \u6211\u4eec\u53ef\u4ee5\u8ddf\u8e2a\u4e0bload_obj()\u8fd9\u4e2a\u65b9\u6cd5 # def load_object(path): # \"\"\"Load an object given its absolute object path, and return it. # # object can be a class, function, variable or an instance. # path ie: 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware' # # # \u8fd4\u56de\u8def\u5f84\u6307\u5b9a\u7684\u7c7b\u5bf9\u8c61\uff0c\u6bd4\u5982path:scrapy.downloadermiddlewares.redirect.RedirectMiddleware\uff0c\u4ed6\u8fd4\u56de\u7684\u662f # RedirectMiddleware\u8fd9\u4e2a\u5bf9\u8c61 # \"\"\" # # try: # # \u8fd4\u56de\u5b50\u5b57\u7b26\u4e32 str \u5728\u5b57\u7b26\u4e32\u4e2d\u6700\u540e\u51fa\u73b0\u7684\u4f4d\u7f6e\uff0c\u51fa\u73b0\u5728\u5b57\u7b26\u4e32path\u4e2d\u51fa\u73b0\u7684\u4f4d\u7f6e\uff0c\u5982\u679c\u6ca1\u6709\u5339\u914d\u7684\u5b57\u7b26\u4e32\u4f1a\u62a5\u5f02\u5e38\u3002 # dot = path.rindex('.') # except ValueError: # raise ValueError(\"Error loading object '%s': not a full path\" % path) # # # \u8fd9\u91cc\u5c06\u6a21\u5757\u548c\u540d\u5b57\u7ed9\u51fa\u5566\uff0c\u6bd4\u5982\u6b63\u5e38\u5c31\u8fd4\u56demodel=scrapy.spiderloader|||name=SpiderLoader # module, name = path[:dot], path[dot + 1:] # mod = import_module(module) # # try: # obj = getattr(mod, name) # except AttributeError: # raise NameError(\"Module '%s' doesn't define any object named '%s'\" % (module, name)) # # return obj # \u521b\u5efa\u4e00\u4e2a\u5b9e\u4f8b\u5bf9\u8c61\uff0c\u5bf9\u5bf9\u8c61\u7684\u4e00\u4e9b\u5c5e\u6027\u505a\u4e00\u4e9b\u5224\u65ad dupefilter = create_instance ( dupefilter_cls , settings , crawler ) # \u83b7\u53d6\u4f18\u5148\u7ea7\u961f\u5217\uff0c \u9ed8\u8ba4\uff1aqueuelib.pqueue.PriorityQueue\uff0c\u7c7b\u5bf9\u8c61\uff08SCHEDULER_PRIORITY_QUEUE\uff09 pqclass = load_object ( settings [ 'SCHEDULER_PRIORITY_QUEUE' ]) # \u5224\u65ad\u4e00\u4e9b\u83b7\u5f97\u7684\u8fd9\u4e2apqclass\u961f\u5217\u5bf9\u8c61\u662f\u5c31\u662fPriorityQueue\u8fd9\u4e2a\u5bf9\u8c61\uff0c\u662f\u7684\u8bdd\u53d1\u51fa\u8b66\u544a\uff0c\u5e76\u5c06ScrapyPriorityQueue\u8fd9\u4e2a\u5bf9\u8c61 # \u8d4b\u503c\u7ed9ScrapyPriorityQueue if pqclass is PriorityQueue : warnings . warn ( \"SCHEDULER_PRIORITY_QUEUE='queuelib.PriorityQueue'\" \" is no longer supported because of API changes; \" \"please use 'scrapy.pqueues.ScrapyPriorityQueue'\" , ScrapyDeprecationWarning ) from scrapy . pqueues import ScrapyPriorityQueue pqclass = ScrapyPriorityQueue # \u83b7\u53d6\u78c1\u76d8\u961f\u5217 \u7c7b\u5bf9\u8c61\uff08SCHEDULER_DISK_QUEUE,\u91cd\u542f\u4e0d\u4f1a\u4e22\u5931\uff09 dqclass = load_object ( settings [ 'SCHEDULER_DISK_QUEUE' ]) # \u83b7\u53d6\u5185\u5b58\u961f\u5217 \u7c7b\u5bf9\u8c61(SCHEDULER \u4f7f\u7528\u5185\u5b58\u5b58\u50a8\uff0c\u91cd\u542f\u4f1a\u4e22\u5931) mqclass = load_object ( settings [ 'SCHEDULER_MEMORY_QUEUE' ]) # \u662f\u5426\u5f00\u542fdebug logunser = settings . getbool ( 'LOG_UNSERIALIZABLE_REQUESTS' , settings . getbool ( 'SCHEDULER_DEBUG' )) # \u5c06\u8fd9\u4e9b\u53c2\u6570\u4f20\u9012\u7ed9 __init__ \u65b9\u6cd5 return cls ( dupefilter , jobdir = job_dir ( settings ), logunser = logunser , stats = crawler . stats , pqclass = pqclass , dqclass = dqclass , mqclass = mqclass , crawler = crawler ) # \u68c0\u67e5\u662f\u5426\u6709\u6ca1\u6709\u5904\u7406\u7684\u8bf7\u6c42 def has_pending_requests ( self ): return len ( self ) > 0 # Engin\u521b\u5efa\u5b8c\u4e4b\u540e\u4f1a\u8c03\u7528\u8fd9\u4e2a\u65b9\u6cd5 def open ( self , spider ): self . spider = spider # \u521b\u5efa\u4e00\u4e2a\u6709\u4f18\u5148\u7ea7\u7684\u5185\u5b58\u961f\u5217 \u5b9e\u4f8b\u5316\u5bf9\u8c61 # self.pqclass \u9ed8\u8ba4\u662f\uff1aqueuelib.pqueue.PriorityQueue # self._newmq \u4f1a\u8fd4\u56de\u4e00\u4e2a\u5185\u5b58\u961f\u5217\u7684 \u5b9e\u4f8b\u5316\u5bf9\u8c61 \u5728110 111 \u884c self . mqs = self . _mq () # \u5982\u679cself.dqdir \u6709\u8bbe\u7f6e \u5c31\u521b\u5efa\u4e00\u4e2a\u78c1\u76d8\u961f\u5217 \u5426\u5219self.dqs \u4e3a\u7a7a self . dqs = self . _dq () if self . dqdir else None # \u83b7\u5f97\u4e00\u4e2a\u53bb\u91cd\u5b9e\u4f8b\u5bf9\u8c61 open \u65b9\u6cd5\u662f\u4eceBaseDupeFilter\u7ee7\u627f\u7684 # \u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u7528self.df\u6765\u53bb\u91cd\u5566 return self . df . open () # \u5f53Engine\u5173\u95ed\u7684\u65f6\u5019 def close ( self , reason ): # \u5982\u679c\u6709\u78c1\u76d8\u961f\u5217 \u5219\u5bf9\u5176\u8fdb\u884cdump\u540e\u4fdd\u5b58\u5230active.json\u6587\u4ef6\u4e2d if self . dqs : state = self . dqs . close () self . _write_dqs_state ( self . dqdir , state ) # \u7136\u540e\u5173\u95ed\u53bb\u91cd return self . df . close ( reason ) # \u6dfb\u52a0\u4e00\u4e2aRequest\u8fdb\u8c03\u5ea6\u961f\u5217 def enqueue_request ( self , request ): if not request . dont_filter and self . df . request_seen ( request ): # \u5982\u679cRequest\u7684dont_filter\u5c5e\u6027\u6ca1\u6709\u8bbe\u7f6e\uff08\u9ed8\u8ba4\u4e3aFalse\uff09\u548c \u5df2\u7ecf\u5b58\u5728\u5219\u53bb\u91cd # \u4e0dpush\u8fdb\u961f\u5217 self . df . log ( request , self . spider ) return False # \u5148\u5c1d\u8bd5\u5c06Request push\u8fdb\u78c1\u76d8\u961f\u5217 dqok = self . _dqpush ( request ) if dqok : # \u5982\u679c\u6210\u529f \u5219\u5728\u8bb0\u5f55\u4e00\u6b21\u72b6\u6001 self . stats . inc_value ( 'scheduler/enqueued/disk' , spider = self . spider ) else : # \u4e0d\u80fd\u6dfb\u52a0\u8fdb\u78c1\u76d8\u961f\u5217\u5219\u4f1a\u6dfb\u52a0\u8fdb\u5185\u5b58\u961f\u5217 self . _mqpush ( request ) self . stats . inc_value ( 'scheduler/enqueued/memory' , spider = self . spider ) self . stats . inc_value ( 'scheduler/enqueued' , spider = self . spider ) return True # \u4ece\u961f\u5217\u4e2d\u83b7\u53d6\u4e00\u4e2aRequest def next_request ( self ): request = self . mqs . pop () if request : self . stats . inc_value ( 'scheduler/dequeued/memory' , spider = self . spider ) else : # \u4e0d\u80fd\u83b7\u53d6\u7684\u65f6\u5019\u4ece\u78c1\u76d8\u961f\u5217\u961f\u91cc\u83b7\u53d6 request = self . _dqpop () if request : self . stats . inc_value ( 'scheduler/dequeued/disk' , spider = self . spider ) if request : self . stats . inc_value ( 'scheduler/dequeued' , spider = self . spider ) # \u5c06\u83b7\u53d6\u7684\u5230Request\u8fd4\u56de\u7ed9Engine return request def __len__ ( self ): return len ( self . dqs ) + len ( self . mqs ) if self . dqs else len ( self . mqs ) def _dqpush ( self , request ): if self . dqs is None : return try : self . dqs . push ( request , - request . priority ) except ValueError as e : # non serializable request if self . logunser : msg = ( \"Unable to serialize request: %(request)s - reason:\" \" %(reason)s - no more unserializable requests will be\" \" logged (stats being collected)\" ) logger . warning ( msg , { 'request' : request , 'reason' : e } , exc_info = True , extra = { 'spider' : self . spider } ) self . logunser = False self . stats . inc_value ( 'scheduler/unserializable' , spider = self . spider ) return else : return True def _mqpush ( self , request ): self . mqs . push ( request , - request . priority ) def _dqpop ( self ): if self . dqs : return self . dqs . pop () def _newmq ( self , priority ): \"\"\" Factory for creating memory queues. \"\"\" return self . mqclass () def _newdq ( self , priority ): \"\"\" Factory for creating disk queues. \"\"\" path = join ( self . dqdir , 'p%s' % ( priority , )) return self . dqclass ( path ) def _mq ( self ): \"\"\" Create a new priority queue instance, with in-memory storage \"\"\" return create_instance ( self . pqclass , None , self . crawler , self . _newmq , serialize = False ) def _dq ( self ): \"\"\" Create a new priority queue instance, with disk storage \"\"\" state = self . _read_dqs_state ( self . dqdir ) q = create_instance ( self . pqclass , None , self . crawler , self . _newdq , state , serialize = True ) if q : logger . info ( \"Resuming crawl (%(queuesize)d requests scheduled)\" , { 'queuesize' : len ( q ) } , extra = { 'spider' : self . spider } ) return q def _dqdir ( self , jobdir ): \"\"\" Return a folder name to keep disk queue state at \"\"\" if jobdir : dqdir = join ( jobdir , 'requests.queue' ) if not exists ( dqdir ): os . makedirs ( dqdir ) return dqdir def _read_dqs_state ( self , dqdir ): path = join ( dqdir , 'active.json' ) if not exists ( path ): return () with open ( path ) as f : return json . load ( f ) def _write_dqs_state ( self , dqdir , state ): with open ( join ( dqdir , 'active.json' ), 'w' ) as f : json . dump ( state , f ) \u6211\u4eec\u77e5\u9053\u4e86\u8fd9\u4e2aSCHEDULER(\u8c03\u5ea6\u5668)\u4e3b\u8981\u662f\u5b8c\u6210\u4e86push Request pop Request\u53bb\u91cd\u64cd\u4f5c\uff0c\u800c\u4e14\u662fqueue(\u961f\u5217)\u64cd\u4f5c\u662f\u5728\u5185\u5b58\u961f\u5217\u4e2d\u5b8c\u6210\u7684 \u53bb\u91cd\u7684\u6e90\u7801\u6211\u4eec\u8fdb\u4e00\u6b65\u4e86\u89e3\u4e0b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 # scrapy . core . scheduler . py : class RFPDupeFilter ( BaseDupeFilter ) : \"\"\"\u8bf7\u6c42\u6307\u7eb9\u91cd\u590d\u7b5b\u9009\"\"\" def __init__ ( self , path = None , debug = False ) : self . file = None # \u96c6\u5408\u53bb\u91cd\u554a \uff01 \u96c6\u5408\u90a3\u5c31\u662f\u5728\u5185\u5b58\u4e2d \uff0c \u8fd9\u70b9\u5f88\u91cd\u8981 self . fingerprints = set () self . logdupes = True self . debug = debug self . logger = logging . getLogger ( __name__ ) if path : # \u53bb\u770b\u5230\u53bb\u91cd\u5176\u5b9e\u6253\u5f00\u4e86\u4e00\u4e2arequests . seen\u7684\u6587\u4ef6 # \u5982\u679c\u662f\u4f7f\u7528\u7684\u78c1\u76d8\u7684\u8bdd self . file = open ( os . path . join ( path , 'requests.seen' ), 'a+' ) self . file . seek ( 0 ) #\u5c06\u6587\u4ef6\u4e2d\u7684\u8bf7\u6c42\u66f4\u65b0\u5230\u5185\u5b58\u96c6\u5408\u4e2d\u53bb self . fingerprints . update ( x . rstrip () for x in self . file ) @classmethod def from_settings ( cls , settings ) : debug = settings . getbool ( 'DUPEFILTER_DEBUG' ) return cls ( job_dir ( settings ), debug ) def request_seen ( self , request ) : fp = self . request_fingerprint ( request ) if fp in self . fingerprints : # \u5224\u65ad\u6211\u4eec\u7684\u8bf7\u6c42\u662f\u5426\u5728\u8fd9\u4e2a\u96c6\u5408\u4e2d return True self . fingerprints . add ( fp ) # \u5982\u679c\u7528\u7684\u78c1\u76d8\u961f\u5217\u5c31\u5199\u8fdb\u53bb\u8bb0\u5f55\u4e00\u4e0b if self . file : self . file . write ( fp + os . linesep ) def request_fingerprint ( self , request ) : return request_fingerprint ( request ) def close ( self , reason ) : if self . file : self . file . close () #log\u65e5\u5fd7 def log ( self , request , spider ) : if self . debug : msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\" args = { 'request' : request , 'referer' : referer_str ( request ) } self . logger . debug ( msg , args , extra = { 'spider' : spider } ) elif self . logdupes : msg = ( \"Filtered duplicate request: %(request)s\" \" - no more duplicates will be shown\" \" (see DUPEFILTER_DEBUG to show all duplicates)\" ) self . logger . debug ( msg , { 'request' : request } , extra = { 'spider' : spider } ) self . logdupes = False spider . crawler . stats . inc_value ( 'dupefilter/filtered' , spider = spider ) 1 \u53bb\u91cd\u6211\u4eec\u77e5\u9053\u4e86\uff01\u4e00\u4e2a\u662f\u5229\u7528\u78c1\u76d8\u6253\u5f00\u4e2a\u6587\u4ef6\uff0c\u8fd9\u6837\u5c31\u80fd\u5173\u95ed\u722c\u866b\u5c31\u4e0d\u4f1a\u4e22\u5931\u6570\u636e\u4e86\uff0c\u4e00\u4e2a\u5c31\u662f\u5229\u7528\u96c6\u5408\u4f7f\u7528\u5185\u5b58\u7684\u65b9\u5f0f\u53bb\u91cd\uff0c\u4f18\u70b9\u662f\u5feb\uff0c\u4f46\u662f\u5173\u95ed\u722c\u866b\u5c31\u4f1a\u4e22\u5931\u6570\u636e\u3002 scrapy-redis\u6b63\u5f0f\u767b\u573a \u00b6 \u6211\u4eec\u4e86\u89e3\u4e86scrapy \u7684\u8c03\u5ea6\u5668\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u60c5\u51b5\u4e0b\u6211\u4eec\u80fd\u5426\u6539\u88c5\u4e0b\u5462\uff1f\u90a3\u662f\u53ef\u4ee5\u7684 \u6211\u4eec\u53ef\u4ee5\u770b\u4e0b\u4ed6\u7684\u6e90\u7801 1 git clone https://github.com/rmax/scrapy-redis.git \u4ed6\u7684\u6e90\u7801\u662f\u5728src\u4e0b \u5148\u770b\u4ed6\u91cd\u5199scheduler(\u8c03\u5ea6\u5668) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class Scheduler ( object ) : \"\"\"Redis-based scheduler Settings -------- SCHEDULER_PERSIST : bool (default: False) Whether to persist or clear redis queue. SCHEDULER_FLUSH_ON_START : bool (default: False) Whether to flush redis queue on start. SCHEDULER_IDLE_BEFORE_CLOSE : int (default: 0) How many seconds to wait before closing if no message is received. SCHEDULER_QUEUE_KEY : str Scheduler redis key. SCHEDULER_QUEUE_CLASS : str Scheduler queue class. SCHEDULER_DUPEFILTER_KEY : str Scheduler dupefilter redis key. SCHEDULER_DUPEFILTER_CLASS : str Scheduler dupefilter class. SCHEDULER_SERIALIZER : str Scheduler serializer. \"\"\" def __init__ ( self , server , persist = False , flush_on_start = False , queue_key = defaults . SCHEDULER_QUEUE_KEY , queue_cls = defaults . SCHEDULER_QUEUE_CLASS , dupefilter_key = defaults . SCHEDULER_DUPEFILTER_KEY , dupefilter_cls = defaults . SCHEDULER_DUPEFILTER_CLASS , idle_before_close = 0 , serializer = None ) : \"\"\"Initialize scheduler. Parameters ---------- server : Redis The redis server instance. persist : bool Whether to flush requests when closing. Default is False. flush_on_start : bool Whether to flush requests on start. Default is False. queue_key : str Requests queue key. queue_cls : str Importable path to the queue class. dupefilter_key : str Duplicates filter key. dupefilter_cls : str Importable path to the dupefilter class. idle_before_close : int Timeout before giving up. \"\"\" \"\"\" Parameters ---------- server : Redis \u8fd9\u662fRedis\u5b9e\u4f8b persist : bool \u662f\u5426\u5728\u5173\u95ed\u65f6\u6e05\u7a7aRequests.\u9ed8\u8ba4\u503c\u662fFalse\u3002 flush_on_start : bool \u662f\u5426\u5728\u542f\u52a8\u65f6\u6e05\u7a7aRequests\u3002 \u9ed8\u8ba4\u503c\u662fFalse\u3002 queue_key : str Request\u961f\u5217\u7684Key\u540d\u5b57 queue_cls : str \u961f\u5217\u7684\u53ef\u5bfc\u5165\u8def\u5f84\uff08\u5c31\u662f\u4f7f\u7528\u4ec0\u4e48\u961f\u5217\uff09 dupefilter_key : str \u53bb\u91cd\u961f\u5217\u7684Key dupefilter_cls : str \u53bb\u91cd\u7c7b\u7684\u53ef\u5bfc\u5165\u8def\u5f84\u3002 idle_before_close : int \u7b49\u5f85\u591a\u4e45\u5173\u95ed \"\"\" if idle_before_close < 0 : raise TypeError ( \"idle_before_close cannot be negative\" ) self . server = server self . persist = persist self . flush_on_start = flush_on_start self . queue_key = queue_key self . queue_cls = queue_cls self . dupefilter_cls = dupefilter_cls self . dupefilter_key = dupefilter_key self . idle_before_close = idle_before_close self . serializer = serializer self . stats = None def __len__ ( self ) : return len ( self . queue ) @classmethod def from_settings ( cls , settings ) : # \u4ecesetting\u4e2d\u5bfc\u5165\u53c2\u6570 kwargs = { 'persist' : settings . getbool ( 'SCHEDULER_PERSIST' ), 'flush_on_start' : settings . getbool ( 'SCHEDULER_FLUSH_ON_START' ), 'idle_before_close' : settings . getint ( 'SCHEDULER_IDLE_BEFORE_CLOSE' ), } # If these values are missing , it means we want to use the defaults . optional = { # TODO : Use custom prefixes for this settings to note that are # specific to scrapy - redis . 'queue_key' : 'SCHEDULER_QUEUE_KEY' , 'queue_cls' : 'SCHEDULER_QUEUE_CLASS' , 'dupefilter_key' : 'SCHEDULER_DUPEFILTER_KEY' , # We use the default setting name to keep compatibility . 'dupefilter_cls' : 'DUPEFILTER_CLASS' , 'serializer' : 'SCHEDULER_SERIALIZER' , } # \u4ecesetting \u4e2d\u83b7\u53d6\u914d\u7f6e\u7ec4\u88c5\u6210dict ( \u5177\u4f53\u83b7\u53d6\u54ea\u4e9b\u914d\u7f6e\u662foptional\u5b57\u5178\u4e2dkey ) for name , setting_name in optional . items () : val = settings . get ( setting_name ) if val : kwargs [ name ] = val # Support serializer as a path to a module . if isinstance ( kwargs . get ( 'serializer' ), six . string_types ) : kwargs [ 'serializer' ] = importlib . import_module ( kwargs [ 'serializer' ] ) # \u83b7\u53d6\u4e00\u4e2aRedis \u8fde\u63a5 server = connection . from_settings ( settings ) # Ensure the connection is working . # \u786e\u4fdd\u8fde\u63a5\u6b63\u5e38 server . ping () return cls ( server = server , ** kwargs ) @classmethod def from_crawler ( cls , crawler ) : instance = cls . from_settings ( crawler . settings ) # FIXME : for now , stats are only supported from this constructor instance . stats = crawler . stats return instance def open ( self , spider ) : self . spider = spider try : # \u6839\u636eself . queue_cls\u8fd9\u4e2a\u53ef\u4ee5\u5bfc\u5165\u7684\u7c7b \uff0c \u5b9e\u4f8b\u5316\u4e00\u4e2a\u961f\u5217 self . queue = load_object ( self . queue_cls )( server = self . server , spider = spider , key = self . queue_key % { 'spider' : spider . name } , serializer = self . serializer , ) except TypeError as e : raise ValueError ( \"Failed to instantiate queue class '%s': %s\" , self . queue_cls , e ) # \u6839\u636eself . dupefilter_cls \u8fd9\u4e2a\u53ef\u4ee5\u5bfc\u5165\u7684\u7c7b \uff0c \u5b9e\u4f8b\u4e00\u4e2a\u53bb\u91cd\u96c6\u5408 # \u9ed8\u8ba4\u662f\u96c6\u5408 \uff0c \u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u53bb\u91cd\u65b9\u5f0f \uff0c \u6bd4\u5982bool\u53bb\u91cd self . df = load_object ( self . dupefilter_cls ). from_spider ( spider ) if self . flush_on_start : self . flush () # notice if there are requests already in the queue to resume the crawl if len ( self . queue ) : spider . log ( \"Resuming crawl (%d requests scheduled)\" % len ( self . queue )) def close ( self , reason ) : if not self . persist : self . flush () def flush ( self ) : self . df . clear () self . queue . clear () def enqueue_request ( self , request ) : \"\"\" \u8fd9\u4e2a\u548cscrapy \u672c\u8eab\u7684\u4e00\u6837 :param request: :return: \"\"\" if not request . dont_filter and self . df . request_seen ( request ) : self . df . log ( request , self . spider ) return False if self . stats : self . stats . inc_value ( 'scheduler/enqueued/redis' , spider = self . spider ) # \u5411\u961f\u5217\u91cc\u6dfb\u52a0\u4e00\u4e2aRequest self . queue . push ( request ) return True def next_request ( self ) : \"\"\" \u83b7\u53d6\u4e00\u4e2aRequest :return: \"\"\" block_pop_timeout = self . idle_before_close # block_pop_timeout \u662f\u4e00\u4e2a\u7b49\u5f85\u53c2\u6570 \uff0c \u961f\u5217\u6ca1\u6709\u4e1c\u897f\u4f1a\u7b49\u5f85\u8fd9\u4e2a\u65f6\u95f4 \uff0c \u8d85\u65f6\u5c31\u4f1a\u5173\u95ed request = self . queue . pop ( block_pop_timeout ) if request and self . stats : self . stats . inc_value ( 'scheduler/dequeued/redis' , spider = self . spider ) return request # \u5224\u65ad\u662f\u5426\u8fd8\u6709\u672a\u8c03\u5ea6\u7684\u8bf7\u6c42 def has_pending_requests ( self ) : return len ( self ) > 0 \u9664\u4e86\u8c03\u5ea6\u5668\u6539\u5199\u4e86,\u6211\u4eec\u8fd8\u770b\u5230\u6e90\u7801\u603b\u8fd8\u6709queue \u8fd9\u662f\u5bf9\u961f\u5217\u8fdb\u884c\u4e86\u6539\u88c5\uff0c\u6211\u4eec\u770b\u4e00\u4e0b\u961f\u5217\u5230\u5e95\u6539\u9020\u4e86\u4ec0\u4e48 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # scrapy_redis.queue.py class PriorityQueue ( Base ): \"\"\"Per-spider priority queue abstraction using redis' sorted set\"\"\" \"\"\"\u5176\u5b9e\u5c31\u662f\u4f7f\u7528Redis\u7684\u6709\u5e8f\u96c6\u5408\uff0c\u6765\u5bf9Request\u8fdb\u884c\u6392\u5e8f\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u4f18\u5148\u7ea7\u9ad8\u7684\u5728\u6709\u5e8f\u96c6\u5408\u7684\u9876\u5c42 \u6211\u4eec\u53ea\u9700\u8981 \u4ece\u4e0a\u5f80\u4e0b\u83b7\u53d6Request\u5373\u53ef\"\"\" def __len__ ( self ): \"\"\"Return the length of the queue\"\"\" return self . server . zcard ( self . key ) def push ( self , request ): \"\"\"Push a request\"\"\" \"\"\"\u6dfb\u52a0\u4e00\u4e2aRequest\u8fdb\u961f\u5217\"\"\" # self._encode_request(request)\u5c06Request\u8bf7\u6c42\u5e8f\u5217\u5316 data = self . _encode_request ( request ) \"\"\" d = { 'url': to_unicode(request.url), # urls should be safe (safe_string_url) 'callback': cb, 'errback': eb, 'method': request.method, 'headers': dict(request.headers), 'body': request.body, 'cookies': request.cookies, 'meta': request.meta, '_encoding': request._encoding, 'priority': request.priority, 'dont_filter': request.dont_filter, 'flags': request.flags, '_class': request.__module__ + '.' + request.__class__.__name__ } \"\"\" # data \u5c31\u662f\u4e0a\u9762\u8fd9\u4e2a\u5b57\u5178\u7684\u5e8f\u5217\u5316\uff0c\u5728Scrapy.util.reqser.py\u4e2d\u7684request_to_dict\u65b9\u6cd5\u4e2d\u5904\u7406 # \u5728redis \u6709\u5e8f\u96c6\u5408\u4e2d\u6570\u503c\u8d8a\u5c0f\u4f18\u5148\u7ea7\u8d8a\u9ad8\uff08\u5c31\u662f\u4f1a\u88ab\u653e\u5728\u9876\u5c42\uff09 \u6240\u4ee5\u8fd9\u4e2a\u4f4d\u7f6e\u662f\u53d6\u5f97\u76f8\u53cd\u6570 score = - request . priority # We don't use zadd method as the order of arguments change depending on # whether the class is Redis or StrictRedis, and the option of using # kwargs only accepts strings, not bytes. # ZADD \u662f\u6dfb\u52a0\u8fdb\u6765\u6709\u5e8f\u96c6\u5408 self . server . execute_command ( 'ZADD' , self . key , score , data ) def pop ( self , timeout = 0 ): \"\"\" Pop a request timeout not support in this queue class \u6709\u5e8f\u96c6\u5408\u4e0d\u652f\u6301\u8d85\u65f6\u5c31\u6728\u6709\u4f7f\u7528timeout\u4e86 \u8fd9\u4e2atimeout\u5c31\u662f\u6302\u7f8a\u5934\u4e70\u72d7\u8089 \"\"\" # use atomic range/remove using multi/exec pipe = self . server . pipeline () pipe . multi () # \u53d6\u51fa \u9876\u5c42\u7b2c\u4e00\u4e2a # zrange : \u8fd4\u56de\u6709\u5e8f\u96c6 key \u4e2d\uff0c\u6307\u5b9a\u533a\u95f4\u5185\u7684\u6210\u5458 0\uff0c0 \u5c31\u662f\u7b2c\u4e00\u4e2a # zremrangebyrank:\u79fb\u9664\u6709\u5e8f\u96c6key \u4e2d\uff0c\u6307\u5b9a\u6392\u540d\uff08rank\uff09 \u533a\u95f4\u5185\u7684\u6240\u6709\u6210\u5458 0\uff0c0\u4e5f\u5c31\u662f\u7b2c\u4e00\u4e2a\u4e86 pipe . zrange ( self . key , 0 , 0 ). zremrangebyrank ( self . key , 0 , 0 ) results , count = pipe . execute () if results: return self . _decode_request ( results [ 0 ]) \u662f\u4e00\u4e2a\u666e\u901a\u7684\u961f\u5217\uff0c\u653eredis\u91cc\u9762\u5148\u8fdb\u5148\u51fa \u9664\u4e86\u961f\u5217\u8fdb\u884c\u4e86\u6539\u88c5\uff0c\u8fd8\u6709\u5bf9\u53bb\u91cd\u4e5f\u8fdb\u884c\u4e86\u6539\u88c5\uff0c\u6211\u4eec\u6765\u770b\u770b\u53bb\u91cd\u662f\u5982\u4f55\u6539\u88c5\u7684 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 # scrapy_redis . dupefilter . py class RFPDupeFilter ( BaseDupeFilter ) : \"\"\"Redis-based request duplicates filter. This class can also be used with default Scrapy's scheduler. \"\"\" logger = logger def __init__ ( self , server , key , debug = False ) : \"\"\"Initialize the duplicates filter. Parameters ---------- server : redis.StrictRedis The redis server instance. key : str Redis key Where to store fingerprints. debug : bool, optional Whether to log filtered requests. \"\"\" self . server = server self . key = key self . debug = debug self . logdupes = True @classmethod def from_settings ( cls , settings ) : \"\"\"Returns an instance from given settings. This uses by default the key ``dupefilter:<timestamp>``. When using the ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as it needs to pass the spider name in the key. Parameters ---------- settings : scrapy.settings.Settings Returns ------- RFPDupeFilter A RFPDupeFilter instance. \"\"\" server = get_redis_from_settings ( settings ) # XXX : This creates one - time key . needed to support to use this # class as standalone dupefilter with scrapy 's default scheduler # if scrapy passes spider on open() method this wouldn' t be needed # TODO : Use SCRAPY_JOB env as default and fallback to timestamp . key = defaults . DUPEFILTER_KEY % { 'timestamp' : int ( time . time ()) } debug = settings . getbool ( 'DUPEFILTER_DEBUG' ) return cls ( server , key = key , debug = debug ) @classmethod def from_crawler ( cls , crawler ) : \"\"\"Returns instance from crawler. Parameters ---------- crawler : scrapy.crawler.Crawler Returns ------- RFPDupeFilter Instance of RFPDupeFilter. \"\"\" return cls . from_settings ( crawler . settings ) def request_seen ( self , request ) : \"\"\"Returns True if request was already seen. Parameters ---------- request : scrapy.http.Request Returns ------- bool \"\"\" # \u901a\u8fc7self . request_fingerprint \u4f1a\u751f\u4e00\u4e2asha1\u7684\u6307\u7eb9 fp = self . request_fingerprint ( request ) # This returns the number of values added , zero if already exists . # \u6dfb\u52a0\u8fdb\u4e00\u4e2aRedis\u96c6\u5408\u5982\u679cself . key\u8fd9\u4e2a\u96c6\u5408\u4e2d\u5b58\u5728fp\u8fd9\u4e2a\u96c6\u5408\u4f1a\u8fd4\u56de1 \u4e0d\u5b58\u5728\u8fd4\u56de0 added = self . server . sadd ( self . key , fp ) return added == 0 def request_fingerprint ( self , request ) : \"\"\"Returns a fingerprint for a given request. Parameters ---------- request : scrapy.http.Request Returns ------- str \"\"\" return request_fingerprint ( request ) @classmethod def from_spider ( cls , spider ) : settings = spider . settings server = get_redis_from_settings ( settings ) dupefilter_key = settings . get ( \"SCHEDULER_DUPEFILTER_KEY\" , defaults . SCHEDULER_DUPEFILTER_KEY ) key = dupefilter_key % { 'spider' : spider . name } debug = settings . getbool ( 'DUPEFILTER_DEBUG' ) return cls ( server , key = key , debug = debug ) def close ( self , reason = '' ) : \"\"\"Delete data on close. Called by Scrapy's scheduler. Parameters ---------- reason : str, optional \"\"\" self . clear () def clear ( self ) : \"\"\"Clears fingerprints data.\"\"\" self . server . delete ( self . key ) def log ( self , request , spider ) : \"\"\"Logs given request. Parameters ---------- request : scrapy.http.Request spider : scrapy.spiders.Spider \"\"\" if self . debug : msg = \"Filtered duplicate request: %(request)s\" self . logger . debug ( msg , { 'request' : request } , extra = { 'spider' : spider } ) elif self . logdupes : msg = ( \"Filtered duplicate request %(request)s\" \" - no more duplicates will be shown\" \" (see DUPEFILTER_DEBUG to show all duplicates)\" ) self . logger . debug ( msg , { 'request' : request } , extra = { 'spider' : spider } ) self . logdupes = False \u6211\u4eec\u4e3b\u8981\u770b\u4ed6\u8fd9\u4e2a\u65b9\u6cd5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def request_seen ( self , request ): \"\"\"Returns True if request was already seen. Parameters ---------- request : scrapy.http.Request Returns ------- bool \"\"\" # \u901a\u8fc7 self . request_fingerprint \u4f1a\u751f\u4e00\u4e2a sha1\u7684\u6307\u7eb9 fp = self . request_fingerprint ( request ) # This returns the number of values added , zero if already exists . # \u6dfb\u52a0\u8fdb\u4e00\u4e2a Redis\u96c6\u5408\u5982\u679cself . key\u8fd9\u4e2a\u96c6\u5408\u4e2d\u5b58\u5728fp\u8fd9\u4e2a\u96c6\u5408\u4f1a\u8fd4\u56de1 \u4e0d\u5b58\u5728\u8fd4\u56de 0 added = self . server . sadd ( self . key , fp ) return added == 0 \u5c31\u662f\u8bf4request_seen()\u8fd9\u4e2a\u65b9\u6cd5\u4f1a\u5c06\u8bf7\u6c42\u52a0\u5bc6\u7684\u90a3\u4e2akey\u505a\u5224\u65ad\u3002\u5b58\u5728\u5c31\u8fd4\u56de1,\u4e0d\u5b58\u5728\u5c31\u8fd4\u56de0\u3002\u8fd9\u6837\u5c31\u975e\u5e38\u7b80\u7b54\u5224\u65ad\u8bf7\u6c42\uff0c\u7136\u540e\u8fc7\u6ee4 \u603b\u7ed3 \u00b6 scrapy-redis\u4f5c\u4e3a\u5bf9Scrapy \u7684\u8c03\u53d6\u5668\u8fdb\u884c\u6539\u9020\uff0c\u5c06\u539f\u672cscrapy \u7684\u8bf7\u6c42\u57fa\u4e8e\u961f\u5217\uff08Queue\uff09\u5185\u5b58\u5904\u7406\uff0c\u8fd8\u6709\u53bb\u91cd\u8fd9\u4e24\u4e2a\u90e8\u5206\u4f7f\u7528redis\u53bb\u5b9e\u73b0 \u601d\u8003 \u00b6 \u53bb\u91cd\u662f\u5982\u4f55\u505a\u5230\u7684\uff0c\u4f1a\u4e0d\u4f1a\u6709\u91cd\u590d \u53bb\u91cd\u662f\u5c06request hash\u505a\u6210key.request\u4ed6\u662f\u4f1a\u5148\u5e8f\u5217\u5316\u7684\uff0c\u5e8f\u5217\u5316\u7684\u65f6\u5019\u5c31\u5305\u62ec\u4e86\u5f88\u591a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def request_to_dict ( request , spider = None ): \"\"\"Convert Request object to a dict. If a spider is given, it will try to find out the name of the spider method used in the callback and store that as the callback. \"\"\" cb = request . callback if callable ( cb ): cb = _find_method ( spider , cb ) eb = request . errback if callable ( eb ): eb = _find_method ( spider , eb ) d = { 'url' : to_unicode ( request . url ), # urls should be safe (safe_string_url) 'callback' : cb , 'errback' : eb , 'method' : request . method , 'headers' : dict ( request . headers ), 'body' : request . body , 'cookies' : request . cookies , 'meta' : request . meta , '_encoding' : request . _encoding , 'priority' : request . priority , 'dont_filter' : request . dont_filter , 'flags' : request . flags , 'cb_kwargs' : request . cb_kwargs , } if type ( request ) is not Request : d [ '_class' ] = request . __module__ + '.' + request . __class__ . __name__ return d \u5e76\u4e0d\u662f\u5355\u5355\u5bf9url\u505a\u5904\u7406\u54e6\uff01\u6240\u4ee5\u522b\u62c5\u5fc3\u3002\u57fa\u672c\u53c2\u6570\u90fd\u505a\u5904\u7406\uff0c\u90a3\u5c82\u4e0d\u662f\u7f8e\u6ecb\u6ecb\uff0c\u518d\u4e5f\u4e0d\u7528\u62c5\u5fc3\u6211\u722c\u7684\u7adf\u7136\u6ca1\u53bb\u91cd.\u5f53\u7136\u6211\u4eec\u4e5f\u662f\u53ef\u4ee5\u6539\u88c5\u7684\uff0c\u4ee5\u53bb\u9002\u5408\u9700\u6c42\u7684\u53d8\u5316 hash \u4e00\u6b21key\u6570\u503c\u8fdbredis\u503c\u5927\u4e0d\u5927\uff0c\u80fd\u4e0d\u80fd\u4f18\u826f\u6539\u88c5\u4e00\u4e0b \u4f7f\u7528 \u5e03\u9686\u8fc7\u6ee4\u5668 \u5177\u4f53\u539f\u7406\u4e0b\u6b21\u7814\u7a76 3.\u5982\u4f55\u4f7f\u7528scrapy-redis \u8fd9\u91cc\u662f \u5b98\u65b9\u6587\u6863 \uff0c\u6309\u7167\u4ed6\u7684\u8981\u6c42\u5728\u8bbe\u7f6e\u5c31\u884c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #\u542f\u7528Redis\u8c03\u5ea6\u5b58\u50a8\u8bf7\u6c42\u961f\u5217 SCHEDULER = \"scrapy_redis.scheduler.Scheduler\" #\u786e\u4fdd\u6240\u6709\u7684\u722c\u866b\u901a\u8fc7Redis\u53bb\u91cd DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\" #\u9ed8\u8ba4\u8bf7\u6c42\u5e8f\u5217\u5316\u4f7f\u7528\u7684\u662fpickle \u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u66f4\u6539\u4e3a\u5176\u4ed6\u7c7b\u4f3c\u7684\u3002PS\uff1a\u8fd9\u73a9\u610f\u513f2.X\u7684\u53ef\u4ee5\u7528\u30023.X\u7684\u4e0d\u80fd\u7528 #SCHEDULER_SERIALIZER = \"scrapy_redis.picklecompat\" #\u4e0d\u6e05\u9664Redis\u961f\u5217\u3001\u8fd9\u6837\u53ef\u4ee5\u6682\u505c/\u6062\u590d \u722c\u53d6 #SCHEDULER_PERSIST = True #\u4f7f\u7528\u4f18\u5148\u7ea7\u8c03\u5ea6\u8bf7\u6c42\u961f\u5217 \uff08\u9ed8\u8ba4\u4f7f\u7528\uff09 #SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue' #\u53ef\u9009\u7528\u7684\u5176\u5b83\u961f\u5217 #SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue' #SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue' #\u6700\u5927\u7a7a\u95f2\u65f6\u95f4\u9632\u6b62\u5206\u5e03\u5f0f\u722c\u866b\u56e0\u4e3a\u7b49\u5f85\u800c\u5173\u95ed #SCHEDULER_IDLE_BEFORE_CLOSE = 10 #\u5c06\u6e05\u9664\u7684\u9879\u76ee\u5728redis\u8fdb\u884c\u5904\u7406 ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 300 } #\u5e8f\u5217\u5316\u9879\u76ee\u7ba1\u9053\u4f5c\u4e3aredis Key\u5b58\u50a8 #REDIS_ITEMS_KEY = '%(spider)s:items' #\u9ed8\u8ba4\u4f7f\u7528ScrapyJSONEncoder\u8fdb\u884c\u9879\u76ee\u5e8f\u5217\u5316 #You can use any importable path to a callable object. #REDIS_ITEMS_SERIALIZER = 'json.dumps' #\u6307\u5b9a\u8fde\u63a5\u5230redis\u65f6\u4f7f\u7528\u7684\u7aef\u53e3\u548c\u5730\u5740\uff08\u53ef\u9009\uff09 #REDIS_HOST = 'localhost' #REDIS_PORT = 6379 #\u6307\u5b9a\u7528\u4e8e\u8fde\u63a5redis\u7684URL\uff08\u53ef\u9009\uff09 #\u5982\u679c\u8bbe\u7f6e\u6b64\u9879\uff0c\u5219\u6b64\u9879\u4f18\u5148\u7ea7\u9ad8\u4e8e\u8bbe\u7f6e\u7684REDIS_HOST \u548c REDIS_PORT #REDIS_URL = 'redis://user:pass@hostname:9001' #\u81ea\u5b9a\u4e49\u7684redis\u53c2\u6570\uff08\u8fde\u63a5\u8d85\u65f6\u4e4b\u7c7b\u7684\uff09 #REDIS_PARAMS = {} #\u81ea\u5b9a\u4e49redis\u5ba2\u6237\u7aef\u7c7b #REDIS_PARAMS['redis_cls'] = 'myproject.RedisClient' #\u5982\u679c\u4e3aTrue\uff0c\u5219\u4f7f\u7528redis\u7684'spop'\u8fdb\u884c\u64cd\u4f5c\u3002 #\u5982\u679c\u9700\u8981\u907f\u514d\u8d77\u59cb\u7f51\u5740\u5217\u8868\u51fa\u73b0\u91cd\u590d\uff0c\u8fd9\u4e2a\u9009\u9879\u975e\u5e38\u6709\u7528\u3002\u5f00\u542f\u6b64\u9009\u9879urls\u5fc5\u987b\u901a\u8fc7sadd\u6dfb\u52a0\uff0c\u5426\u5219\u4f1a\u51fa\u73b0\u7c7b\u578b\u9519\u8bef\u3002 #REDIS_START_URLS_AS_SET = False #RedisSpider\u548cRedisCrawlSpider\u9ed8\u8ba4 start_usls \u952e #REDIS_START_URLS_KEY = '%(name)s:start_urls' #\u8bbe\u7f6eredis\u4f7f\u7528utf-8\u4e4b\u5916\u7684\u7f16\u7801 #REDIS_ENCODING = 'latin1' \u5b9e\u8df5\u6848\u4f8b\u6709\u5417\uff1f \u53ef\u4ee5\u770b\u4e0b\u722c\u53d6scrapy-redis\u5168\u7f51\u7684\u5f53\u5f53\u7f51 \u53c2\u8003\u8d44\u6599 \u00b6 https://zhuanlan.zhihu.com/p/79120407 https://www.jianshu.com/p/2104d11ee0a2","title":"Scrapy \u6e90\u7801 scrapy-redis"},{"location":"basics/scrapy/07_scrapy_redis/07_scrapy_redis/#scrapy-redis","text":"\u4f5c\u4e3a\u5bf9scrapy \u6846\u67b6\u7684\u8865\u5145\u3002\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u57fa\u7840\uff0c\u6211\u4eec\u5b9e\u9645\u6765\u770b\u4e00\u770b\u4ed6\u5230\u5e95\u5bf9scrapy\u505a\u4e86\u54ea\u4e9b\u8865\u5145\uff0c\u4e3a\u4ec0\u4e48\u5982\u6b64\u53d7\u6b22\u8fce \u4ece\u67b6\u6784\u56fe\u4e2d\u6211\u4eec\u53ef\u4ee5\u77e5\u9053\u4ed6\u662f\u5bf9scrapy\u672c\u8eab\u7684scheduler\u8fdb\u884c\u4e86\u6539\u9020\u3002\u90a3\u5230\u5e95\u4ec0\u4e48\u662fscheduleer(\u8c03\u5ea6\u5668)\u5462\uff1f\u5148\u67e5\u4e0b\u4ed6\u7684\u6e90\u7801 scrapy.core.scheduler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 # scrapy.core.scheduler.py: class Scheduler ( object ): \"\"\" Scrapy Scheduler. It allows to enqueue requests and then get a next request to download. Scheduler is also handling duplication filtering, via dupefilter. Prioritization and queueing is not performed by the Scheduler. User sets ``priority`` field for each Request, and a PriorityQueue (defined by :setting:`SCHEDULER_PRIORITY_QUEUE`) uses these priorities to dequeue requests in a desired order. Scheduler uses two PriorityQueue instances, configured to work in-memory and on-disk (optional). When on-disk queue is present, it is used by default, and an in-memory queue is used as a fallback for cases where a disk queue can't handle a request (can't serialize it). :setting:`SCHEDULER_MEMORY_QUEUE` and :setting:`SCHEDULER_DISK_QUEUE` allow to specify lower-level queue classes which PriorityQueue instances would be instantiated with, to keep requests on disk and in memory respectively. Overall, Scheduler is an object which holds several PriorityQueue instances (in-memory and on-disk) and implements fallback logic for them. Also, it handles dupefilters. \"\"\" def __init__ ( self , dupefilter , jobdir = None , dqclass = None , mqclass = None , logunser = False , stats = None , pqclass = None , crawler = None ): self . df = dupefilter self . dqdir = self . _dqdir ( jobdir ) self . pqclass = pqclass self . dqclass = dqclass self . mqclass = mqclass self . logunser = logunser self . stats = stats self . crawler = crawler # \u53ef\u4ee5\u770b\u5230\u8fd9\u662f\u4e00\u4e2a\u94a9\u5b50\uff0c\u4ed6\u80fd\u8bbf\u95ee\u5f53\u524d\u722c\u866b\u7684\u914d\u7f6e\uff0c\u7136\u540e\u8f7d\u5165 @ classmethod def from_crawler ( cls , crawler ): settings = crawler . settings # \u83b7\u53d6\u53bb\u91cd\u7528\u7684\u7c7b\uff0c\u9ed8\u8ba4\u662fscrapy.dupefilters.RFPDupeFilter dupefilter_cls = load_object ( settings [ 'DUPEFILTER_CLASS' ]) # \u6211\u4eec\u53ef\u4ee5\u8ddf\u8e2a\u4e0bload_obj()\u8fd9\u4e2a\u65b9\u6cd5 # def load_object(path): # \"\"\"Load an object given its absolute object path, and return it. # # object can be a class, function, variable or an instance. # path ie: 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware' # # # \u8fd4\u56de\u8def\u5f84\u6307\u5b9a\u7684\u7c7b\u5bf9\u8c61\uff0c\u6bd4\u5982path:scrapy.downloadermiddlewares.redirect.RedirectMiddleware\uff0c\u4ed6\u8fd4\u56de\u7684\u662f # RedirectMiddleware\u8fd9\u4e2a\u5bf9\u8c61 # \"\"\" # # try: # # \u8fd4\u56de\u5b50\u5b57\u7b26\u4e32 str \u5728\u5b57\u7b26\u4e32\u4e2d\u6700\u540e\u51fa\u73b0\u7684\u4f4d\u7f6e\uff0c\u51fa\u73b0\u5728\u5b57\u7b26\u4e32path\u4e2d\u51fa\u73b0\u7684\u4f4d\u7f6e\uff0c\u5982\u679c\u6ca1\u6709\u5339\u914d\u7684\u5b57\u7b26\u4e32\u4f1a\u62a5\u5f02\u5e38\u3002 # dot = path.rindex('.') # except ValueError: # raise ValueError(\"Error loading object '%s': not a full path\" % path) # # # \u8fd9\u91cc\u5c06\u6a21\u5757\u548c\u540d\u5b57\u7ed9\u51fa\u5566\uff0c\u6bd4\u5982\u6b63\u5e38\u5c31\u8fd4\u56demodel=scrapy.spiderloader|||name=SpiderLoader # module, name = path[:dot], path[dot + 1:] # mod = import_module(module) # # try: # obj = getattr(mod, name) # except AttributeError: # raise NameError(\"Module '%s' doesn't define any object named '%s'\" % (module, name)) # # return obj # \u521b\u5efa\u4e00\u4e2a\u5b9e\u4f8b\u5bf9\u8c61\uff0c\u5bf9\u5bf9\u8c61\u7684\u4e00\u4e9b\u5c5e\u6027\u505a\u4e00\u4e9b\u5224\u65ad dupefilter = create_instance ( dupefilter_cls , settings , crawler ) # \u83b7\u53d6\u4f18\u5148\u7ea7\u961f\u5217\uff0c \u9ed8\u8ba4\uff1aqueuelib.pqueue.PriorityQueue\uff0c\u7c7b\u5bf9\u8c61\uff08SCHEDULER_PRIORITY_QUEUE\uff09 pqclass = load_object ( settings [ 'SCHEDULER_PRIORITY_QUEUE' ]) # \u5224\u65ad\u4e00\u4e9b\u83b7\u5f97\u7684\u8fd9\u4e2apqclass\u961f\u5217\u5bf9\u8c61\u662f\u5c31\u662fPriorityQueue\u8fd9\u4e2a\u5bf9\u8c61\uff0c\u662f\u7684\u8bdd\u53d1\u51fa\u8b66\u544a\uff0c\u5e76\u5c06ScrapyPriorityQueue\u8fd9\u4e2a\u5bf9\u8c61 # \u8d4b\u503c\u7ed9ScrapyPriorityQueue if pqclass is PriorityQueue : warnings . warn ( \"SCHEDULER_PRIORITY_QUEUE='queuelib.PriorityQueue'\" \" is no longer supported because of API changes; \" \"please use 'scrapy.pqueues.ScrapyPriorityQueue'\" , ScrapyDeprecationWarning ) from scrapy . pqueues import ScrapyPriorityQueue pqclass = ScrapyPriorityQueue # \u83b7\u53d6\u78c1\u76d8\u961f\u5217 \u7c7b\u5bf9\u8c61\uff08SCHEDULER_DISK_QUEUE,\u91cd\u542f\u4e0d\u4f1a\u4e22\u5931\uff09 dqclass = load_object ( settings [ 'SCHEDULER_DISK_QUEUE' ]) # \u83b7\u53d6\u5185\u5b58\u961f\u5217 \u7c7b\u5bf9\u8c61(SCHEDULER \u4f7f\u7528\u5185\u5b58\u5b58\u50a8\uff0c\u91cd\u542f\u4f1a\u4e22\u5931) mqclass = load_object ( settings [ 'SCHEDULER_MEMORY_QUEUE' ]) # \u662f\u5426\u5f00\u542fdebug logunser = settings . getbool ( 'LOG_UNSERIALIZABLE_REQUESTS' , settings . getbool ( 'SCHEDULER_DEBUG' )) # \u5c06\u8fd9\u4e9b\u53c2\u6570\u4f20\u9012\u7ed9 __init__ \u65b9\u6cd5 return cls ( dupefilter , jobdir = job_dir ( settings ), logunser = logunser , stats = crawler . stats , pqclass = pqclass , dqclass = dqclass , mqclass = mqclass , crawler = crawler ) # \u68c0\u67e5\u662f\u5426\u6709\u6ca1\u6709\u5904\u7406\u7684\u8bf7\u6c42 def has_pending_requests ( self ): return len ( self ) > 0 # Engin\u521b\u5efa\u5b8c\u4e4b\u540e\u4f1a\u8c03\u7528\u8fd9\u4e2a\u65b9\u6cd5 def open ( self , spider ): self . spider = spider # \u521b\u5efa\u4e00\u4e2a\u6709\u4f18\u5148\u7ea7\u7684\u5185\u5b58\u961f\u5217 \u5b9e\u4f8b\u5316\u5bf9\u8c61 # self.pqclass \u9ed8\u8ba4\u662f\uff1aqueuelib.pqueue.PriorityQueue # self._newmq \u4f1a\u8fd4\u56de\u4e00\u4e2a\u5185\u5b58\u961f\u5217\u7684 \u5b9e\u4f8b\u5316\u5bf9\u8c61 \u5728110 111 \u884c self . mqs = self . _mq () # \u5982\u679cself.dqdir \u6709\u8bbe\u7f6e \u5c31\u521b\u5efa\u4e00\u4e2a\u78c1\u76d8\u961f\u5217 \u5426\u5219self.dqs \u4e3a\u7a7a self . dqs = self . _dq () if self . dqdir else None # \u83b7\u5f97\u4e00\u4e2a\u53bb\u91cd\u5b9e\u4f8b\u5bf9\u8c61 open \u65b9\u6cd5\u662f\u4eceBaseDupeFilter\u7ee7\u627f\u7684 # \u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u7528self.df\u6765\u53bb\u91cd\u5566 return self . df . open () # \u5f53Engine\u5173\u95ed\u7684\u65f6\u5019 def close ( self , reason ): # \u5982\u679c\u6709\u78c1\u76d8\u961f\u5217 \u5219\u5bf9\u5176\u8fdb\u884cdump\u540e\u4fdd\u5b58\u5230active.json\u6587\u4ef6\u4e2d if self . dqs : state = self . dqs . close () self . _write_dqs_state ( self . dqdir , state ) # \u7136\u540e\u5173\u95ed\u53bb\u91cd return self . df . close ( reason ) # \u6dfb\u52a0\u4e00\u4e2aRequest\u8fdb\u8c03\u5ea6\u961f\u5217 def enqueue_request ( self , request ): if not request . dont_filter and self . df . request_seen ( request ): # \u5982\u679cRequest\u7684dont_filter\u5c5e\u6027\u6ca1\u6709\u8bbe\u7f6e\uff08\u9ed8\u8ba4\u4e3aFalse\uff09\u548c \u5df2\u7ecf\u5b58\u5728\u5219\u53bb\u91cd # \u4e0dpush\u8fdb\u961f\u5217 self . df . log ( request , self . spider ) return False # \u5148\u5c1d\u8bd5\u5c06Request push\u8fdb\u78c1\u76d8\u961f\u5217 dqok = self . _dqpush ( request ) if dqok : # \u5982\u679c\u6210\u529f \u5219\u5728\u8bb0\u5f55\u4e00\u6b21\u72b6\u6001 self . stats . inc_value ( 'scheduler/enqueued/disk' , spider = self . spider ) else : # \u4e0d\u80fd\u6dfb\u52a0\u8fdb\u78c1\u76d8\u961f\u5217\u5219\u4f1a\u6dfb\u52a0\u8fdb\u5185\u5b58\u961f\u5217 self . _mqpush ( request ) self . stats . inc_value ( 'scheduler/enqueued/memory' , spider = self . spider ) self . stats . inc_value ( 'scheduler/enqueued' , spider = self . spider ) return True # \u4ece\u961f\u5217\u4e2d\u83b7\u53d6\u4e00\u4e2aRequest def next_request ( self ): request = self . mqs . pop () if request : self . stats . inc_value ( 'scheduler/dequeued/memory' , spider = self . spider ) else : # \u4e0d\u80fd\u83b7\u53d6\u7684\u65f6\u5019\u4ece\u78c1\u76d8\u961f\u5217\u961f\u91cc\u83b7\u53d6 request = self . _dqpop () if request : self . stats . inc_value ( 'scheduler/dequeued/disk' , spider = self . spider ) if request : self . stats . inc_value ( 'scheduler/dequeued' , spider = self . spider ) # \u5c06\u83b7\u53d6\u7684\u5230Request\u8fd4\u56de\u7ed9Engine return request def __len__ ( self ): return len ( self . dqs ) + len ( self . mqs ) if self . dqs else len ( self . mqs ) def _dqpush ( self , request ): if self . dqs is None : return try : self . dqs . push ( request , - request . priority ) except ValueError as e : # non serializable request if self . logunser : msg = ( \"Unable to serialize request: %(request)s - reason:\" \" %(reason)s - no more unserializable requests will be\" \" logged (stats being collected)\" ) logger . warning ( msg , { 'request' : request , 'reason' : e } , exc_info = True , extra = { 'spider' : self . spider } ) self . logunser = False self . stats . inc_value ( 'scheduler/unserializable' , spider = self . spider ) return else : return True def _mqpush ( self , request ): self . mqs . push ( request , - request . priority ) def _dqpop ( self ): if self . dqs : return self . dqs . pop () def _newmq ( self , priority ): \"\"\" Factory for creating memory queues. \"\"\" return self . mqclass () def _newdq ( self , priority ): \"\"\" Factory for creating disk queues. \"\"\" path = join ( self . dqdir , 'p%s' % ( priority , )) return self . dqclass ( path ) def _mq ( self ): \"\"\" Create a new priority queue instance, with in-memory storage \"\"\" return create_instance ( self . pqclass , None , self . crawler , self . _newmq , serialize = False ) def _dq ( self ): \"\"\" Create a new priority queue instance, with disk storage \"\"\" state = self . _read_dqs_state ( self . dqdir ) q = create_instance ( self . pqclass , None , self . crawler , self . _newdq , state , serialize = True ) if q : logger . info ( \"Resuming crawl (%(queuesize)d requests scheduled)\" , { 'queuesize' : len ( q ) } , extra = { 'spider' : self . spider } ) return q def _dqdir ( self , jobdir ): \"\"\" Return a folder name to keep disk queue state at \"\"\" if jobdir : dqdir = join ( jobdir , 'requests.queue' ) if not exists ( dqdir ): os . makedirs ( dqdir ) return dqdir def _read_dqs_state ( self , dqdir ): path = join ( dqdir , 'active.json' ) if not exists ( path ): return () with open ( path ) as f : return json . load ( f ) def _write_dqs_state ( self , dqdir , state ): with open ( join ( dqdir , 'active.json' ), 'w' ) as f : json . dump ( state , f ) \u6211\u4eec\u77e5\u9053\u4e86\u8fd9\u4e2aSCHEDULER(\u8c03\u5ea6\u5668)\u4e3b\u8981\u662f\u5b8c\u6210\u4e86push Request pop Request\u53bb\u91cd\u64cd\u4f5c\uff0c\u800c\u4e14\u662fqueue(\u961f\u5217)\u64cd\u4f5c\u662f\u5728\u5185\u5b58\u961f\u5217\u4e2d\u5b8c\u6210\u7684 \u53bb\u91cd\u7684\u6e90\u7801\u6211\u4eec\u8fdb\u4e00\u6b65\u4e86\u89e3\u4e0b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 # scrapy . core . scheduler . py : class RFPDupeFilter ( BaseDupeFilter ) : \"\"\"\u8bf7\u6c42\u6307\u7eb9\u91cd\u590d\u7b5b\u9009\"\"\" def __init__ ( self , path = None , debug = False ) : self . file = None # \u96c6\u5408\u53bb\u91cd\u554a \uff01 \u96c6\u5408\u90a3\u5c31\u662f\u5728\u5185\u5b58\u4e2d \uff0c \u8fd9\u70b9\u5f88\u91cd\u8981 self . fingerprints = set () self . logdupes = True self . debug = debug self . logger = logging . getLogger ( __name__ ) if path : # \u53bb\u770b\u5230\u53bb\u91cd\u5176\u5b9e\u6253\u5f00\u4e86\u4e00\u4e2arequests . seen\u7684\u6587\u4ef6 # \u5982\u679c\u662f\u4f7f\u7528\u7684\u78c1\u76d8\u7684\u8bdd self . file = open ( os . path . join ( path , 'requests.seen' ), 'a+' ) self . file . seek ( 0 ) #\u5c06\u6587\u4ef6\u4e2d\u7684\u8bf7\u6c42\u66f4\u65b0\u5230\u5185\u5b58\u96c6\u5408\u4e2d\u53bb self . fingerprints . update ( x . rstrip () for x in self . file ) @classmethod def from_settings ( cls , settings ) : debug = settings . getbool ( 'DUPEFILTER_DEBUG' ) return cls ( job_dir ( settings ), debug ) def request_seen ( self , request ) : fp = self . request_fingerprint ( request ) if fp in self . fingerprints : # \u5224\u65ad\u6211\u4eec\u7684\u8bf7\u6c42\u662f\u5426\u5728\u8fd9\u4e2a\u96c6\u5408\u4e2d return True self . fingerprints . add ( fp ) # \u5982\u679c\u7528\u7684\u78c1\u76d8\u961f\u5217\u5c31\u5199\u8fdb\u53bb\u8bb0\u5f55\u4e00\u4e0b if self . file : self . file . write ( fp + os . linesep ) def request_fingerprint ( self , request ) : return request_fingerprint ( request ) def close ( self , reason ) : if self . file : self . file . close () #log\u65e5\u5fd7 def log ( self , request , spider ) : if self . debug : msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\" args = { 'request' : request , 'referer' : referer_str ( request ) } self . logger . debug ( msg , args , extra = { 'spider' : spider } ) elif self . logdupes : msg = ( \"Filtered duplicate request: %(request)s\" \" - no more duplicates will be shown\" \" (see DUPEFILTER_DEBUG to show all duplicates)\" ) self . logger . debug ( msg , { 'request' : request } , extra = { 'spider' : spider } ) self . logdupes = False spider . crawler . stats . inc_value ( 'dupefilter/filtered' , spider = spider ) 1 \u53bb\u91cd\u6211\u4eec\u77e5\u9053\u4e86\uff01\u4e00\u4e2a\u662f\u5229\u7528\u78c1\u76d8\u6253\u5f00\u4e2a\u6587\u4ef6\uff0c\u8fd9\u6837\u5c31\u80fd\u5173\u95ed\u722c\u866b\u5c31\u4e0d\u4f1a\u4e22\u5931\u6570\u636e\u4e86\uff0c\u4e00\u4e2a\u5c31\u662f\u5229\u7528\u96c6\u5408\u4f7f\u7528\u5185\u5b58\u7684\u65b9\u5f0f\u53bb\u91cd\uff0c\u4f18\u70b9\u662f\u5feb\uff0c\u4f46\u662f\u5173\u95ed\u722c\u866b\u5c31\u4f1a\u4e22\u5931\u6570\u636e\u3002","title":"Scrapy-redis"},{"location":"basics/scrapy/07_scrapy_redis/07_scrapy_redis/#scrapy-redis_1","text":"\u6211\u4eec\u4e86\u89e3\u4e86scrapy \u7684\u8c03\u5ea6\u5668\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u60c5\u51b5\u4e0b\u6211\u4eec\u80fd\u5426\u6539\u88c5\u4e0b\u5462\uff1f\u90a3\u662f\u53ef\u4ee5\u7684 \u6211\u4eec\u53ef\u4ee5\u770b\u4e0b\u4ed6\u7684\u6e90\u7801 1 git clone https://github.com/rmax/scrapy-redis.git \u4ed6\u7684\u6e90\u7801\u662f\u5728src\u4e0b \u5148\u770b\u4ed6\u91cd\u5199scheduler(\u8c03\u5ea6\u5668) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class Scheduler ( object ) : \"\"\"Redis-based scheduler Settings -------- SCHEDULER_PERSIST : bool (default: False) Whether to persist or clear redis queue. SCHEDULER_FLUSH_ON_START : bool (default: False) Whether to flush redis queue on start. SCHEDULER_IDLE_BEFORE_CLOSE : int (default: 0) How many seconds to wait before closing if no message is received. SCHEDULER_QUEUE_KEY : str Scheduler redis key. SCHEDULER_QUEUE_CLASS : str Scheduler queue class. SCHEDULER_DUPEFILTER_KEY : str Scheduler dupefilter redis key. SCHEDULER_DUPEFILTER_CLASS : str Scheduler dupefilter class. SCHEDULER_SERIALIZER : str Scheduler serializer. \"\"\" def __init__ ( self , server , persist = False , flush_on_start = False , queue_key = defaults . SCHEDULER_QUEUE_KEY , queue_cls = defaults . SCHEDULER_QUEUE_CLASS , dupefilter_key = defaults . SCHEDULER_DUPEFILTER_KEY , dupefilter_cls = defaults . SCHEDULER_DUPEFILTER_CLASS , idle_before_close = 0 , serializer = None ) : \"\"\"Initialize scheduler. Parameters ---------- server : Redis The redis server instance. persist : bool Whether to flush requests when closing. Default is False. flush_on_start : bool Whether to flush requests on start. Default is False. queue_key : str Requests queue key. queue_cls : str Importable path to the queue class. dupefilter_key : str Duplicates filter key. dupefilter_cls : str Importable path to the dupefilter class. idle_before_close : int Timeout before giving up. \"\"\" \"\"\" Parameters ---------- server : Redis \u8fd9\u662fRedis\u5b9e\u4f8b persist : bool \u662f\u5426\u5728\u5173\u95ed\u65f6\u6e05\u7a7aRequests.\u9ed8\u8ba4\u503c\u662fFalse\u3002 flush_on_start : bool \u662f\u5426\u5728\u542f\u52a8\u65f6\u6e05\u7a7aRequests\u3002 \u9ed8\u8ba4\u503c\u662fFalse\u3002 queue_key : str Request\u961f\u5217\u7684Key\u540d\u5b57 queue_cls : str \u961f\u5217\u7684\u53ef\u5bfc\u5165\u8def\u5f84\uff08\u5c31\u662f\u4f7f\u7528\u4ec0\u4e48\u961f\u5217\uff09 dupefilter_key : str \u53bb\u91cd\u961f\u5217\u7684Key dupefilter_cls : str \u53bb\u91cd\u7c7b\u7684\u53ef\u5bfc\u5165\u8def\u5f84\u3002 idle_before_close : int \u7b49\u5f85\u591a\u4e45\u5173\u95ed \"\"\" if idle_before_close < 0 : raise TypeError ( \"idle_before_close cannot be negative\" ) self . server = server self . persist = persist self . flush_on_start = flush_on_start self . queue_key = queue_key self . queue_cls = queue_cls self . dupefilter_cls = dupefilter_cls self . dupefilter_key = dupefilter_key self . idle_before_close = idle_before_close self . serializer = serializer self . stats = None def __len__ ( self ) : return len ( self . queue ) @classmethod def from_settings ( cls , settings ) : # \u4ecesetting\u4e2d\u5bfc\u5165\u53c2\u6570 kwargs = { 'persist' : settings . getbool ( 'SCHEDULER_PERSIST' ), 'flush_on_start' : settings . getbool ( 'SCHEDULER_FLUSH_ON_START' ), 'idle_before_close' : settings . getint ( 'SCHEDULER_IDLE_BEFORE_CLOSE' ), } # If these values are missing , it means we want to use the defaults . optional = { # TODO : Use custom prefixes for this settings to note that are # specific to scrapy - redis . 'queue_key' : 'SCHEDULER_QUEUE_KEY' , 'queue_cls' : 'SCHEDULER_QUEUE_CLASS' , 'dupefilter_key' : 'SCHEDULER_DUPEFILTER_KEY' , # We use the default setting name to keep compatibility . 'dupefilter_cls' : 'DUPEFILTER_CLASS' , 'serializer' : 'SCHEDULER_SERIALIZER' , } # \u4ecesetting \u4e2d\u83b7\u53d6\u914d\u7f6e\u7ec4\u88c5\u6210dict ( \u5177\u4f53\u83b7\u53d6\u54ea\u4e9b\u914d\u7f6e\u662foptional\u5b57\u5178\u4e2dkey ) for name , setting_name in optional . items () : val = settings . get ( setting_name ) if val : kwargs [ name ] = val # Support serializer as a path to a module . if isinstance ( kwargs . get ( 'serializer' ), six . string_types ) : kwargs [ 'serializer' ] = importlib . import_module ( kwargs [ 'serializer' ] ) # \u83b7\u53d6\u4e00\u4e2aRedis \u8fde\u63a5 server = connection . from_settings ( settings ) # Ensure the connection is working . # \u786e\u4fdd\u8fde\u63a5\u6b63\u5e38 server . ping () return cls ( server = server , ** kwargs ) @classmethod def from_crawler ( cls , crawler ) : instance = cls . from_settings ( crawler . settings ) # FIXME : for now , stats are only supported from this constructor instance . stats = crawler . stats return instance def open ( self , spider ) : self . spider = spider try : # \u6839\u636eself . queue_cls\u8fd9\u4e2a\u53ef\u4ee5\u5bfc\u5165\u7684\u7c7b \uff0c \u5b9e\u4f8b\u5316\u4e00\u4e2a\u961f\u5217 self . queue = load_object ( self . queue_cls )( server = self . server , spider = spider , key = self . queue_key % { 'spider' : spider . name } , serializer = self . serializer , ) except TypeError as e : raise ValueError ( \"Failed to instantiate queue class '%s': %s\" , self . queue_cls , e ) # \u6839\u636eself . dupefilter_cls \u8fd9\u4e2a\u53ef\u4ee5\u5bfc\u5165\u7684\u7c7b \uff0c \u5b9e\u4f8b\u4e00\u4e2a\u53bb\u91cd\u96c6\u5408 # \u9ed8\u8ba4\u662f\u96c6\u5408 \uff0c \u53ef\u4ee5\u5b9e\u73b0\u81ea\u5df1\u7684\u53bb\u91cd\u65b9\u5f0f \uff0c \u6bd4\u5982bool\u53bb\u91cd self . df = load_object ( self . dupefilter_cls ). from_spider ( spider ) if self . flush_on_start : self . flush () # notice if there are requests already in the queue to resume the crawl if len ( self . queue ) : spider . log ( \"Resuming crawl (%d requests scheduled)\" % len ( self . queue )) def close ( self , reason ) : if not self . persist : self . flush () def flush ( self ) : self . df . clear () self . queue . clear () def enqueue_request ( self , request ) : \"\"\" \u8fd9\u4e2a\u548cscrapy \u672c\u8eab\u7684\u4e00\u6837 :param request: :return: \"\"\" if not request . dont_filter and self . df . request_seen ( request ) : self . df . log ( request , self . spider ) return False if self . stats : self . stats . inc_value ( 'scheduler/enqueued/redis' , spider = self . spider ) # \u5411\u961f\u5217\u91cc\u6dfb\u52a0\u4e00\u4e2aRequest self . queue . push ( request ) return True def next_request ( self ) : \"\"\" \u83b7\u53d6\u4e00\u4e2aRequest :return: \"\"\" block_pop_timeout = self . idle_before_close # block_pop_timeout \u662f\u4e00\u4e2a\u7b49\u5f85\u53c2\u6570 \uff0c \u961f\u5217\u6ca1\u6709\u4e1c\u897f\u4f1a\u7b49\u5f85\u8fd9\u4e2a\u65f6\u95f4 \uff0c \u8d85\u65f6\u5c31\u4f1a\u5173\u95ed request = self . queue . pop ( block_pop_timeout ) if request and self . stats : self . stats . inc_value ( 'scheduler/dequeued/redis' , spider = self . spider ) return request # \u5224\u65ad\u662f\u5426\u8fd8\u6709\u672a\u8c03\u5ea6\u7684\u8bf7\u6c42 def has_pending_requests ( self ) : return len ( self ) > 0 \u9664\u4e86\u8c03\u5ea6\u5668\u6539\u5199\u4e86,\u6211\u4eec\u8fd8\u770b\u5230\u6e90\u7801\u603b\u8fd8\u6709queue \u8fd9\u662f\u5bf9\u961f\u5217\u8fdb\u884c\u4e86\u6539\u88c5\uff0c\u6211\u4eec\u770b\u4e00\u4e0b\u961f\u5217\u5230\u5e95\u6539\u9020\u4e86\u4ec0\u4e48 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # scrapy_redis.queue.py class PriorityQueue ( Base ): \"\"\"Per-spider priority queue abstraction using redis' sorted set\"\"\" \"\"\"\u5176\u5b9e\u5c31\u662f\u4f7f\u7528Redis\u7684\u6709\u5e8f\u96c6\u5408\uff0c\u6765\u5bf9Request\u8fdb\u884c\u6392\u5e8f\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u4f18\u5148\u7ea7\u9ad8\u7684\u5728\u6709\u5e8f\u96c6\u5408\u7684\u9876\u5c42 \u6211\u4eec\u53ea\u9700\u8981 \u4ece\u4e0a\u5f80\u4e0b\u83b7\u53d6Request\u5373\u53ef\"\"\" def __len__ ( self ): \"\"\"Return the length of the queue\"\"\" return self . server . zcard ( self . key ) def push ( self , request ): \"\"\"Push a request\"\"\" \"\"\"\u6dfb\u52a0\u4e00\u4e2aRequest\u8fdb\u961f\u5217\"\"\" # self._encode_request(request)\u5c06Request\u8bf7\u6c42\u5e8f\u5217\u5316 data = self . _encode_request ( request ) \"\"\" d = { 'url': to_unicode(request.url), # urls should be safe (safe_string_url) 'callback': cb, 'errback': eb, 'method': request.method, 'headers': dict(request.headers), 'body': request.body, 'cookies': request.cookies, 'meta': request.meta, '_encoding': request._encoding, 'priority': request.priority, 'dont_filter': request.dont_filter, 'flags': request.flags, '_class': request.__module__ + '.' + request.__class__.__name__ } \"\"\" # data \u5c31\u662f\u4e0a\u9762\u8fd9\u4e2a\u5b57\u5178\u7684\u5e8f\u5217\u5316\uff0c\u5728Scrapy.util.reqser.py\u4e2d\u7684request_to_dict\u65b9\u6cd5\u4e2d\u5904\u7406 # \u5728redis \u6709\u5e8f\u96c6\u5408\u4e2d\u6570\u503c\u8d8a\u5c0f\u4f18\u5148\u7ea7\u8d8a\u9ad8\uff08\u5c31\u662f\u4f1a\u88ab\u653e\u5728\u9876\u5c42\uff09 \u6240\u4ee5\u8fd9\u4e2a\u4f4d\u7f6e\u662f\u53d6\u5f97\u76f8\u53cd\u6570 score = - request . priority # We don't use zadd method as the order of arguments change depending on # whether the class is Redis or StrictRedis, and the option of using # kwargs only accepts strings, not bytes. # ZADD \u662f\u6dfb\u52a0\u8fdb\u6765\u6709\u5e8f\u96c6\u5408 self . server . execute_command ( 'ZADD' , self . key , score , data ) def pop ( self , timeout = 0 ): \"\"\" Pop a request timeout not support in this queue class \u6709\u5e8f\u96c6\u5408\u4e0d\u652f\u6301\u8d85\u65f6\u5c31\u6728\u6709\u4f7f\u7528timeout\u4e86 \u8fd9\u4e2atimeout\u5c31\u662f\u6302\u7f8a\u5934\u4e70\u72d7\u8089 \"\"\" # use atomic range/remove using multi/exec pipe = self . server . pipeline () pipe . multi () # \u53d6\u51fa \u9876\u5c42\u7b2c\u4e00\u4e2a # zrange : \u8fd4\u56de\u6709\u5e8f\u96c6 key \u4e2d\uff0c\u6307\u5b9a\u533a\u95f4\u5185\u7684\u6210\u5458 0\uff0c0 \u5c31\u662f\u7b2c\u4e00\u4e2a # zremrangebyrank:\u79fb\u9664\u6709\u5e8f\u96c6key \u4e2d\uff0c\u6307\u5b9a\u6392\u540d\uff08rank\uff09 \u533a\u95f4\u5185\u7684\u6240\u6709\u6210\u5458 0\uff0c0\u4e5f\u5c31\u662f\u7b2c\u4e00\u4e2a\u4e86 pipe . zrange ( self . key , 0 , 0 ). zremrangebyrank ( self . key , 0 , 0 ) results , count = pipe . execute () if results: return self . _decode_request ( results [ 0 ]) \u662f\u4e00\u4e2a\u666e\u901a\u7684\u961f\u5217\uff0c\u653eredis\u91cc\u9762\u5148\u8fdb\u5148\u51fa \u9664\u4e86\u961f\u5217\u8fdb\u884c\u4e86\u6539\u88c5\uff0c\u8fd8\u6709\u5bf9\u53bb\u91cd\u4e5f\u8fdb\u884c\u4e86\u6539\u88c5\uff0c\u6211\u4eec\u6765\u770b\u770b\u53bb\u91cd\u662f\u5982\u4f55\u6539\u88c5\u7684 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 # scrapy_redis . dupefilter . py class RFPDupeFilter ( BaseDupeFilter ) : \"\"\"Redis-based request duplicates filter. This class can also be used with default Scrapy's scheduler. \"\"\" logger = logger def __init__ ( self , server , key , debug = False ) : \"\"\"Initialize the duplicates filter. Parameters ---------- server : redis.StrictRedis The redis server instance. key : str Redis key Where to store fingerprints. debug : bool, optional Whether to log filtered requests. \"\"\" self . server = server self . key = key self . debug = debug self . logdupes = True @classmethod def from_settings ( cls , settings ) : \"\"\"Returns an instance from given settings. This uses by default the key ``dupefilter:<timestamp>``. When using the ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as it needs to pass the spider name in the key. Parameters ---------- settings : scrapy.settings.Settings Returns ------- RFPDupeFilter A RFPDupeFilter instance. \"\"\" server = get_redis_from_settings ( settings ) # XXX : This creates one - time key . needed to support to use this # class as standalone dupefilter with scrapy 's default scheduler # if scrapy passes spider on open() method this wouldn' t be needed # TODO : Use SCRAPY_JOB env as default and fallback to timestamp . key = defaults . DUPEFILTER_KEY % { 'timestamp' : int ( time . time ()) } debug = settings . getbool ( 'DUPEFILTER_DEBUG' ) return cls ( server , key = key , debug = debug ) @classmethod def from_crawler ( cls , crawler ) : \"\"\"Returns instance from crawler. Parameters ---------- crawler : scrapy.crawler.Crawler Returns ------- RFPDupeFilter Instance of RFPDupeFilter. \"\"\" return cls . from_settings ( crawler . settings ) def request_seen ( self , request ) : \"\"\"Returns True if request was already seen. Parameters ---------- request : scrapy.http.Request Returns ------- bool \"\"\" # \u901a\u8fc7self . request_fingerprint \u4f1a\u751f\u4e00\u4e2asha1\u7684\u6307\u7eb9 fp = self . request_fingerprint ( request ) # This returns the number of values added , zero if already exists . # \u6dfb\u52a0\u8fdb\u4e00\u4e2aRedis\u96c6\u5408\u5982\u679cself . key\u8fd9\u4e2a\u96c6\u5408\u4e2d\u5b58\u5728fp\u8fd9\u4e2a\u96c6\u5408\u4f1a\u8fd4\u56de1 \u4e0d\u5b58\u5728\u8fd4\u56de0 added = self . server . sadd ( self . key , fp ) return added == 0 def request_fingerprint ( self , request ) : \"\"\"Returns a fingerprint for a given request. Parameters ---------- request : scrapy.http.Request Returns ------- str \"\"\" return request_fingerprint ( request ) @classmethod def from_spider ( cls , spider ) : settings = spider . settings server = get_redis_from_settings ( settings ) dupefilter_key = settings . get ( \"SCHEDULER_DUPEFILTER_KEY\" , defaults . SCHEDULER_DUPEFILTER_KEY ) key = dupefilter_key % { 'spider' : spider . name } debug = settings . getbool ( 'DUPEFILTER_DEBUG' ) return cls ( server , key = key , debug = debug ) def close ( self , reason = '' ) : \"\"\"Delete data on close. Called by Scrapy's scheduler. Parameters ---------- reason : str, optional \"\"\" self . clear () def clear ( self ) : \"\"\"Clears fingerprints data.\"\"\" self . server . delete ( self . key ) def log ( self , request , spider ) : \"\"\"Logs given request. Parameters ---------- request : scrapy.http.Request spider : scrapy.spiders.Spider \"\"\" if self . debug : msg = \"Filtered duplicate request: %(request)s\" self . logger . debug ( msg , { 'request' : request } , extra = { 'spider' : spider } ) elif self . logdupes : msg = ( \"Filtered duplicate request %(request)s\" \" - no more duplicates will be shown\" \" (see DUPEFILTER_DEBUG to show all duplicates)\" ) self . logger . debug ( msg , { 'request' : request } , extra = { 'spider' : spider } ) self . logdupes = False \u6211\u4eec\u4e3b\u8981\u770b\u4ed6\u8fd9\u4e2a\u65b9\u6cd5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def request_seen ( self , request ): \"\"\"Returns True if request was already seen. Parameters ---------- request : scrapy.http.Request Returns ------- bool \"\"\" # \u901a\u8fc7 self . request_fingerprint \u4f1a\u751f\u4e00\u4e2a sha1\u7684\u6307\u7eb9 fp = self . request_fingerprint ( request ) # This returns the number of values added , zero if already exists . # \u6dfb\u52a0\u8fdb\u4e00\u4e2a Redis\u96c6\u5408\u5982\u679cself . key\u8fd9\u4e2a\u96c6\u5408\u4e2d\u5b58\u5728fp\u8fd9\u4e2a\u96c6\u5408\u4f1a\u8fd4\u56de1 \u4e0d\u5b58\u5728\u8fd4\u56de 0 added = self . server . sadd ( self . key , fp ) return added == 0 \u5c31\u662f\u8bf4request_seen()\u8fd9\u4e2a\u65b9\u6cd5\u4f1a\u5c06\u8bf7\u6c42\u52a0\u5bc6\u7684\u90a3\u4e2akey\u505a\u5224\u65ad\u3002\u5b58\u5728\u5c31\u8fd4\u56de1,\u4e0d\u5b58\u5728\u5c31\u8fd4\u56de0\u3002\u8fd9\u6837\u5c31\u975e\u5e38\u7b80\u7b54\u5224\u65ad\u8bf7\u6c42\uff0c\u7136\u540e\u8fc7\u6ee4","title":"scrapy-redis\u6b63\u5f0f\u767b\u573a"},{"location":"basics/scrapy/07_scrapy_redis/07_scrapy_redis/#_1","text":"scrapy-redis\u4f5c\u4e3a\u5bf9Scrapy \u7684\u8c03\u53d6\u5668\u8fdb\u884c\u6539\u9020\uff0c\u5c06\u539f\u672cscrapy \u7684\u8bf7\u6c42\u57fa\u4e8e\u961f\u5217\uff08Queue\uff09\u5185\u5b58\u5904\u7406\uff0c\u8fd8\u6709\u53bb\u91cd\u8fd9\u4e24\u4e2a\u90e8\u5206\u4f7f\u7528redis\u53bb\u5b9e\u73b0","title":"\u603b\u7ed3"},{"location":"basics/scrapy/07_scrapy_redis/07_scrapy_redis/#_2","text":"\u53bb\u91cd\u662f\u5982\u4f55\u505a\u5230\u7684\uff0c\u4f1a\u4e0d\u4f1a\u6709\u91cd\u590d \u53bb\u91cd\u662f\u5c06request hash\u505a\u6210key.request\u4ed6\u662f\u4f1a\u5148\u5e8f\u5217\u5316\u7684\uff0c\u5e8f\u5217\u5316\u7684\u65f6\u5019\u5c31\u5305\u62ec\u4e86\u5f88\u591a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def request_to_dict ( request , spider = None ): \"\"\"Convert Request object to a dict. If a spider is given, it will try to find out the name of the spider method used in the callback and store that as the callback. \"\"\" cb = request . callback if callable ( cb ): cb = _find_method ( spider , cb ) eb = request . errback if callable ( eb ): eb = _find_method ( spider , eb ) d = { 'url' : to_unicode ( request . url ), # urls should be safe (safe_string_url) 'callback' : cb , 'errback' : eb , 'method' : request . method , 'headers' : dict ( request . headers ), 'body' : request . body , 'cookies' : request . cookies , 'meta' : request . meta , '_encoding' : request . _encoding , 'priority' : request . priority , 'dont_filter' : request . dont_filter , 'flags' : request . flags , 'cb_kwargs' : request . cb_kwargs , } if type ( request ) is not Request : d [ '_class' ] = request . __module__ + '.' + request . __class__ . __name__ return d \u5e76\u4e0d\u662f\u5355\u5355\u5bf9url\u505a\u5904\u7406\u54e6\uff01\u6240\u4ee5\u522b\u62c5\u5fc3\u3002\u57fa\u672c\u53c2\u6570\u90fd\u505a\u5904\u7406\uff0c\u90a3\u5c82\u4e0d\u662f\u7f8e\u6ecb\u6ecb\uff0c\u518d\u4e5f\u4e0d\u7528\u62c5\u5fc3\u6211\u722c\u7684\u7adf\u7136\u6ca1\u53bb\u91cd.\u5f53\u7136\u6211\u4eec\u4e5f\u662f\u53ef\u4ee5\u6539\u88c5\u7684\uff0c\u4ee5\u53bb\u9002\u5408\u9700\u6c42\u7684\u53d8\u5316 hash \u4e00\u6b21key\u6570\u503c\u8fdbredis\u503c\u5927\u4e0d\u5927\uff0c\u80fd\u4e0d\u80fd\u4f18\u826f\u6539\u88c5\u4e00\u4e0b \u4f7f\u7528 \u5e03\u9686\u8fc7\u6ee4\u5668 \u5177\u4f53\u539f\u7406\u4e0b\u6b21\u7814\u7a76 3.\u5982\u4f55\u4f7f\u7528scrapy-redis \u8fd9\u91cc\u662f \u5b98\u65b9\u6587\u6863 \uff0c\u6309\u7167\u4ed6\u7684\u8981\u6c42\u5728\u8bbe\u7f6e\u5c31\u884c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #\u542f\u7528Redis\u8c03\u5ea6\u5b58\u50a8\u8bf7\u6c42\u961f\u5217 SCHEDULER = \"scrapy_redis.scheduler.Scheduler\" #\u786e\u4fdd\u6240\u6709\u7684\u722c\u866b\u901a\u8fc7Redis\u53bb\u91cd DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\" #\u9ed8\u8ba4\u8bf7\u6c42\u5e8f\u5217\u5316\u4f7f\u7528\u7684\u662fpickle \u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u66f4\u6539\u4e3a\u5176\u4ed6\u7c7b\u4f3c\u7684\u3002PS\uff1a\u8fd9\u73a9\u610f\u513f2.X\u7684\u53ef\u4ee5\u7528\u30023.X\u7684\u4e0d\u80fd\u7528 #SCHEDULER_SERIALIZER = \"scrapy_redis.picklecompat\" #\u4e0d\u6e05\u9664Redis\u961f\u5217\u3001\u8fd9\u6837\u53ef\u4ee5\u6682\u505c/\u6062\u590d \u722c\u53d6 #SCHEDULER_PERSIST = True #\u4f7f\u7528\u4f18\u5148\u7ea7\u8c03\u5ea6\u8bf7\u6c42\u961f\u5217 \uff08\u9ed8\u8ba4\u4f7f\u7528\uff09 #SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue' #\u53ef\u9009\u7528\u7684\u5176\u5b83\u961f\u5217 #SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue' #SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue' #\u6700\u5927\u7a7a\u95f2\u65f6\u95f4\u9632\u6b62\u5206\u5e03\u5f0f\u722c\u866b\u56e0\u4e3a\u7b49\u5f85\u800c\u5173\u95ed #SCHEDULER_IDLE_BEFORE_CLOSE = 10 #\u5c06\u6e05\u9664\u7684\u9879\u76ee\u5728redis\u8fdb\u884c\u5904\u7406 ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 300 } #\u5e8f\u5217\u5316\u9879\u76ee\u7ba1\u9053\u4f5c\u4e3aredis Key\u5b58\u50a8 #REDIS_ITEMS_KEY = '%(spider)s:items' #\u9ed8\u8ba4\u4f7f\u7528ScrapyJSONEncoder\u8fdb\u884c\u9879\u76ee\u5e8f\u5217\u5316 #You can use any importable path to a callable object. #REDIS_ITEMS_SERIALIZER = 'json.dumps' #\u6307\u5b9a\u8fde\u63a5\u5230redis\u65f6\u4f7f\u7528\u7684\u7aef\u53e3\u548c\u5730\u5740\uff08\u53ef\u9009\uff09 #REDIS_HOST = 'localhost' #REDIS_PORT = 6379 #\u6307\u5b9a\u7528\u4e8e\u8fde\u63a5redis\u7684URL\uff08\u53ef\u9009\uff09 #\u5982\u679c\u8bbe\u7f6e\u6b64\u9879\uff0c\u5219\u6b64\u9879\u4f18\u5148\u7ea7\u9ad8\u4e8e\u8bbe\u7f6e\u7684REDIS_HOST \u548c REDIS_PORT #REDIS_URL = 'redis://user:pass@hostname:9001' #\u81ea\u5b9a\u4e49\u7684redis\u53c2\u6570\uff08\u8fde\u63a5\u8d85\u65f6\u4e4b\u7c7b\u7684\uff09 #REDIS_PARAMS = {} #\u81ea\u5b9a\u4e49redis\u5ba2\u6237\u7aef\u7c7b #REDIS_PARAMS['redis_cls'] = 'myproject.RedisClient' #\u5982\u679c\u4e3aTrue\uff0c\u5219\u4f7f\u7528redis\u7684'spop'\u8fdb\u884c\u64cd\u4f5c\u3002 #\u5982\u679c\u9700\u8981\u907f\u514d\u8d77\u59cb\u7f51\u5740\u5217\u8868\u51fa\u73b0\u91cd\u590d\uff0c\u8fd9\u4e2a\u9009\u9879\u975e\u5e38\u6709\u7528\u3002\u5f00\u542f\u6b64\u9009\u9879urls\u5fc5\u987b\u901a\u8fc7sadd\u6dfb\u52a0\uff0c\u5426\u5219\u4f1a\u51fa\u73b0\u7c7b\u578b\u9519\u8bef\u3002 #REDIS_START_URLS_AS_SET = False #RedisSpider\u548cRedisCrawlSpider\u9ed8\u8ba4 start_usls \u952e #REDIS_START_URLS_KEY = '%(name)s:start_urls' #\u8bbe\u7f6eredis\u4f7f\u7528utf-8\u4e4b\u5916\u7684\u7f16\u7801 #REDIS_ENCODING = 'latin1' \u5b9e\u8df5\u6848\u4f8b\u6709\u5417\uff1f \u53ef\u4ee5\u770b\u4e0b\u722c\u53d6scrapy-redis\u5168\u7f51\u7684\u5f53\u5f53\u7f51","title":"\u601d\u8003"},{"location":"basics/scrapy/07_scrapy_redis/07_scrapy_redis/#_3","text":"https://zhuanlan.zhihu.com/p/79120407 https://www.jianshu.com/p/2104d11ee0a2","title":"\u53c2\u8003\u8d44\u6599"},{"location":"basics/scrapy/08_scrapy_scrapyd/08_scrapy_scrapyd/","text":"\u524d\u8a00 \u00b6 scrapyd \u901a\u5e38\u4f5c\u4e3a\u5b88\u62a4\u8fdb\u7a0b\u8fd0\u884c\uff0c\u4ed6\u76d1\u542c\u8fd0\u884c\u722c\u866b\u7684\u8bf7\u6c42\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u8bf7\u6c42\u751f\u6210\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u8be5\u8fdb\u7a0b\u57fa\u672c\u4e0a\u6267\u884cscrapy crawl [mysider]\u3002\u597d\u5904\u5c31\u662f\u53ef\u4ee5\u4f7f\u7528restapi\u53bb\u63a7\u5236\u7ebf\u4e0a\u7684\u722c\u866b\uff0c\u7f3a\u70b9\u5c31\u662f\u6ca1\u6709\u8ba4\u8bc1\uff0c\u4f1a\u6ca6\u4e3a\u77ff\u673a\uff0c\u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u505a\u767d\u540d\u5355\u6216\u8005\u662fnginx\u8ba4\u8bc1\u7684\u65b9\u5f0f\u53bb\u505a\u4fdd\u62a4 docker\u5236\u4f5c \u00b6 Dockerfile\u6587\u4ef6 1 2 3 4 5 6 7 8 9 # dockerfile FROM python:3.6 ADD . /code WORKDIR /code COPY ./scrapyd.conf /etc/scrapyd/ EXPOSE 6800 RUN pip --default-timeout = 100 install -U pip RUN pip3 install -r requirements.txt -i https://pypi.douban.com/simple CMD scrapyd scrapyd.conf\u6587\u4ef6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # scrapyd . conf [ scrapyd ] eggs_dir = eggs logs_dir = logs items_dir = jobs_to_keep = 5 dbs_dir = dbs max_proc = 0 max_proc_per_cpu = 10 finished_to_keep = 100 poll_interval = 5.0 # \u652f\u6301\u8fdc\u7a0b\u8fde\u63a5 bind_address = 0.0.0.0 http_port = 6800 debug = off runner = scrapyd . runner application = scrapyd . app . application launcher = scrapyd . launcher . Launcher webroot = scrapyd . website . Root [ services ] schedule . json = scrapyd . webservice . Schedule cancel . json = scrapyd . webservice . Cancel addversion . json = scrapyd . webservice . AddVersion listprojects . json = scrapyd . webservice . ListProjects listversions . json = scrapyd . webservice . ListVersions listspiders . json = scrapyd . webservice . ListSpiders delproject . json = scrapyd . webservice . DeleteProject delversion . json = scrapyd . webservice . DeleteVersion listjobs . json = scrapyd . webservice . ListJobs daemonstatus . json = scrapyd . webservice . DaemonStatus requirements.txt 1 2 scrapyd .... \u8fd9\u91ccrequirements.txt\u662fscrapy\u91cc\u6240\u6709\u7684\u4f9d\u8d56\u73af\u5883,\u6240\u4ee5\u5fc5\u987b\u5148\u628ascrapy\u7684\u4f9d\u8d56\u5bfc\u51fa 1 2 # scrapy \u6839\u76ee\u5f55\u4e0b pip freeze >> requirements.txt \u542f\u52a8 \u00b6 1 2 3 # \u5728\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u4f7f\u7528\uff0c\u66b4\u9732\u7aef\u53e36800\uff0c\u652f\u6301\u8fdc\u7a0b\u8fde\u63a5 docker build -t scrapyd:latest . docker run -d -p 6800:6800 scrapyd \u53c2\u8003\u94fe\u63a5 \u00b6 https://juejin.im/post/5d771c69f265da03ee6a8038","title":"Scrapy scrapyd"},{"location":"basics/scrapy/08_scrapy_scrapyd/08_scrapy_scrapyd/#_1","text":"scrapyd \u901a\u5e38\u4f5c\u4e3a\u5b88\u62a4\u8fdb\u7a0b\u8fd0\u884c\uff0c\u4ed6\u76d1\u542c\u8fd0\u884c\u722c\u866b\u7684\u8bf7\u6c42\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u8bf7\u6c42\u751f\u6210\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u8be5\u8fdb\u7a0b\u57fa\u672c\u4e0a\u6267\u884cscrapy crawl [mysider]\u3002\u597d\u5904\u5c31\u662f\u53ef\u4ee5\u4f7f\u7528restapi\u53bb\u63a7\u5236\u7ebf\u4e0a\u7684\u722c\u866b\uff0c\u7f3a\u70b9\u5c31\u662f\u6ca1\u6709\u8ba4\u8bc1\uff0c\u4f1a\u6ca6\u4e3a\u77ff\u673a\uff0c\u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u505a\u767d\u540d\u5355\u6216\u8005\u662fnginx\u8ba4\u8bc1\u7684\u65b9\u5f0f\u53bb\u505a\u4fdd\u62a4","title":"\u524d\u8a00"},{"location":"basics/scrapy/08_scrapy_scrapyd/08_scrapy_scrapyd/#docker","text":"Dockerfile\u6587\u4ef6 1 2 3 4 5 6 7 8 9 # dockerfile FROM python:3.6 ADD . /code WORKDIR /code COPY ./scrapyd.conf /etc/scrapyd/ EXPOSE 6800 RUN pip --default-timeout = 100 install -U pip RUN pip3 install -r requirements.txt -i https://pypi.douban.com/simple CMD scrapyd scrapyd.conf\u6587\u4ef6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # scrapyd . conf [ scrapyd ] eggs_dir = eggs logs_dir = logs items_dir = jobs_to_keep = 5 dbs_dir = dbs max_proc = 0 max_proc_per_cpu = 10 finished_to_keep = 100 poll_interval = 5.0 # \u652f\u6301\u8fdc\u7a0b\u8fde\u63a5 bind_address = 0.0.0.0 http_port = 6800 debug = off runner = scrapyd . runner application = scrapyd . app . application launcher = scrapyd . launcher . Launcher webroot = scrapyd . website . Root [ services ] schedule . json = scrapyd . webservice . Schedule cancel . json = scrapyd . webservice . Cancel addversion . json = scrapyd . webservice . AddVersion listprojects . json = scrapyd . webservice . ListProjects listversions . json = scrapyd . webservice . ListVersions listspiders . json = scrapyd . webservice . ListSpiders delproject . json = scrapyd . webservice . DeleteProject delversion . json = scrapyd . webservice . DeleteVersion listjobs . json = scrapyd . webservice . ListJobs daemonstatus . json = scrapyd . webservice . DaemonStatus requirements.txt 1 2 scrapyd .... \u8fd9\u91ccrequirements.txt\u662fscrapy\u91cc\u6240\u6709\u7684\u4f9d\u8d56\u73af\u5883,\u6240\u4ee5\u5fc5\u987b\u5148\u628ascrapy\u7684\u4f9d\u8d56\u5bfc\u51fa 1 2 # scrapy \u6839\u76ee\u5f55\u4e0b pip freeze >> requirements.txt","title":"docker\u5236\u4f5c"},{"location":"basics/scrapy/08_scrapy_scrapyd/08_scrapy_scrapyd/#_2","text":"1 2 3 # \u5728\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u4f7f\u7528\uff0c\u66b4\u9732\u7aef\u53e36800\uff0c\u652f\u6301\u8fdc\u7a0b\u8fde\u63a5 docker build -t scrapyd:latest . docker run -d -p 6800:6800 scrapyd","title":"\u542f\u52a8"},{"location":"basics/scrapy/08_scrapy_scrapyd/08_scrapy_scrapyd/#_3","text":"https://juejin.im/post/5d771c69f265da03ee6a8038","title":"\u53c2\u8003\u94fe\u63a5"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/","text":"\u524d\u8a00 \u00b6 python-scrapyd-api \u662f\u7528restfulapi\u7684\u5f62\u5f0f\u8c03\u7528scrapyd\u7684\u5f62\u5f0f\u53bb\u8c03\u7528scrapy \u722c\u866b \u4ecb\u7ecd \u00b6 \u8fde\u63a5 1 2 from scrapyd_api import ScrapydAPI scrapyd = ScrapydAPI ( 'http://localhost:6800' ) \u53d6\u6d88\u4efb\u52a1\u8c03\u5ea6\u8ba1\u5212 \u00b6 1 2 3 scrapyd.cancel('project_name', '14a6599ef67111e38a0e080027880ca6') # Returns the \"previous state\" of the job before it was cancelled: 'running' or 'pending'. 'running' \u5220\u9664\u9879\u76ee\u548c\u540c\u7ea7\u7248\u672c \u00b6 1 2 3 scrapyd.delete_project('project_name') # Returns True if the request was met with an OK response. True \u5220\u9664\u67d0\u4e2a\u7248\u672c\u7684\u9879\u76ee \u00b6 1 2 3 scrapyd.delete_version('project_name', 'version_name') # Returns True if the request was met with an OK response. True \u67e5\u770b\u67d0\u4e2ajob\u7684\u72b6\u6001 \u00b6 1 2 3 scrapyd.job_status('project_name', '14a6599ef67111e38a0e080027880ca6') # Returns 'running', 'pending', 'finished' or '' for unknown state. 'running' \u5217\u51fa\u6240\u6709\u7684\u9879\u76ee\u72b6\u6001\u53ca\u8be6\u7ec6\u4fe1\u606f \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 scrapyd.list_jobs('project_name') # Returns a dict of running, finished and pending job lists. { 'pending': [ { u'id': u'24c35...f12ae', u'spider': u'spider_name' }, ], 'running': [ { u'id': u'14a65...b27ce', u'spider': u'spider_name', u'start_time': u'2014-06-17 22:45:31.975358' }, ], 'finished': [ { u'id': u'34c23...b21ba', u'spider': u'spider_name', u'start_time': u'2014-06-17 22:45:31.975358', u'end_time': u'2014-06-23 14:01:18.209680' } ] } \u4efb\u52a1\u8c03\u5ea6 \u00b6 1 2 3 scrapyd.schedule('project_name', 'spider_name') # Returns the Scrapyd job id. u'14a6599ef67111e38a0e080027880ca6' \u63d0\u4f9bsetting \u4efb\u52a1\u8c03\u5ea6 \u00b6 1 2 settings = {'DOWNLOAD_DELAY': 2} scrapyd.schedule('project_name', 'spider_name', settings=settings) \u5c06\u989d\u5916\u7684\u5c5e\u6027\u4f20\u9012\u7ed9Spider\u521d\u59cb\u5316 \u00b6 1 scrapyd.schedule('project_name', 'spider_name', extra_attribute='value') \u4e3e\u4f8b \u00b6 \u6211\u4eec\u9700\u8981\u4f20\u9012\u989d\u5916\u7684\u53c2\u6570\u63a7\u5236\u722c\u866b\u53bb\u722c\u53d6\u6570\u636e\u600e\u4e48\u529e\u5462\uff1f\u4f8b\u5982Django 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # django class NewApiView ( APIView ): # permission_classes = (IsAuthenticated,) def get ( self , request ): \"\"\" \u67e5\u770b\u722c\u866b\u7684\u4e00\u4e2a\u72b6\u6001 :param request: :return: \"\"\" job_id = request . data . get ( \"job\" , None ) job_state = self . scrapyd . job_status ( 'Facebook_Request' , job_id ) if job_state == \"running\" : return Response ( \"\u6b63\u5728\u8fd0\u884c\" ) elif job_state == \"pending\" : return Response ( \"\u6b63\u5728\u542f\u52a8\" ) elif job_state == \"finished\" : return Response ( \"\u5df2\u7ecf\u7ed3\u675f\" ) else: return Response ( \"\u8bf7\u67e5\u770bjob_id\u662f\u5426\u586b\u5199\u6b63\u786e,\u72b6\u6001\u672a\u77e5\" ) def post ( self , request ): try: # \u8bbe\u7f6esetting\u7684\u53c2\u6570\u6765\u542f\u52a8\u722c\u866b scrapyd = ScrapydAPI ( 'http://localhost:6800/' ) # \u9879\u76ee\u540d\u79f0 print ( scrapyd . list_projects ()) # ['Facebook_Request'] # \u722c\u866b\u540d\u79f0 print ( scrapyd . list_spiders ( 'Request' )) goole_start = request . data . get ( \"goole_start\" , 0 ) goole_defalut_number = request . data . get ( \"goole_defalut_number\" , 30 ) language = request . data . get ( \"language\" ) settings = { 'DOWNLOAD_DELAY' : 3 , } google_keyword = request . data . get ( \"google_keyword\" , \"\u957f\u57ce\" ) job_id = scrapyd . schedule ( 'Facebook_Request' , 'google' , settings = settings , google_keyword = google_keyword , goole_start = goole_start , goole_defalut_number = goole_defalut_number , language = language ) return Response ( data = job_id ) except: return Response ( data = \"\u51fa\u73b0\u8fde\u63a5\u95ee\u9898\uff0c\u8bf7\u6392\u67e5\u65e5\u5fd7\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # scrapy class GoogleSpider ( Spider ): custom_settings = { \"DOWNLOADER_MIDDLEWARES\" : { 'mss.middlewares.ProxyMiddleware' : 543 , 'mss.middlewares.PrintUrlMiddleware' : 543 , # \u968f\u673a\u6362header 'mss.middlewares.RandomUserAgentMiddlware' : 543 , }, # 'mss.middlewares.ProxyMiddleware': 543, # 'mss.pipelines.GooglePipeline':500, \"LOG_ENABLED\" : True , \"LOG_LEVEL\" : 'DEBUG' , } # goole_start = 0 url = 'https://www.google.com/search?q=site:{0}&tbs=qdr:d' name = 'google_site' allowed_domains = [ 'google.com' ] def __init__ ( self , * args , ** kwargs ): super ( GoogleSpider , self ). __init__ (* args , ** kwargs ) self . goole_start = kwargs . get ( \"goole_start\" , 0 ) self . site = kwargs . get ( \"google_keyword\" , \"bbc.com\" ) self . goole_defalut_number = kwargs . get ( \"goole_defalut_number\" , 10 ) self . start_urls = [ self . url . format ( self . site )] # \u8fd9\u91cc\u6211\u4eec\u5c31\u80fd\u63a5\u53d7\u5230\u53d1\u9001\u7684\u989d\u5916\u53c2\u6570\u4ee5\u53casetting\u4e2d\u7684\u503c\u4e86\uff0c\u662f\u4e0d\u662f\u611f\u89c9\u975e\u5e38\u65b9\u4fbf\u3002\u8fd9\u5bf9\u5e94\u5bf9\u590d\u6742\u7684\u9700\u6c42\u662f\u975e\u5e38\u65b9\u4fbf\u7684 \u601d\u8003\u6211\u4eec\u5982\u4f55 \u53c2\u8003\u8d44\u6599 \u00b6 https://github.com/djm/python-scrapyd-api doc","title":"Scrapy scrapyd-api"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#_1","text":"python-scrapyd-api \u662f\u7528restfulapi\u7684\u5f62\u5f0f\u8c03\u7528scrapyd\u7684\u5f62\u5f0f\u53bb\u8c03\u7528scrapy \u722c\u866b","title":"\u524d\u8a00"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#_2","text":"\u8fde\u63a5 1 2 from scrapyd_api import ScrapydAPI scrapyd = ScrapydAPI ( 'http://localhost:6800' )","title":"\u4ecb\u7ecd"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#_3","text":"1 2 3 scrapyd.cancel('project_name', '14a6599ef67111e38a0e080027880ca6') # Returns the \"previous state\" of the job before it was cancelled: 'running' or 'pending'. 'running'","title":"\u53d6\u6d88\u4efb\u52a1\u8c03\u5ea6\u8ba1\u5212"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#_4","text":"1 2 3 scrapyd.delete_project('project_name') # Returns True if the request was met with an OK response. True","title":"\u5220\u9664\u9879\u76ee\u548c\u540c\u7ea7\u7248\u672c"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#_5","text":"1 2 3 scrapyd.delete_version('project_name', 'version_name') # Returns True if the request was met with an OK response. True","title":"\u5220\u9664\u67d0\u4e2a\u7248\u672c\u7684\u9879\u76ee"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#job","text":"1 2 3 scrapyd.job_status('project_name', '14a6599ef67111e38a0e080027880ca6') # Returns 'running', 'pending', 'finished' or '' for unknown state. 'running'","title":"\u67e5\u770b\u67d0\u4e2ajob\u7684\u72b6\u6001"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#_6","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 scrapyd.list_jobs('project_name') # Returns a dict of running, finished and pending job lists. { 'pending': [ { u'id': u'24c35...f12ae', u'spider': u'spider_name' }, ], 'running': [ { u'id': u'14a65...b27ce', u'spider': u'spider_name', u'start_time': u'2014-06-17 22:45:31.975358' }, ], 'finished': [ { u'id': u'34c23...b21ba', u'spider': u'spider_name', u'start_time': u'2014-06-17 22:45:31.975358', u'end_time': u'2014-06-23 14:01:18.209680' } ] }","title":"\u5217\u51fa\u6240\u6709\u7684\u9879\u76ee\u72b6\u6001\u53ca\u8be6\u7ec6\u4fe1\u606f"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#_7","text":"1 2 3 scrapyd.schedule('project_name', 'spider_name') # Returns the Scrapyd job id. u'14a6599ef67111e38a0e080027880ca6'","title":"\u4efb\u52a1\u8c03\u5ea6"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#setting","text":"1 2 settings = {'DOWNLOAD_DELAY': 2} scrapyd.schedule('project_name', 'spider_name', settings=settings)","title":"\u63d0\u4f9bsetting \u4efb\u52a1\u8c03\u5ea6"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#spider","text":"1 scrapyd.schedule('project_name', 'spider_name', extra_attribute='value')","title":"\u5c06\u989d\u5916\u7684\u5c5e\u6027\u4f20\u9012\u7ed9Spider\u521d\u59cb\u5316"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#_8","text":"\u6211\u4eec\u9700\u8981\u4f20\u9012\u989d\u5916\u7684\u53c2\u6570\u63a7\u5236\u722c\u866b\u53bb\u722c\u53d6\u6570\u636e\u600e\u4e48\u529e\u5462\uff1f\u4f8b\u5982Django 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # django class NewApiView ( APIView ): # permission_classes = (IsAuthenticated,) def get ( self , request ): \"\"\" \u67e5\u770b\u722c\u866b\u7684\u4e00\u4e2a\u72b6\u6001 :param request: :return: \"\"\" job_id = request . data . get ( \"job\" , None ) job_state = self . scrapyd . job_status ( 'Facebook_Request' , job_id ) if job_state == \"running\" : return Response ( \"\u6b63\u5728\u8fd0\u884c\" ) elif job_state == \"pending\" : return Response ( \"\u6b63\u5728\u542f\u52a8\" ) elif job_state == \"finished\" : return Response ( \"\u5df2\u7ecf\u7ed3\u675f\" ) else: return Response ( \"\u8bf7\u67e5\u770bjob_id\u662f\u5426\u586b\u5199\u6b63\u786e,\u72b6\u6001\u672a\u77e5\" ) def post ( self , request ): try: # \u8bbe\u7f6esetting\u7684\u53c2\u6570\u6765\u542f\u52a8\u722c\u866b scrapyd = ScrapydAPI ( 'http://localhost:6800/' ) # \u9879\u76ee\u540d\u79f0 print ( scrapyd . list_projects ()) # ['Facebook_Request'] # \u722c\u866b\u540d\u79f0 print ( scrapyd . list_spiders ( 'Request' )) goole_start = request . data . get ( \"goole_start\" , 0 ) goole_defalut_number = request . data . get ( \"goole_defalut_number\" , 30 ) language = request . data . get ( \"language\" ) settings = { 'DOWNLOAD_DELAY' : 3 , } google_keyword = request . data . get ( \"google_keyword\" , \"\u957f\u57ce\" ) job_id = scrapyd . schedule ( 'Facebook_Request' , 'google' , settings = settings , google_keyword = google_keyword , goole_start = goole_start , goole_defalut_number = goole_defalut_number , language = language ) return Response ( data = job_id ) except: return Response ( data = \"\u51fa\u73b0\u8fde\u63a5\u95ee\u9898\uff0c\u8bf7\u6392\u67e5\u65e5\u5fd7\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # scrapy class GoogleSpider ( Spider ): custom_settings = { \"DOWNLOADER_MIDDLEWARES\" : { 'mss.middlewares.ProxyMiddleware' : 543 , 'mss.middlewares.PrintUrlMiddleware' : 543 , # \u968f\u673a\u6362header 'mss.middlewares.RandomUserAgentMiddlware' : 543 , }, # 'mss.middlewares.ProxyMiddleware': 543, # 'mss.pipelines.GooglePipeline':500, \"LOG_ENABLED\" : True , \"LOG_LEVEL\" : 'DEBUG' , } # goole_start = 0 url = 'https://www.google.com/search?q=site:{0}&tbs=qdr:d' name = 'google_site' allowed_domains = [ 'google.com' ] def __init__ ( self , * args , ** kwargs ): super ( GoogleSpider , self ). __init__ (* args , ** kwargs ) self . goole_start = kwargs . get ( \"goole_start\" , 0 ) self . site = kwargs . get ( \"google_keyword\" , \"bbc.com\" ) self . goole_defalut_number = kwargs . get ( \"goole_defalut_number\" , 10 ) self . start_urls = [ self . url . format ( self . site )] # \u8fd9\u91cc\u6211\u4eec\u5c31\u80fd\u63a5\u53d7\u5230\u53d1\u9001\u7684\u989d\u5916\u53c2\u6570\u4ee5\u53casetting\u4e2d\u7684\u503c\u4e86\uff0c\u662f\u4e0d\u662f\u611f\u89c9\u975e\u5e38\u65b9\u4fbf\u3002\u8fd9\u5bf9\u5e94\u5bf9\u590d\u6742\u7684\u9700\u6c42\u662f\u975e\u5e38\u65b9\u4fbf\u7684 \u601d\u8003\u6211\u4eec\u5982\u4f55","title":"\u4e3e\u4f8b"},{"location":"basics/scrapy/09_python_scrapyd_api/09_python_scrapyd_api/#_9","text":"https://github.com/djm/python-scrapyd-api doc","title":"\u53c2\u8003\u8d44\u6599"},{"location":"basics/scrapy/10_scrapy_splash/10_scrapy_splash/","text":"\u524d\u8a00 \u00b6 scrapy-splash \u662f\u4e3a\u4e86\u65b9\u4fbfscrapy\u6846\u67b6\u4f7f\u7528splash\u800c\u8fdb\u884c\u7684\u5c01\u88c5\uff0c\u4ed6\u53ef\u4ee5\u4e0escrapy\u6846\u67b6\u66f4\u597d\u7684\u7ed3\u5408\uff0c\u60f3\u6bd4\u8f83python\u4e2d\u4f7f\u7528requests\u5e93\u6216\u8005\u4f7f\u7528scrapy \u7684Request\u5bf9\u8c61\u6765\u8bf4\uff0c\u66f4\u4e3a\u65b9\u4fbf\uff0c\u800c\u4e14\u652f\u6301\u5f02\u6b65 \u4f7f\u7528\u65b9\u5f0f \u00b6 python \u4e2d\u5b89\u88c5 1 2 # shell pip install scrapy-splash \u542f\u52a8 \u00b6 1 2 # shell docker run -p 8050 :8050 scrapinghub/splash setting\u4e2d\u8bbe\u7f6e 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SPLASH_URL = 'http://192.168.59.103:8050' # \u5fc5\u987b\u8981\u8bbe\u7f6e\u8fd9\u4e2a\u4e2d\u95f4\u4ef6\uff0c\u7ea7\u522b\u8bbe\u7f6e\u6700\u5c0f\uff0c\u4f18\u5148\u7ea7\u6700\u9ad8 DOWNLOADER_MIDDLEWARES = { 'scrapy_splash.SplashCookiesMiddleware' : 723 , 'scrapy_splash.SplashMiddleware' : 725 , 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware' : 810 , } # spider\u4e2d\u95f4\u4ef6 SPIDER_MIDDLEWARES = { 'scrapy_splash.SplashDeduplicateArgsMiddleware' : 100 , } # \u8fc7\u6ee4\uff0c\u8fd9\u4e2a\u81ea\u5b9a\u4e49 DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter' # scrapy http\u7f13\u5b58\u81ea\u5b9a\u4e49 HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage' \u4f7f\u7528 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import scrapy from scrapy_splash import SplashRequest class MySpider ( scrapy . Spider ): start_urls = [ \"http://example.com\" , \"http://example.com/foo\" ] def start_requests ( self ): for url in self . start_urls : # SplashRequest\u6e32\u67d3\u8bf7\u6c42\u7684\u3002 yield SplashRequest ( url , self . parse , args = { 'wait' : 0.5 }) def parse ( self , response ): # response.body is a result of render.html call; it # contains HTML processed by a browser. # ... \u6700\u91cd\u8981\u7684\u662f\u80fd\u8fd0\u884cLua\u811a\u672c\uff0c\u8fd9\u5c31\u5f88\u795e\u5947\u4e86\uff0c\u5177\u4f53\u66f4\u591a\u53ef\u4ee5\u67e5\u770b\u6587\u6863\u3002\u7ecf\u5e38\u4f7f\u7528\u7684\u5c31\u662f\u6709\u4e9b\u662fajax\u8bf7\u6c42\uff0c\u9700\u8981\u6e32\u67d3\u9875\u9762\u53bb\u722c\uff0c\u8fd9\u6837\u4f7f\u7528SplashRequest\u5c31\u80fd\u5f88\u65b9\u4fbf\u505a\u9875\u9762\u6e32\u67d3\u8bf7\u6c42 \u53c2\u8003\u8fde\u63a5 \u00b6 https://splash-cn-doc.readthedocs.io/zh_CN/latest/scrapy-splash-toturial.html https://github.com/scrapy-plugins/scrapy-splash","title":"Scrapy splash"},{"location":"basics/scrapy/10_scrapy_splash/10_scrapy_splash/#_1","text":"scrapy-splash \u662f\u4e3a\u4e86\u65b9\u4fbfscrapy\u6846\u67b6\u4f7f\u7528splash\u800c\u8fdb\u884c\u7684\u5c01\u88c5\uff0c\u4ed6\u53ef\u4ee5\u4e0escrapy\u6846\u67b6\u66f4\u597d\u7684\u7ed3\u5408\uff0c\u60f3\u6bd4\u8f83python\u4e2d\u4f7f\u7528requests\u5e93\u6216\u8005\u4f7f\u7528scrapy \u7684Request\u5bf9\u8c61\u6765\u8bf4\uff0c\u66f4\u4e3a\u65b9\u4fbf\uff0c\u800c\u4e14\u652f\u6301\u5f02\u6b65","title":"\u524d\u8a00"},{"location":"basics/scrapy/10_scrapy_splash/10_scrapy_splash/#_2","text":"python \u4e2d\u5b89\u88c5 1 2 # shell pip install scrapy-splash","title":"\u4f7f\u7528\u65b9\u5f0f"},{"location":"basics/scrapy/10_scrapy_splash/10_scrapy_splash/#_3","text":"1 2 # shell docker run -p 8050 :8050 scrapinghub/splash setting\u4e2d\u8bbe\u7f6e 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SPLASH_URL = 'http://192.168.59.103:8050' # \u5fc5\u987b\u8981\u8bbe\u7f6e\u8fd9\u4e2a\u4e2d\u95f4\u4ef6\uff0c\u7ea7\u522b\u8bbe\u7f6e\u6700\u5c0f\uff0c\u4f18\u5148\u7ea7\u6700\u9ad8 DOWNLOADER_MIDDLEWARES = { 'scrapy_splash.SplashCookiesMiddleware' : 723 , 'scrapy_splash.SplashMiddleware' : 725 , 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware' : 810 , } # spider\u4e2d\u95f4\u4ef6 SPIDER_MIDDLEWARES = { 'scrapy_splash.SplashDeduplicateArgsMiddleware' : 100 , } # \u8fc7\u6ee4\uff0c\u8fd9\u4e2a\u81ea\u5b9a\u4e49 DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter' # scrapy http\u7f13\u5b58\u81ea\u5b9a\u4e49 HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'","title":"\u542f\u52a8"},{"location":"basics/scrapy/10_scrapy_splash/10_scrapy_splash/#_4","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import scrapy from scrapy_splash import SplashRequest class MySpider ( scrapy . Spider ): start_urls = [ \"http://example.com\" , \"http://example.com/foo\" ] def start_requests ( self ): for url in self . start_urls : # SplashRequest\u6e32\u67d3\u8bf7\u6c42\u7684\u3002 yield SplashRequest ( url , self . parse , args = { 'wait' : 0.5 }) def parse ( self , response ): # response.body is a result of render.html call; it # contains HTML processed by a browser. # ... \u6700\u91cd\u8981\u7684\u662f\u80fd\u8fd0\u884cLua\u811a\u672c\uff0c\u8fd9\u5c31\u5f88\u795e\u5947\u4e86\uff0c\u5177\u4f53\u66f4\u591a\u53ef\u4ee5\u67e5\u770b\u6587\u6863\u3002\u7ecf\u5e38\u4f7f\u7528\u7684\u5c31\u662f\u6709\u4e9b\u662fajax\u8bf7\u6c42\uff0c\u9700\u8981\u6e32\u67d3\u9875\u9762\u53bb\u722c\uff0c\u8fd9\u6837\u4f7f\u7528SplashRequest\u5c31\u80fd\u5f88\u65b9\u4fbf\u505a\u9875\u9762\u6e32\u67d3\u8bf7\u6c42","title":"\u4f7f\u7528"},{"location":"basics/scrapy/10_scrapy_splash/10_scrapy_splash/#_5","text":"https://splash-cn-doc.readthedocs.io/zh_CN/latest/scrapy-splash-toturial.html https://github.com/scrapy-plugins/scrapy-splash","title":"\u53c2\u8003\u8fde\u63a5"},{"location":"basics/scrapy/11_scrapy_index/11_scrapy_index/","text":"\u4f55\u4e3ascrapy \u00b6 scrapy \u662f\u4e00\u4e2a\u5b8c\u5168\u7528python\u7f16\u5199\u4e3a\u4e86\u6570\u636e\u6293\u53d6\u57fa\u4e8e\u5f02\u6b65\u7684\u6846\u67b6\uff0c\u4f5c\u7528\u5728\u7f51\u7edc\u4fe1\u606f\u7684\u641c\u53d6\uff0c\u4f7f\u7528\u5b83\u5927\u5927\u7b80\u4fbf\u4e86\u64cd\u4f5c\u95e8\u69db\uff01 scrapy\u6846\u67b6\u56fe \u00b6 Scrapy Engine(\u5f15\u64ce): \u8d1f\u8d23Spider\u3001ItemPipeline\u3001Downloader\u3001Scheduler\u4e2d\u95f4\u7684\u901a\u8baf\uff0c\u4fe1\u53f7\u3001\u6570\u636e\u4f20\u9012\u7b49\u3002 Scheduler(\u8c03\u5ea6\u5668): \u5b83\u8d1f\u8d23\u63a5\u53d7\u5f15\u64ce\u53d1\u9001\u8fc7\u6765\u7684Request\u8bf7\u6c42\uff0c\u5e76\u6309\u7167\u4e00\u5b9a\u7684\u65b9\u5f0f\u8fdb\u884c\u6574\u7406\u6392\u5217\uff0c\u5165\u961f\uff0c\u5f53\u5f15\u64ce\u9700\u8981\u65f6\uff0c\u4ea4\u8fd8\u7ed9\u5f15\u64ce\u3002 Downloader\uff08\u4e0b\u8f7d\u5668\uff09\uff1a\u8d1f\u8d23\u4e0b\u8f7dScrapy Engine(\u5f15\u64ce)\u53d1\u9001\u7684\u6240\u6709Requests\u8bf7\u6c42\uff0c\u5e76\u5c06\u5176\u83b7\u53d6\u5230\u7684Responses\u4ea4\u8fd8\u7ed9Scrapy Engine(\u5f15\u64ce)\uff0c\u7531\u5f15\u64ce\u4ea4\u7ed9Spider\u6765\u5904\u7406\uff0c Spider\uff08\u722c\u866b\uff09\uff1a\u5b83\u8d1f\u8d23\u5904\u7406\u6240\u6709Responses,\u4ece\u4e2d\u5206\u6790\u63d0\u53d6\u6570\u636e\uff0c\u83b7\u53d6Item\u5b57\u6bb5\u9700\u8981\u7684\u6570\u636e\uff0c\u5e76\u5c06\u9700\u8981\u8ddf\u8fdb\u7684URL\u63d0\u4ea4\u7ed9\u5f15\u64ce\uff0c\u518d\u6b21\u8fdb\u5165Scheduler(\u8c03\u5ea6\u5668). Item Pipeline(\u7ba1\u9053)\uff1a\u5b83\u8d1f\u8d23\u5904\u7406Spider\u4e2d\u83b7\u53d6\u5230\u7684Item\uff0c\u5e76\u8fdb\u884c\u8fdb\u884c\u540e\u671f\u5904\u7406\uff08\u8be6\u7ec6\u5206\u6790\u3001\u8fc7\u6ee4\u3001\u5b58\u50a8\u7b49\uff09\u7684\u5730\u65b9\u3002 Downloader Middlewares\uff08\u4e0b\u8f7d\u4e2d\u95f4\u4ef6\uff09\uff1a\u4f60\u53ef\u4ee5\u5f53\u4f5c\u662f\u4e00\u4e2a\u53ef\u4ee5\u81ea\u5b9a\u4e49\u6269\u5c55\u4e0b\u8f7d\u529f\u80fd\u7684\u7ec4\u4ef6\u3002 Spider Middlewares\uff08Spider\u4e2d\u95f4\u4ef6\uff09\uff1a\u4f60\u53ef\u4ee5\u7406\u89e3\u4e3a\u662f\u4e00\u4e2a\u53ef\u4ee5\u81ea\u5b9a\u6269\u5c55\u548c\u64cd\u4f5c\u5f15\u64ce\u548cSpider\u4e2d\u95f4\u901a\u4fe1\u7684\u529f\u80fd\u7ec4\u4ef6\uff08\u6bd4\u5982\u8fdb\u5165Spider\u7684Responses;\u548c\u4eceSpider\u51fa\u53bb\u7684Requests\uff09 \u8fd0\u884c\u8fc7\u7a0b \u00b6 \u5f15\u64ce\uff1aHi\uff01Spider, \u4f60\u8981\u5904\u7406\u54ea\u4e00\u4e2a\u7f51\u7ad9\uff1f Spider\uff1a\u8001\u5927\u8981\u6211\u5904\u7406xxxx.com\u3002 \u5f15\u64ce\uff1a\u4f60\u628a\u7b2c\u4e00\u4e2a\u9700\u8981\u5904\u7406\u7684URL\u7ed9\u6211\u5427\u3002 Spider\uff1a\u7ed9\u4f60\uff0c\u7b2c\u4e00\u4e2aURL\u662fxxxxxxx.com\u3002 \u5f15\u64ce\uff1aHi\uff01\u8c03\u5ea6\u5668\uff0c\u6211\u8fd9\u6709request\u8bf7\u6c42\u4f60\u5e2e\u6211\u6392\u5e8f\u5165\u961f\u4e00\u4e0b\u3002 \u8c03\u5ea6\u5668\uff1a\u597d\u7684\uff0c\u6b63\u5728\u5904\u7406\u4f60\u7b49\u4e00\u4e0b\u3002 \u5f15\u64ce\uff1aHi\uff01\u8c03\u5ea6\u5668\uff0c\u628a\u4f60\u5904\u7406\u597d\u7684request\u8bf7\u6c42\u7ed9\u6211\u3002 \u8c03\u5ea6\u5668\uff1a\u7ed9\u4f60\uff0c\u8fd9\u662f\u6211\u5904\u7406\u597d\u7684request \u5f15\u64ce\uff1aHi\uff01\u4e0b\u8f7d\u5668\uff0c\u4f60\u6309\u7167\u8001\u5927\u7684\u4e0b\u8f7d\u4e2d\u95f4\u4ef6\u7684\u8bbe\u7f6e\u5e2e\u6211\u4e0b\u8f7d\u4e00\u4e0b\u8fd9\u4e2arequest\u8bf7\u6c42 \u4e0b\u8f7d\u5668\uff1a\u597d\u7684\uff01\u7ed9\u4f60\uff0c\u8fd9\u662f\u4e0b\u8f7d\u597d\u7684\u4e1c\u897f\u3002\uff08\u5982\u679c\u5931\u8d25\uff1asorry\uff0c\u8fd9\u4e2arequest\u4e0b\u8f7d\u5931\u8d25\u4e86\u3002\u7136\u540e\u5f15\u64ce\u544a\u8bc9\u8c03\u5ea6\u5668\uff0c\u8fd9\u4e2arequest\u4e0b\u8f7d\u5931\u8d25\u4e86\uff0c\u4f60\u8bb0\u5f55\u4e00\u4e0b\uff0c\u6211\u4eec\u5f85\u4f1a\u513f\u518d\u4e0b\u8f7d\uff09 \u5f15\u64ce\uff1aHi\uff01Spider\uff0c\u8fd9\u662f\u4e0b\u8f7d\u597d\u7684\u4e1c\u897f\uff0c\u5e76\u4e14\u5df2\u7ecf\u6309\u7167\u8001\u5927\u7684\u4e0b\u8f7d\u4e2d\u95f4\u4ef6\u5904\u7406\u8fc7\u4e86\uff0c\u4f60\u81ea\u5df1\u5904\u7406\u4e00\u4e0b\uff08\u6ce8\u610f\uff01\u8fd9\u513fresponses\u9ed8\u8ba4\u662f\u4ea4\u7ed9def parse()\u8fd9\u4e2a\u51fd\u6570\u5904\u7406\u7684\uff09 Spider\uff1a\uff08\u5904\u7406\u5b8c\u6bd5\u6570\u636e\u4e4b\u540e\u5bf9\u4e8e\u9700\u8981\u8ddf\u8fdb\u7684URL\uff09\uff0cHi\uff01\u5f15\u64ce\uff0c\u6211\u8fd9\u91cc\u6709\u4e24\u4e2a\u7ed3\u679c\uff0c\u8fd9\u4e2a\u662f\u6211\u9700\u8981\u8ddf\u8fdb\u7684URL\uff0c\u8fd8\u6709\u8fd9\u4e2a\u662f\u6211\u83b7\u53d6\u5230\u7684Item\u6570\u636e\u3002 \u5f15\u64ce\uff1aHi \uff01\u7ba1\u9053 \u6211\u8fd9\u513f\u6709\u4e2aitem\u4f60\u5e2e\u6211\u5904\u7406\u4e00\u4e0b\uff01\u8c03\u5ea6\u5668\uff01\u8fd9\u662f\u9700\u8981\u8ddf\u8fdbURL\u4f60\u5e2e\u6211\u5904\u7406\u4e0b\u3002\u7136\u540e\u4ece\u7b2c\u56db\u6b65\u5f00\u59cb\u5faa\u73af\uff0c\u76f4\u5230\u83b7\u53d6\u5b8c\u8001\u5927\u9700\u8981\u5168\u90e8\u4fe1\u606f\u3002 \u7ba1\u9053\u8c03\u5ea6\u5668\uff1a\u597d\u7684\uff0c\u73b0\u5728\u5c31\u505a\uff01 \u4e3a\u4ec0\u4e48\u8981\u770b\u4e0b\u4ed6\u7684\u6e90\u7801 \u00b6 \u56e0\u4e3a\u5e94\u5bf9\u590d\u6742\u7684\u5404\u79cd\u9700\u6c42\uff0c\u6846\u67b6\u867d\u7136\u7075\u6d3b\uff0c\u4f46\u662f\u6211\u4eec\u8fd8\u9700\u8981\u505a\u5230\u5728\u7f16\u5199\u89c4\u5219\u7684\u65f6\u5019\u5fc3\u4e2d\u6709\u6570\uff0c\u4e0d\u81f3\u4e8e\u5199\u5b8c\u4e86\u4e0d\u77e5\u9053\u5982\u4f55\u53bb\u66f4\u6539\uff0c\u9762\u5bf9\u7a0d\u5fae\u590d\u6742\u70b9\u7684\u9700\u6c42\u5c31\u624b\u8db3\u65e0\u63aa\uff01 \u53c2\u8003\u94fe\u63a5 \u00b6 \u5b98\u65b9github","title":"Scrapy \u7b80\u5355\u4ecb\u7ecd"},{"location":"basics/scrapy/11_scrapy_index/11_scrapy_index/#scrapy","text":"scrapy \u662f\u4e00\u4e2a\u5b8c\u5168\u7528python\u7f16\u5199\u4e3a\u4e86\u6570\u636e\u6293\u53d6\u57fa\u4e8e\u5f02\u6b65\u7684\u6846\u67b6\uff0c\u4f5c\u7528\u5728\u7f51\u7edc\u4fe1\u606f\u7684\u641c\u53d6\uff0c\u4f7f\u7528\u5b83\u5927\u5927\u7b80\u4fbf\u4e86\u64cd\u4f5c\u95e8\u69db\uff01","title":"\u4f55\u4e3ascrapy"},{"location":"basics/scrapy/11_scrapy_index/11_scrapy_index/#scrapy_1","text":"Scrapy Engine(\u5f15\u64ce): \u8d1f\u8d23Spider\u3001ItemPipeline\u3001Downloader\u3001Scheduler\u4e2d\u95f4\u7684\u901a\u8baf\uff0c\u4fe1\u53f7\u3001\u6570\u636e\u4f20\u9012\u7b49\u3002 Scheduler(\u8c03\u5ea6\u5668): \u5b83\u8d1f\u8d23\u63a5\u53d7\u5f15\u64ce\u53d1\u9001\u8fc7\u6765\u7684Request\u8bf7\u6c42\uff0c\u5e76\u6309\u7167\u4e00\u5b9a\u7684\u65b9\u5f0f\u8fdb\u884c\u6574\u7406\u6392\u5217\uff0c\u5165\u961f\uff0c\u5f53\u5f15\u64ce\u9700\u8981\u65f6\uff0c\u4ea4\u8fd8\u7ed9\u5f15\u64ce\u3002 Downloader\uff08\u4e0b\u8f7d\u5668\uff09\uff1a\u8d1f\u8d23\u4e0b\u8f7dScrapy Engine(\u5f15\u64ce)\u53d1\u9001\u7684\u6240\u6709Requests\u8bf7\u6c42\uff0c\u5e76\u5c06\u5176\u83b7\u53d6\u5230\u7684Responses\u4ea4\u8fd8\u7ed9Scrapy Engine(\u5f15\u64ce)\uff0c\u7531\u5f15\u64ce\u4ea4\u7ed9Spider\u6765\u5904\u7406\uff0c Spider\uff08\u722c\u866b\uff09\uff1a\u5b83\u8d1f\u8d23\u5904\u7406\u6240\u6709Responses,\u4ece\u4e2d\u5206\u6790\u63d0\u53d6\u6570\u636e\uff0c\u83b7\u53d6Item\u5b57\u6bb5\u9700\u8981\u7684\u6570\u636e\uff0c\u5e76\u5c06\u9700\u8981\u8ddf\u8fdb\u7684URL\u63d0\u4ea4\u7ed9\u5f15\u64ce\uff0c\u518d\u6b21\u8fdb\u5165Scheduler(\u8c03\u5ea6\u5668). Item Pipeline(\u7ba1\u9053)\uff1a\u5b83\u8d1f\u8d23\u5904\u7406Spider\u4e2d\u83b7\u53d6\u5230\u7684Item\uff0c\u5e76\u8fdb\u884c\u8fdb\u884c\u540e\u671f\u5904\u7406\uff08\u8be6\u7ec6\u5206\u6790\u3001\u8fc7\u6ee4\u3001\u5b58\u50a8\u7b49\uff09\u7684\u5730\u65b9\u3002 Downloader Middlewares\uff08\u4e0b\u8f7d\u4e2d\u95f4\u4ef6\uff09\uff1a\u4f60\u53ef\u4ee5\u5f53\u4f5c\u662f\u4e00\u4e2a\u53ef\u4ee5\u81ea\u5b9a\u4e49\u6269\u5c55\u4e0b\u8f7d\u529f\u80fd\u7684\u7ec4\u4ef6\u3002 Spider Middlewares\uff08Spider\u4e2d\u95f4\u4ef6\uff09\uff1a\u4f60\u53ef\u4ee5\u7406\u89e3\u4e3a\u662f\u4e00\u4e2a\u53ef\u4ee5\u81ea\u5b9a\u6269\u5c55\u548c\u64cd\u4f5c\u5f15\u64ce\u548cSpider\u4e2d\u95f4\u901a\u4fe1\u7684\u529f\u80fd\u7ec4\u4ef6\uff08\u6bd4\u5982\u8fdb\u5165Spider\u7684Responses;\u548c\u4eceSpider\u51fa\u53bb\u7684Requests\uff09","title":"scrapy\u6846\u67b6\u56fe"},{"location":"basics/scrapy/11_scrapy_index/11_scrapy_index/#_1","text":"\u5f15\u64ce\uff1aHi\uff01Spider, \u4f60\u8981\u5904\u7406\u54ea\u4e00\u4e2a\u7f51\u7ad9\uff1f Spider\uff1a\u8001\u5927\u8981\u6211\u5904\u7406xxxx.com\u3002 \u5f15\u64ce\uff1a\u4f60\u628a\u7b2c\u4e00\u4e2a\u9700\u8981\u5904\u7406\u7684URL\u7ed9\u6211\u5427\u3002 Spider\uff1a\u7ed9\u4f60\uff0c\u7b2c\u4e00\u4e2aURL\u662fxxxxxxx.com\u3002 \u5f15\u64ce\uff1aHi\uff01\u8c03\u5ea6\u5668\uff0c\u6211\u8fd9\u6709request\u8bf7\u6c42\u4f60\u5e2e\u6211\u6392\u5e8f\u5165\u961f\u4e00\u4e0b\u3002 \u8c03\u5ea6\u5668\uff1a\u597d\u7684\uff0c\u6b63\u5728\u5904\u7406\u4f60\u7b49\u4e00\u4e0b\u3002 \u5f15\u64ce\uff1aHi\uff01\u8c03\u5ea6\u5668\uff0c\u628a\u4f60\u5904\u7406\u597d\u7684request\u8bf7\u6c42\u7ed9\u6211\u3002 \u8c03\u5ea6\u5668\uff1a\u7ed9\u4f60\uff0c\u8fd9\u662f\u6211\u5904\u7406\u597d\u7684request \u5f15\u64ce\uff1aHi\uff01\u4e0b\u8f7d\u5668\uff0c\u4f60\u6309\u7167\u8001\u5927\u7684\u4e0b\u8f7d\u4e2d\u95f4\u4ef6\u7684\u8bbe\u7f6e\u5e2e\u6211\u4e0b\u8f7d\u4e00\u4e0b\u8fd9\u4e2arequest\u8bf7\u6c42 \u4e0b\u8f7d\u5668\uff1a\u597d\u7684\uff01\u7ed9\u4f60\uff0c\u8fd9\u662f\u4e0b\u8f7d\u597d\u7684\u4e1c\u897f\u3002\uff08\u5982\u679c\u5931\u8d25\uff1asorry\uff0c\u8fd9\u4e2arequest\u4e0b\u8f7d\u5931\u8d25\u4e86\u3002\u7136\u540e\u5f15\u64ce\u544a\u8bc9\u8c03\u5ea6\u5668\uff0c\u8fd9\u4e2arequest\u4e0b\u8f7d\u5931\u8d25\u4e86\uff0c\u4f60\u8bb0\u5f55\u4e00\u4e0b\uff0c\u6211\u4eec\u5f85\u4f1a\u513f\u518d\u4e0b\u8f7d\uff09 \u5f15\u64ce\uff1aHi\uff01Spider\uff0c\u8fd9\u662f\u4e0b\u8f7d\u597d\u7684\u4e1c\u897f\uff0c\u5e76\u4e14\u5df2\u7ecf\u6309\u7167\u8001\u5927\u7684\u4e0b\u8f7d\u4e2d\u95f4\u4ef6\u5904\u7406\u8fc7\u4e86\uff0c\u4f60\u81ea\u5df1\u5904\u7406\u4e00\u4e0b\uff08\u6ce8\u610f\uff01\u8fd9\u513fresponses\u9ed8\u8ba4\u662f\u4ea4\u7ed9def parse()\u8fd9\u4e2a\u51fd\u6570\u5904\u7406\u7684\uff09 Spider\uff1a\uff08\u5904\u7406\u5b8c\u6bd5\u6570\u636e\u4e4b\u540e\u5bf9\u4e8e\u9700\u8981\u8ddf\u8fdb\u7684URL\uff09\uff0cHi\uff01\u5f15\u64ce\uff0c\u6211\u8fd9\u91cc\u6709\u4e24\u4e2a\u7ed3\u679c\uff0c\u8fd9\u4e2a\u662f\u6211\u9700\u8981\u8ddf\u8fdb\u7684URL\uff0c\u8fd8\u6709\u8fd9\u4e2a\u662f\u6211\u83b7\u53d6\u5230\u7684Item\u6570\u636e\u3002 \u5f15\u64ce\uff1aHi \uff01\u7ba1\u9053 \u6211\u8fd9\u513f\u6709\u4e2aitem\u4f60\u5e2e\u6211\u5904\u7406\u4e00\u4e0b\uff01\u8c03\u5ea6\u5668\uff01\u8fd9\u662f\u9700\u8981\u8ddf\u8fdbURL\u4f60\u5e2e\u6211\u5904\u7406\u4e0b\u3002\u7136\u540e\u4ece\u7b2c\u56db\u6b65\u5f00\u59cb\u5faa\u73af\uff0c\u76f4\u5230\u83b7\u53d6\u5b8c\u8001\u5927\u9700\u8981\u5168\u90e8\u4fe1\u606f\u3002 \u7ba1\u9053\u8c03\u5ea6\u5668\uff1a\u597d\u7684\uff0c\u73b0\u5728\u5c31\u505a\uff01","title":"\u8fd0\u884c\u8fc7\u7a0b"},{"location":"basics/scrapy/11_scrapy_index/11_scrapy_index/#_2","text":"\u56e0\u4e3a\u5e94\u5bf9\u590d\u6742\u7684\u5404\u79cd\u9700\u6c42\uff0c\u6846\u67b6\u867d\u7136\u7075\u6d3b\uff0c\u4f46\u662f\u6211\u4eec\u8fd8\u9700\u8981\u505a\u5230\u5728\u7f16\u5199\u89c4\u5219\u7684\u65f6\u5019\u5fc3\u4e2d\u6709\u6570\uff0c\u4e0d\u81f3\u4e8e\u5199\u5b8c\u4e86\u4e0d\u77e5\u9053\u5982\u4f55\u53bb\u66f4\u6539\uff0c\u9762\u5bf9\u7a0d\u5fae\u590d\u6742\u70b9\u7684\u9700\u6c42\u5c31\u624b\u8db3\u65e0\u63aa\uff01","title":"\u4e3a\u4ec0\u4e48\u8981\u770b\u4e0b\u4ed6\u7684\u6e90\u7801"},{"location":"basics/scrapy/11_scrapy_index/11_scrapy_index/#_3","text":"\u5b98\u65b9github","title":"\u53c2\u8003\u94fe\u63a5"},{"location":"basics/scrapy/12_scrapy_%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE/12_scrapy_%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE/","text":"\u5b9e\u6218\u4ecb\u7ecd \u00b6 \u524d\u7aef \u00b6 css+js+ajax \u540e\u7aef \u00b6 django+Elasticsearch \u722c\u866b\u7aef \u00b6 Scrapy","title":"Scrapy \u5b9e\u6218\u9879\u76ee"},{"location":"basics/scrapy/12_scrapy_%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE/12_scrapy_%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE/#_1","text":"","title":"\u5b9e\u6218\u4ecb\u7ecd"},{"location":"basics/scrapy/12_scrapy_%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE/12_scrapy_%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE/#_2","text":"css+js+ajax","title":"\u524d\u7aef"},{"location":"basics/scrapy/12_scrapy_%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE/12_scrapy_%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE/#_3","text":"django+Elasticsearch","title":"\u540e\u7aef"},{"location":"basics/scrapy/12_scrapy_%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE/12_scrapy_%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE/#_4","text":"Scrapy","title":"\u722c\u866b\u7aef"},{"location":"basics/scrapy/13_scrapy_%E5%A5%87%E6%B7%AB%E6%8A%80%E5%B7%A7/13_scrapy_%E5%A5%87%E6%B7%AB%E6%8A%80%E5%B7%A7/","text":"\u4e2d\u95f4\u4ef6\u4e2d\u52a0\u5165\u5f02\u6b65 \u00b6 \u4ee3\u7406ip \u8bbe\u7f6e\u5f02\u6b65 \u4ee5\u524d 1 2 3 4 5 6 7 8 9 10 # middleware.py import requests import json import random class ProxyMiddleware : def process_request ( self , request , spider ): ip_info = requests . get ( '\u4ee3\u7406\u4f9b\u5e94\u5546\u7684\u7f51\u5740' ) . json () ip_list = ip_info [ 'proxy' ] ip = random . choice ( ip_list ) request . meta [ 'proxy' ] = ip \u73b0\u5728 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # middleware.py import asyncio import aiohttp class TestAiohttp : async def get_ip ( self ): async with aiohttp . ClientSession () as client : resp = await client . get ( 'http://httpbin.org/delay/5' ) result = await resp . json () print ( result ) async def process_request ( self , request , spider ): print ( '\u8bf7\u6c42\u4e00\u4e2a\u5ef6\u8fdf5\u79d2\u7684\u7f51\u5740\u5f00\u59cb' ) await asyncio . create_task ( self . get_ip ()) 1 2 3 # setting.py TWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor' \u540c\u4e00\u4e2a\u9879\u76ee\u542f\u52a8\u591a\u4e2aspider \u00b6 \u5229\u7528scrapy \u7684CrawlerProcess 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # mian.py from scrapy.crawler import CrawlerProcess from scrapy.utils.project import get_project_settings settings = get_project_settings () crawler = CrawlerProcess ( settings ) # exercise\u3001ua\u90fd\u662fspider\u540d\u79f0 crawler . crawl ( 'exercise' ) crawler . crawl ( 'ua' ) crawler . start () crawler . start ()","title":"Scrapy \u5947\u6deb\u6280\u5de7"},{"location":"basics/scrapy/13_scrapy_%E5%A5%87%E6%B7%AB%E6%8A%80%E5%B7%A7/13_scrapy_%E5%A5%87%E6%B7%AB%E6%8A%80%E5%B7%A7/#_1","text":"\u4ee3\u7406ip \u8bbe\u7f6e\u5f02\u6b65 \u4ee5\u524d 1 2 3 4 5 6 7 8 9 10 # middleware.py import requests import json import random class ProxyMiddleware : def process_request ( self , request , spider ): ip_info = requests . get ( '\u4ee3\u7406\u4f9b\u5e94\u5546\u7684\u7f51\u5740' ) . json () ip_list = ip_info [ 'proxy' ] ip = random . choice ( ip_list ) request . meta [ 'proxy' ] = ip \u73b0\u5728 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # middleware.py import asyncio import aiohttp class TestAiohttp : async def get_ip ( self ): async with aiohttp . ClientSession () as client : resp = await client . get ( 'http://httpbin.org/delay/5' ) result = await resp . json () print ( result ) async def process_request ( self , request , spider ): print ( '\u8bf7\u6c42\u4e00\u4e2a\u5ef6\u8fdf5\u79d2\u7684\u7f51\u5740\u5f00\u59cb' ) await asyncio . create_task ( self . get_ip ()) 1 2 3 # setting.py TWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'","title":"\u4e2d\u95f4\u4ef6\u4e2d\u52a0\u5165\u5f02\u6b65"},{"location":"basics/scrapy/13_scrapy_%E5%A5%87%E6%B7%AB%E6%8A%80%E5%B7%A7/13_scrapy_%E5%A5%87%E6%B7%AB%E6%8A%80%E5%B7%A7/#spider","text":"\u5229\u7528scrapy \u7684CrawlerProcess 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # mian.py from scrapy.crawler import CrawlerProcess from scrapy.utils.project import get_project_settings settings = get_project_settings () crawler = CrawlerProcess ( settings ) # exercise\u3001ua\u90fd\u662fspider\u540d\u79f0 crawler . crawl ( 'exercise' ) crawler . crawl ( 'ua' ) crawler . start () crawler . start ()","title":"\u540c\u4e00\u4e2a\u9879\u76ee\u542f\u52a8\u591a\u4e2aspider"}]}